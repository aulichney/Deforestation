{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ba5f3dad",
   "metadata": {},
   "source": [
    "## Notes and Questions\n",
    "- The metrics they use to select their model parameters are metrics for classification, will have to look more into the analogs in regression beyond the standard ones I've done in class\n",
    "- how strictly to follow paper e.g. number trees in random forest\n",
    "\n",
    "- when splitting into test/train noticed that there are different numbers of entries in each municipality, should I split based on data points of number of municipalities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cb69bea",
   "metadata": {},
   "source": [
    "## ToDo\n",
    "- use gridsearch instead of random grid search eventually\n",
    "- municipality level test-train split\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "867c7b45",
   "metadata": {},
   "source": [
    "### Covariates in corruption paper \n",
    "1. private sector includes different measures of economic activity and sectoral distributions\n",
    "\n",
    "- Average business establishments size based on employment, number of business establishments, payroll per employee, average business establishments payroll, share of business establishments entering, share of business establishments exiting, business establishments churning, share of private sector workers over population, Hirschman-Herfindahl index based on business establishments size, average growth in business establishments and in employment in past 3 years, share of business establishments below 5 employees, share of business establishments between 5 and 25 employees, share of business establishments above 25 employees, share of business establishments in construction, share of business establishments in retail, share of business establishments in services.\n",
    "\n",
    "2. public sector features include the size, relative importance, and wages of public officials\n",
    "\n",
    "- Share of public sector employees over population, average wage of public sector employees, share of public institutions opening,share of public institutions closing, public institutions churning, share of workers by position within the institution, average growth in public employment and public institutions in past 3 years, share of public sector employees from municipal institutions, number of public institutions, average public institution size based on employment.\n",
    "\n",
    "3. financial development includes measures of credit-related variables from public and private banks\n",
    "\n",
    "- Share of business establishments receiving public loans, number of public loans per business establishment, total public credit per business establishment, average interest rate in public lending, bank branches per capita, banks per capita, total private credit per capita, total deposits per capita, and Hirschman-Herfindahl index based on private banks total assets and based on private banks credit.\n",
    "\n",
    "4. human capital includes measures of education and access to it\n",
    "\n",
    "- Literacy rate, the share of population between 15 and 24 years old that finished, the first, second, and third cycle of primary education (Census), illiteracy rate (Census), average test scores in Portuguese and maths for nationwide tests at 4th and 8th grade, average private sector employees education, average private sector employees education by worker position within the firm, share of unqualified public employees based on job requirements, share of unqualified public employees by position within the institution, average public employees education, average public employees education by position within the institution, number of higher public education institutions per capita, number of higher private education institutions per capita.\n",
    "\n",
    "5. public spending includes different types of spending as well as local procurement variables\n",
    "\n",
    "- Total expenditures per capita, personnel expenditures per capita, budget surplus per capita, total revenue per capita, federal transfers of capital per capita, federal current transfers per capita, transfers from the national tax fund per capita, share of business establishments in the municipality with public procurement, number of contracts per business establishments, federal procurement expenditure over population, share of discretionary contracts, and share of competitive contracts.\n",
    "\n",
    "6. local politics includes variables of political competition and alignment with the central government\n",
    "\n",
    "- Number of candidates, Hirschman-Herfindahl index based on the vote shares, margin of victory between the winner and the runner-up, an indicator for whether the mayor is in his second term, an indicator for whether the mayor’s party is the same as the one of the governor, an indicator for whether the mayor’s party is from the same party as the one of the president, an indicator if the mayor is from right-wing party, an indicator if the mayor is from left-wing party, average candidate campaign donations and expenditures for firms and individuals, and per capita campaign donations and expenditures for firms and individuals.\n",
    "\n",
    "7. local demographics\n",
    "\n",
    "- Population density, GDP per capita, share of population living in rural areas (Census), deaths by aggression, GINI coefficient for income distribution (Census), average night light intensity coverage performing deblurring, inter-calibration, and geometric corrections, local radio, local newspapers, infant mortality rate, child mortality rate, average number of prenatal visits, share of abnormal births, share of underweight births, share of births with more than seven prenatal visits, and share of births with more than four prenatal visits.\n",
    "\n",
    "8. natural resources’ dependency includes the relevance of different natural resources, and finally \n",
    "- Share of business establishments in agriculture and mining sector, share of production of each of the top-7 crops in the country multiplied by the the log change in international prices and share of value of production over GDP (as constructed in Bernstein et al., 2018). The crops included are sugar cane, oranges, soybeans, maize, rice, rice, banana, and wheat, covering more than 98% of total agricultural production."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "101a83ae",
   "metadata": {},
   "source": [
    "### Covariates in deforestation\n",
    "1. federal politics\n",
    "- \"lula\": Lula government years (2003-2010),\n",
    "- \"dilma\": Dilma government yeras (2011-2016),\n",
    "- \"temer\": Temer interim government (2017-2018),\n",
    "- \"bolsonaro\": Bolsonaro government (2019-2020),\n",
    "- \"fed_election_year\": Years where there was a Federal Election \n",
    "- \"new_forest_code\": years after new forest code (post 2012), --? is this one sorted correctly\n",
    "\n",
    "2. local politics includes variables of political competition and alignment with the central government\n",
    "- \"mun_election_year\": municipal election year,\n",
    "\n",
    "\n",
    "3. local demographics\n",
    "\n",
    "- \"populacao\": Population (I think this is census data so 2000 and 2010),\n",
    "- \"pib_pc\": GDP per capita,\n",
    "- \"indigenous_homol\": pixel is inside an indigenous, homologated territory\n",
    "\n",
    "4. natural resources and economy\n",
    "\n",
    "- \"ironore\": whether the municipality produces ironore,\n",
    "- \"silver\": whether the municipality produces silver,\n",
    "- \"copper\": whether the municipality produces copper,\n",
    "- \"gold\": whether the municipality produces gold,\n",
    "\n",
    "- \"soy_price\": whether the municipality produces soy,\n",
    "- \"beef_price\": whether the municipality produces beef,\n",
    "\n",
    "- \"ag_jobs\": total employment in the agricultural sector,\n",
    "- \"mining_jobs\": total employment in the mining sector,\n",
    "- \"public_jobs\": total employment in the public sector,\n",
    "- \"construction_jobs\": total employment in the construction sector\n",
    "\n",
    "\n",
    "5. non-human related geographic variables\n",
    "- \"rain1\": rainfall,\n",
    "- \"elevation\": elevation (meters above sea level),\n",
    "- \"slope\": slope,\n",
    "- \"aspect\": aspect,\n",
    "- \"near_mines\": distance to nearest mine,\n",
    "- \"near_roads\": distance to nearest road,\n",
    "- \"near_hidrovia\": distance to nearest hydroeletric,"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e45c0d6",
   "metadata": {},
   "source": [
    "## Models\n",
    "- Random Forests\n",
    "    - In this application, we keep fixed the number of fitted trees (500) and use cross-validation to determine the optimal number of features available in every node\n",
    "- Gradient Boosting \n",
    "    - we keep fixed the learning rate (shrinkage parameter) and the minimum number of observations in the terminal nodes to avoid overfitting, and use cross-validation to determine the optimal number of trees and the interaction depth\n",
    "- Neural Networks \n",
    "    - we keep fixed a logistic activation function and use cross-validation to determine the optimal number of units in the hidden layer (size) and the regularization parameter (decay)\n",
    "- LASSO\n",
    "    - The tuning parameter in the cross-validation is the weight of the penalization term in the objective function (λ), which is optimized over a grid of potential values\n",
    "- Super Learner Ensemble\n",
    "    - we use the Super Learner ensemble method developed by Polley et al. (2011), which finds an optimal combination of individual prediction models by minimizing the cross-validated out-of-bag risk of these predic- tions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e335012",
   "metadata": {},
   "source": [
    "## Protocol\n",
    "- We divide our dataset into 70% as our training set and 30% as our testing set\n",
    "- In our training set, we perform a 5-fold cross-validation procedure in order to train our models and choose the optimal combination of parameters\n",
    "- The previous step is repeated 10 times with different random partitions. Hence, we obtain 10 “optimal parameters” and we use as our optimal parameter the average of them. For the case of integer parameters, we round it to the closest integer\n",
    "- Using these optimal parameters, we assess the performance of our models in the testing set that has never been used for training purposes\n",
    "- We standardize the data by the mean and standard deviation of the training set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da8ffde0",
   "metadata": {},
   "source": [
    "## Assessing Models’ Performance\n",
    "- We use as a first performance measure of interest the area under the ROC (Receiver Operating Characteristic) curve (AUC)\n",
    "- We also present each model’s level of accuracy, which corresponds to the proportion of municipalities correctly predicted as corrupt; models’ precision, which is the proportion of positive identifications that are correct (or true positives over true positives plus false positives); models’ recall, which is the proportion of actual positives identified correctly (true positives over true positives plus false negatives), and models’ F1, which is the harmonic mean of precision and recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "10c662d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#general\n",
    "import pyreadr\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "#random forests\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "#gradient boosting\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "#plotting\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e412c4f",
   "metadata": {},
   "source": [
    "## Get Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6f27ac2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = pyreadr.read_r('/Users/annieulichney/Desktop/Deforestation/analysis.Rdata') #read R dataframe into Python\n",
    "df1 = pd.DataFrame(result['forest_full'])\n",
    "df = df1.sample(1000000) #trim because this frame is quite large, test that all runs before doing entire dataset!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "00a6dc71",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(['nn_forest.l', 'forest.diff'], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8c7a354",
   "metadata": {},
   "source": [
    "## Test Train Split Based on Municipalities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b50e8544",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ID\n",
       "1100015    3480\n",
       "1100023    2085\n",
       "1100031     704\n",
       "1100049    1815\n",
       "1100056    1425\n",
       "           ... \n",
       "5108808     540\n",
       "5108857     930\n",
       "5108907    5760\n",
       "5108956    2535\n",
       "5204904      15\n",
       "Length: 770, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#observe that there are diff numbers of entries in each mun, will get 30% of rows instead of 30% of muns\n",
    "df1.groupby(['ID']).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3f353dcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#there are 770 municipalities included in the whole set \n",
    "all_muns = np.unique(df['ID'])\n",
    "num_mun = len(all_muns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c9133b0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#30% of the municipalities will be used to test, others for train\n",
    "\n",
    "num_test = int(round(0.3*num_mun, 0))\n",
    "num_train = num_mun - num_test\n",
    "\n",
    "test_split_threshold = int(round(0.3*df.shape[0], 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4dd381ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#randomly select num_test muns to test on, the rest are for training\n",
    "\n",
    "train_mun = list(all_muns)\n",
    "test_mun = []\n",
    "\n",
    "num_rows_test = 0\n",
    "\n",
    "while num_rows_test < test_split_threshold:\n",
    "\n",
    "    this_mun = np.random.choice(np.array(train_mun))\n",
    "    train_mun.remove(this_mun)\n",
    "    test_mun.append(this_mun)\n",
    "    \n",
    "    df_test = df[df['ID'].isin(test_mun)]\n",
    "    \n",
    "    num_rows_test = df_test.shape[0]\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "296d8935",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the complementary train set \n",
    "df_train = df[df['ID'].isin(train_mun)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "23523579",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.301062"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check to see how much of the data is in the test set\n",
    "df_test.shape[0]/(df_test.shape[0] + df_train.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bfabd43f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#make sure we correctly split into disjoint sets\n",
    "for mun in train_mun: \n",
    "    if mun in test_mun:\n",
    "        print('Fail')\n",
    "        \n",
    "for mun in test_mun: \n",
    "    if mun in train_mun:\n",
    "        print('Fail')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "56076c84",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create unscaled test and train vars\n",
    "\n",
    "y_train = df_train['forest.l']\n",
    "y_test = df_test['forest.l']\n",
    "\n",
    "X_train_unscaled = df_train.drop('forest.l', axis =1)\n",
    "X_test_unscaled = df_test.drop('forest.l', axis =1)\n",
    "\n",
    "#scale them\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train_unscaled)\n",
    "X_test = sc.transform(X_test_unscaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a3edecec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#if information leakage persists block by time period too \n",
    "#same pixels being repeated multiple times, FID \n",
    "#try filtering to keep each pixel only once-- identify source of leakage\n",
    "#can then use element of prediction \n",
    "#consider adding time as variable, later use more sophisticated tools to get around this\n",
    "#nearest neighbor variable, try without this variable, this variable absorbs a lot of the variation potentially\n",
    "#explanatory abilities vs. variable importance\n",
    "#commodity prices vary by year "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4990f390",
   "metadata": {},
   "source": [
    "## Random Forests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8fa7f3c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#use random search cross validation to tune model hyperparameters\n",
    "\n",
    "# number of trees in random forest\n",
    "n_estimators = [int(x) for x in np.linspace(start = 10, stop = 20, num = 3)]\n",
    "\n",
    "# number of features to consider at every split\n",
    "max_features = ['auto', 'sqrt']\n",
    "\n",
    "# maximum number of levels in tree\n",
    "max_depth = [int(x) for x in np.linspace(10, 110, num = 10)]\n",
    "max_depth.append(None)\n",
    "\n",
    "# minimum number of samples required to split a node\n",
    "min_samples_split = [2, 5, 10]\n",
    "\n",
    "# minimum number of samples required at each leaf node\n",
    "min_samples_leaf = [1, 2, 4]\n",
    "\n",
    "# method of selecting samples for training each tree\n",
    "bootstrap = [True, False]\n",
    "\n",
    "#create random grid, all poss combos of these variables\n",
    "random_grid = {'n_estimators': n_estimators,\n",
    "               'max_features': max_features,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf,\n",
    "               'bootstrap': bootstrap}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "31d363fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 3 candidates, totalling 9 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=3, estimator=RandomForestRegressor(), n_iter=3, n_jobs=-1,\n",
       "                   param_distributions={'bootstrap': [True, False],\n",
       "                                        'max_depth': [10, 21, 32, 43, 54, 65,\n",
       "                                                      76, 87, 98, 110, None],\n",
       "                                        'max_features': ['auto', 'sqrt'],\n",
       "                                        'min_samples_leaf': [1, 2, 4],\n",
       "                                        'min_samples_split': [2, 5, 10],\n",
       "                                        'n_estimators': [10, 15, 20]},\n",
       "                   random_state=42, scoring='r2', verbose=2)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use the random grid to search for best hyperparameters\n",
    "\n",
    "#create initial model to tune\n",
    "rf = RandomForestRegressor()\n",
    "\n",
    "# Random search of parameters, using 5 fold cross validation, \n",
    "# search across 1000 different combinations\n",
    "#note that we're using r2 scoring here since the goal of the project is to explain deforestation\n",
    "#todo: research additional scoring methods \n",
    "\n",
    "rf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, n_iter = 3, scoring = 'r2', cv = 3, verbose = 2, random_state = 42, n_jobs = -1)\n",
    "\n",
    "# Fit the random search model\n",
    "rf_random.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6c67d566",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_estimators': 20,\n",
       " 'min_samples_split': 5,\n",
       " 'min_samples_leaf': 4,\n",
       " 'max_features': 'sqrt',\n",
       " 'max_depth': 110,\n",
       " 'bootstrap': False}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#let's see what the best parameters are\n",
    "rf_random.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6ef4e84f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9020786374949644"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check how our best score did-- 93% best_score\n",
    "rf_random.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "aaea3785",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model(model, test_features, test_labels):\n",
    "    predictions = model.predict(test_features)\n",
    "    \n",
    "    print('Mean Absolute Error (MAE):', metrics.mean_absolute_error(test_labels, predictions))\n",
    "    print('Mean Squared Error (MSE):', metrics.mean_squared_error(test_labels, predictions))\n",
    "    print('Root Mean Squared Error (RMSE):', metrics.mean_squared_error(test_labels, predictions, squared=False))\n",
    "    print('Mean Absolute Percentage Error (MAPE):', metrics.mean_absolute_percentage_error(test_labels, predictions))\n",
    "    print('Explained Variance Score:', metrics.explained_variance_score(test_labels, predictions))\n",
    "    print('Max Error:', metrics.max_error(test_labels, predictions))\n",
    "    print('Mean Squared Log Error:', metrics.mean_squared_log_error(test_labels, predictions))\n",
    "    print('Median Absolute Error:', metrics.median_absolute_error(test_labels, predictions))\n",
    "    print('R^2:', metrics.r2_score(test_labels, predictions))\n",
    "    print('Mean Poisson Deviance:', metrics.mean_poisson_deviance(test_labels, predictions))\n",
    "    #print('Mean Gamma Deviance:', metrics.mean_gamma_deviance(test_labels, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5b6b1746",
   "metadata": {},
   "outputs": [],
   "source": [
    "def diff(previous, current):\n",
    "    return ((float(current)-previous)/previous)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2e6f3a1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_2_models(model1, model2, test_features, test_labels):\n",
    "    \n",
    "    predictions1 = model1.predict(test_features)\n",
    "    predictions2 = model2.predict(test_features)\n",
    "    \n",
    "    print('% Diff Mean Absolute Error (MAE):', diff(metrics.mean_absolute_error(test_labels, predictions1), metrics.mean_absolute_error(test_labels, predictions2)))\n",
    "    print('% Diff Mean Squared Error (MSE):', diff(metrics.mean_squared_error(test_labels, predictions1), metrics.mean_squared_error(test_labels, predictions2)))\n",
    "    print('% Diff Root Mean Squared Error (RMSE):', diff(metrics.mean_squared_error(test_labels, predictions1, squared=False), metrics.mean_squared_error(test_labels, predictions2, squared=False)))\n",
    "    print('% Diff Mean Absolute Percentage Error (MAPE):', diff(metrics.mean_absolute_percentage_error(test_labels, predictions1), metrics.mean_absolute_percentage_error(test_labels, predictions2)))\n",
    "    print('% Diff Explained Variance Score:', diff(metrics.explained_variance_score(test_labels, predictions1), metrics.explained_variance_score(test_labels, predictions2)))\n",
    "    print('% Diff Max Error:', diff(metrics.max_error(test_labels, predictions1), metrics.max_error(test_labels, predictions2)))\n",
    "    print('% Diff Mean Squared Log Error:', diff(metrics.mean_squared_log_error(test_labels, predictions1), metrics.mean_squared_log_error(test_labels, predictions2)))\n",
    "    print('% Diff Median Absolute Error:', diff(metrics.median_absolute_error(test_labels, predictions1), metrics.median_absolute_error(test_labels, predictions2)))\n",
    "    print('% Diff R^2:', diff(metrics.r2_score(test_labels, predictions1), metrics.r2_score(test_labels, predictions2)))\n",
    "    print('% Diff Mean Poisson Deviance:', diff(metrics.mean_poisson_deviance(test_labels, predictions1), metrics.mean_poisson_deviance(test_labels, predictions2)))\n",
    "    #print('% Diff Mean Gamma Deviance:', diff(metrics.mean_gamma_deviance(test_labels, predictions1), metrics.mean_gamma_deviance(test_labels, predictions2)))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fb172747",
   "metadata": {},
   "outputs": [],
   "source": [
    "#compare arbitrarily selected base model to our cv searched model \n",
    "base_model = RandomForestRegressor(n_estimators = 10, random_state = 42)\n",
    "base_model.fit(X_train, y_train)\n",
    "best_random = rf_random.best_estimator_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "15585c28",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error (MAE): 11.477990247855924\n",
      "Mean Squared Error (MSE): 259.66052238409367\n",
      "Root Mean Squared Error (RMSE): 16.113985304203727\n",
      "Mean Absolute Percentage Error (MAPE): 39430543402309.08\n",
      "Explained Variance Score: 0.6962721194824429\n",
      "Max Error: 95.0\n",
      "Mean Squared Log Error: 0.12977111081437911\n",
      "Median Absolute Error: 8.099999999999994\n",
      "R^2: 0.6856902885343743\n",
      "Mean Poisson Deviance: 5.25298001156206\n"
     ]
    }
   ],
   "source": [
    "eval_model(base_model, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "22d24d8b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error (MAE): 11.41483933882142\n",
      "Mean Squared Error (MSE): 217.0241709951998\n",
      "Root Mean Squared Error (RMSE): 14.731740256846772\n",
      "Mean Absolute Percentage Error (MAPE): 41398616017227.17\n",
      "Explained Variance Score: 0.7463449424118906\n",
      "Max Error: 81.85065476190476\n",
      "Mean Squared Log Error: 0.1228878023905446\n",
      "Median Absolute Error: 8.977974137931042\n",
      "R^2: 0.7373000564727107\n",
      "Mean Poisson Deviance: 4.084195986520492\n"
     ]
    }
   ],
   "source": [
    "eval_model(best_random, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "41b24c1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "% Diff Mean Absolute Error (MAE): -0.5501913459658265\n",
      "% Diff Mean Squared Error (MSE): -16.420036052236526\n",
      "% Diff Root Mean Squared Error (RMSE): -8.577921732349864\n",
      "% Diff Mean Absolute Percentage Error (MAPE): 4.991238885140097\n",
      "% Diff Explained Variance Score: 7.191559381505629\n",
      "% Diff Max Error: -13.841416040100249\n",
      "% Diff Mean Squared Log Error: -5.304191649927541\n",
      "% Diff Median Absolute Error: 10.839186888037638\n",
      "% Diff R^2: 7.526687894129207\n",
      "% Diff Mean Poisson Deviance: -22.249923328644275\n"
     ]
    }
   ],
   "source": [
    "#compare the above\n",
    "eval_2_models(base_model, best_random, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c26694c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4b5501dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#n_estimators should be 500 as in the paper? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6add199b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cv': 3,\n",
       " 'error_score': nan,\n",
       " 'estimator__bootstrap': True,\n",
       " 'estimator__ccp_alpha': 0.0,\n",
       " 'estimator__criterion': 'mse',\n",
       " 'estimator__max_depth': None,\n",
       " 'estimator__max_features': 'auto',\n",
       " 'estimator__max_leaf_nodes': None,\n",
       " 'estimator__max_samples': None,\n",
       " 'estimator__min_impurity_decrease': 0.0,\n",
       " 'estimator__min_impurity_split': None,\n",
       " 'estimator__min_samples_leaf': 1,\n",
       " 'estimator__min_samples_split': 2,\n",
       " 'estimator__min_weight_fraction_leaf': 0.0,\n",
       " 'estimator__n_estimators': 100,\n",
       " 'estimator__n_jobs': None,\n",
       " 'estimator__oob_score': False,\n",
       " 'estimator__random_state': None,\n",
       " 'estimator__verbose': 0,\n",
       " 'estimator__warm_start': False,\n",
       " 'estimator': RandomForestRegressor(),\n",
       " 'n_iter': 3,\n",
       " 'n_jobs': -1,\n",
       " 'param_distributions': {'n_estimators': [10, 15, 20],\n",
       "  'max_features': ['auto', 'sqrt'],\n",
       "  'max_depth': [10, 21, 32, 43, 54, 65, 76, 87, 98, 110, None],\n",
       "  'min_samples_split': [2, 5, 10],\n",
       "  'min_samples_leaf': [1, 2, 4],\n",
       "  'bootstrap': [True, False]},\n",
       " 'pre_dispatch': '2*n_jobs',\n",
       " 'random_state': 42,\n",
       " 'refit': True,\n",
       " 'return_train_score': False,\n",
       " 'scoring': 'r2',\n",
       " 'verbose': 2}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_random.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cc1fb13",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0b4cf0a2",
   "metadata": {},
   "source": [
    "## Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "88ddfd40",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b704ab9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df['forest.l']\n",
    "X = df.drop('forest.l', axis =1)\n",
    "\n",
    "X_train_unscaled, X_test_unscaled, y_train, y_test = train_test_split(X, y, test_size = 0.3) #30/70 data split\n",
    "\n",
    "#normalize our explanatory variables\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train_unscaled)\n",
    "X_test = sc.transform(X_test_unscaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4e57d8e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#use random search cross validation to tune model hyperparameters\n",
    "\n",
    "# trade-off between learning_rate and n_estimators\n",
    "learning_rate = [0.1, 0.2, 0.3]\n",
    "\n",
    "# \n",
    "n_estimators = [80, 100, 200, 300]\n",
    "\n",
    "# maximum number of levels in tree\n",
    "criterion = ['friedman_mse', 'squared_error', 'mse', 'mae']\n",
    "\n",
    "# minimum number of samples required to split a node\n",
    "min_samples_split = [2, 5, 10]\n",
    "\n",
    "# minimum number of samples required at each leaf node\n",
    "min_samples_leaf = [1, 2, 4]\n",
    "\n",
    "\n",
    "#create random grid, all poss combos of these variables\n",
    "random_grid = {'n_estimators': n_estimators,\n",
    "               'criterion': criterion,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf,}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7901858b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n"
     ]
    }
   ],
   "source": [
    "# Use the random grid to search for best hyperparameters\n",
    "\n",
    "#create initial model to tune\n",
    "gbr = GradientBoostingRegressor()\n",
    "\n",
    "# Random search of parameters, using 5 fold cross validation, \n",
    "# search across 1000 different combinations\n",
    "#note that we're using r2 scoring here since the goal of the project is to explain deforestation\n",
    "#todo: research additional scoring methods \n",
    "\n",
    "gbr_random = RandomizedSearchCV(estimator = gbr, param_distributions = random_grid, n_iter = 10, scoring = 'r2', cv = 3, verbose=2, random_state=42, n_jobs = -1)\n",
    "\n",
    "# Fit the random search model\n",
    "gbr_random.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2232ef0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's see what the best parameters are\n",
    "gbr_random.best_params_\n",
    "\n",
    "#check how our best score did-- 93% best_score\n",
    "gbr_random.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4081374",
   "metadata": {},
   "outputs": [],
   "source": [
    "#compare arbitrarily selected base model to our cv searched model \n",
    "base_model = GradientBoostingRegressor(n_estimators = 10, random_state = 42)\n",
    "base_model.fit(X_train, y_train)\n",
    "best_random = gbr_random.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83287be5",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_model(base_model, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "089df2b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_model(best_random, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "188d1161",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_2_models(base_model, best_random, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04f0268b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#great changes on this one, much better effects of using cross validation with GBR than RF\n",
    "#Is this excpected? compare my results here to other findings. How much do these metrics usually change? \n",
    "#What metrics do we care most about given our applications? \n",
    "#How can each model translate to variable importance estimates?\n",
    "#How do the important variables change when we use Machine learning vs. Simple Linear Regression? Why? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd1493d0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8d897b6b",
   "metadata": {},
   "source": [
    "## Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "8cbb138b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80c7f033",
   "metadata": {},
   "source": [
    "corruption paper uses logistic activation function, uses cross-validation to determine the optimal number of units in the hidden layer (size) and the regularization parameter (decay)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "5d651a04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape[0] == y_train.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "71ec3ef0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "34964/34964 [==============================] - 26s 748us/step - loss: 916.7074\n",
      "Epoch 2/10\n",
      "34964/34964 [==============================] - 27s 770us/step - loss: 50.2547\n",
      "Epoch 3/10\n",
      "34964/34964 [==============================] - 26s 739us/step - loss: 50.0724\n",
      "Epoch 4/10\n",
      "34964/34964 [==============================] - 28s 808us/step - loss: 49.9234\n",
      "Epoch 5/10\n",
      "34964/34964 [==============================] - 27s 784us/step - loss: 49.8015\n",
      "Epoch 6/10\n",
      "34964/34964 [==============================] - 25s 706us/step - loss: 48.6472\n",
      "Epoch 7/10\n",
      "34964/34964 [==============================] - 25s 715us/step - loss: 47.4189\n",
      "Epoch 8/10\n",
      "34964/34964 [==============================] - 26s 733us/step - loss: 46.7291\n",
      "Epoch 9/10\n",
      "34964/34964 [==============================] - 29s 841us/step - loss: 46.3762\n",
      "Epoch 10/10\n",
      "34964/34964 [==============================] - 26s 731us/step - loss: 46.0243\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7ffa4de6af70>"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create ANN model\n",
    "model = Sequential()\n",
    " \n",
    "# Defining the Input layer and FIRST hidden layer, both are same!\n",
    "model.add(Dense(units = 5, input_dim = X_train.shape[1], kernel_initializer = 'normal', activation = 'relu'))\n",
    " \n",
    "# Defining the Second layer of the model\n",
    "# after the first layer we don't have to specify input_dim as keras configure it automatically\n",
    "model.add(Dense(units = 5, kernel_initializer = 'normal', activation = 'tanh'))\n",
    "  \n",
    "# The output neuron is a single fully connected node \n",
    "# Since we will be predicting a single number\n",
    "model.add(Dense(1, kernel_initializer = 'normal'))\n",
    " \n",
    "# Compiling the model\n",
    "model.compile(loss = 'mean_squared_error', optimizer = 'adam')\n",
    " \n",
    "# Fitting the ANN to the Training set\n",
    "model.fit(X_train, y_train ,batch_size = 20, epochs = 10, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "c2bfca48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error (MAE): 4.130995156660067\n",
      "Mean Squared Error (MSE): 42.2252100603819\n",
      "Root Mean Squared Error (RMSE): 6.498092801767446\n",
      "Mean Absolute Percentage Error (MAPE): 10851031446015.746\n",
      "Explained Variance Score: 0.9418334031060395\n",
      "Max Error: 66.75267028808594\n",
      "Mean Squared Log Error: 0.024134077658375514\n",
      "Median Absolute Error: 2.3586502075195312\n",
      "R^2: 0.9418048882212294\n",
      "Mean Poisson Deviance: 0.8027225105071734\n"
     ]
    }
   ],
   "source": [
    "eval_model(model, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5ad3883",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a function to find the best parameters for ANN\n",
    "def FunctionFindBestParams(X_train, y_train, X_test, y_test):\n",
    "    \n",
    "    # Defining the list of hyper parameters to try\n",
    "    batch_size_list=[5, 10, 15, 20]\n",
    "    epoch_list  =   [5, 10, 50, 100]\n",
    "    \n",
    "    import pandas as pd\n",
    "    SearchResultsData=pd.DataFrame(columns=['TrialNumber', 'Parameters', 'Accuracy'])\n",
    "    \n",
    "    # initializing the trials\n",
    "    TrialNumber=0\n",
    "    for batch_size_trial in batch_size_list:\n",
    "        for epochs_trial in epoch_list:\n",
    "            TrialNumber+=1\n",
    "            # create ANN model\n",
    "            model = Sequential()\n",
    "            # Defining the first layer of the model\n",
    "            model.add(Dense(units=5, input_dim=X_train.shape[1], kernel_initializer='normal', activation='relu'))\n",
    " \n",
    "            # Defining the Second layer of the model\n",
    "            model.add(Dense(units=5, kernel_initializer='normal', activation='relu'))\n",
    " \n",
    "            # The output neuron is a single fully connected node \n",
    "            # Since we will be predicting a single number\n",
    "            model.add(Dense(1, kernel_initializer='normal'))\n",
    " \n",
    "            # Compiling the model\n",
    "            model.compile(loss='mean_squared_error', optimizer='adam')\n",
    " \n",
    "            # Fitting the ANN to the Training set\n",
    "            model.fit(X_train, y_train ,batch_size = batch_size_trial, epochs = epochs_trial, verbose=0)\n",
    " \n",
    "            MAPE = np.mean(100 * (np.abs(y_test-model.predict(X_test))/y_test))\n",
    "            \n",
    "            # printing the results of the current iteration\n",
    "            print(TrialNumber, 'Parameters:','batch_size:', batch_size_trial,'-', 'epochs:',epochs_trial, 'Accuracy:', 100-MAPE)\n",
    "            \n",
    "            SearchResultsData=SearchResultsData.append(pd.DataFrame(data=[[TrialNumber, str(batch_size_trial)+'-'+str(epochs_trial), 100-MAPE]],\n",
    "                                                                    columns=['TrialNumber', 'Parameters', 'Accuracy'] ))\n",
    "    return(SearchResultsData)\n",
    " \n",
    "\n",
    "ResultsData=FunctionFindBestParams(X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ffe7ff4",
   "metadata": {},
   "source": [
    "## Variable Importance Things to look into: \n",
    "https://mljar.com/blog/feature-importance-in-random-forest/\n",
    "\n",
    "https://arxiv.org/pdf/1407.7502.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9f97565",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
