{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ba5f3dad",
   "metadata": {},
   "source": [
    "## Notes and Questions\n",
    "- The metrics they use to select their model parameters are metrics for classification, will have to look more into the analogs in regression beyond the standard ones I've done in class\n",
    "- how strictly to follow paper e.g. number trees in random forest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cb69bea",
   "metadata": {},
   "source": [
    "## ToDo\n",
    "- use gridsearch instead of random grid search eventually\n",
    "- municipality level test-train split\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "867c7b45",
   "metadata": {},
   "source": [
    "### Covariates in corruption paper \n",
    "1. private sector includes different measures of economic activity and sectoral distributions\n",
    "\n",
    "- Average business establishments size based on employment, number of business establishments, payroll per employee, average business establishments payroll, share of business establishments entering, share of business establishments exiting, business establishments churning, share of private sector workers over population, Hirschman-Herfindahl index based on business establishments size, average growth in business establishments and in employment in past 3 years, share of business establishments below 5 employees, share of business establishments between 5 and 25 employees, share of business establishments above 25 employees, share of business establishments in construction, share of business establishments in retail, share of business establishments in services.\n",
    "\n",
    "2. public sector features include the size, relative importance, and wages of public officials\n",
    "\n",
    "- Share of public sector employees over population, average wage of public sector employees, share of public institutions opening,share of public institutions closing, public institutions churning, share of workers by position within the institution, average growth in public employment and public institutions in past 3 years, share of public sector employees from municipal institutions, number of public institutions, average public institution size based on employment.\n",
    "\n",
    "3. financial development includes measures of credit-related variables from public and private banks\n",
    "\n",
    "- Share of business establishments receiving public loans, number of public loans per business establishment, total public credit per business establishment, average interest rate in public lending, bank branches per capita, banks per capita, total private credit per capita, total deposits per capita, and Hirschman-Herfindahl index based on private banks total assets and based on private banks credit.\n",
    "\n",
    "4. human capital includes measures of education and access to it\n",
    "\n",
    "- Literacy rate, the share of population between 15 and 24 years old that finished, the first, second, and third cycle of primary education (Census), illiteracy rate (Census), average test scores in Portuguese and maths for nationwide tests at 4th and 8th grade, average private sector employees education, average private sector employees education by worker position within the firm, share of unqualified public employees based on job requirements, share of unqualified public employees by position within the institution, average public employees education, average public employees education by position within the institution, number of higher public education institutions per capita, number of higher private education institutions per capita.\n",
    "\n",
    "5. public spending includes different types of spending as well as local procurement variables\n",
    "\n",
    "- Total expenditures per capita, personnel expenditures per capita, budget surplus per capita, total revenue per capita, federal transfers of capital per capita, federal current transfers per capita, transfers from the national tax fund per capita, share of business establishments in the municipality with public procurement, number of contracts per business establishments, federal procurement expenditure over population, share of discretionary contracts, and share of competitive contracts.\n",
    "\n",
    "6. local politics includes variables of political competition and alignment with the central government\n",
    "\n",
    "- Number of candidates, Hirschman-Herfindahl index based on the vote shares, margin of victory between the winner and the runner-up, an indicator for whether the mayor is in his second term, an indicator for whether the mayor’s party is the same as the one of the governor, an indicator for whether the mayor’s party is from the same party as the one of the president, an indicator if the mayor is from right-wing party, an indicator if the mayor is from left-wing party, average candidate campaign donations and expenditures for firms and individuals, and per capita campaign donations and expenditures for firms and individuals.\n",
    "\n",
    "7. local demographics\n",
    "\n",
    "- Population density, GDP per capita, share of population living in rural areas (Census), deaths by aggression, GINI coefficient for income distribution (Census), average night light intensity coverage performing deblurring, inter-calibration, and geometric corrections, local radio, local newspapers, infant mortality rate, child mortality rate, average number of prenatal visits, share of abnormal births, share of underweight births, share of births with more than seven prenatal visits, and share of births with more than four prenatal visits.\n",
    "\n",
    "8. natural resources’ dependency includes the relevance of different natural resources, and finally \n",
    "- Share of business establishments in agriculture and mining sector, share of production of each of the top-7 crops in the country multiplied by the the log change in international prices and share of value of production over GDP (as constructed in Bernstein et al., 2018). The crops included are sugar cane, oranges, soybeans, maize, rice, rice, banana, and wheat, covering more than 98% of total agricultural production."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "101a83ae",
   "metadata": {},
   "source": [
    "### Covariates in deforestation\n",
    "1. federal politics\n",
    "- \"lula\": Lula government years (2003-2010),\n",
    "- \"dilma\": Dilma government yeras (2011-2016),\n",
    "- \"temer\": Temer interim government (2017-2018),\n",
    "- \"bolsonaro\": Bolsonaro government (2019-2020),\n",
    "- \"fed_election_year\": Years where there was a Federal Election \n",
    "- \"new_forest_code\": years after new forest code (post 2012), --? is this one sorted correctly\n",
    "\n",
    "2. local politics includes variables of political competition and alignment with the central government\n",
    "- \"mun_election_year\": municipal election year,\n",
    "\n",
    "\n",
    "3. local demographics\n",
    "\n",
    "- \"populacao\": Population (I think this is census data so 2000 and 2010),\n",
    "- \"pib_pc\": GDP per capita,\n",
    "- \"indigenous_homol\": pixel is inside an indigenous, homologated territory\n",
    "\n",
    "4. natural resources and economy\n",
    "\n",
    "- \"ironore\": whether the municipality produces ironore,\n",
    "- \"silver\": whether the municipality produces silver,\n",
    "- \"copper\": whether the municipality produces copper,\n",
    "- \"gold\": whether the municipality produces gold,\n",
    "\n",
    "- \"soy_price\": whether the municipality produces soy,\n",
    "- \"beef_price\": whether the municipality produces beef,\n",
    "\n",
    "- \"ag_jobs\": total employment in the agricultural sector,\n",
    "- \"mining_jobs\": total employment in the mining sector,\n",
    "- \"public_jobs\": total employment in the public sector,\n",
    "- \"construction_jobs\": total employment in the construction sector\n",
    "\n",
    "\n",
    "5. non-human related geographic variables\n",
    "- \"rain1\": rainfall,\n",
    "- \"elevation\": elevation (meters above sea level),\n",
    "- \"slope\": slope,\n",
    "- \"aspect\": aspect,\n",
    "- \"near_mines\": distance to nearest mine,\n",
    "- \"near_roads\": distance to nearest road,\n",
    "- \"near_hidrovia\": distance to nearest hydroeletric,"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e45c0d6",
   "metadata": {},
   "source": [
    "## Models\n",
    "- Random Forests\n",
    "    - In this application, we keep fixed the number of fitted trees (500) and use cross-validation to determine the optimal number of features available in every node\n",
    "- Gradient Boosting \n",
    "    - we keep fixed the learning rate (shrinkage parameter) and the minimum number of observations in the terminal nodes to avoid overfitting, and use cross-validation to determine the optimal number of trees and the interaction depth\n",
    "- Neural Networks \n",
    "    - we keep fixed a logistic activation function and use cross-validation to determine the optimal number of units in the hidden layer (size) and the regularization parameter (decay)\n",
    "- LASSO\n",
    "    - The tuning parameter in the cross-validation is the weight of the penalization term in the objective function (λ), which is optimized over a grid of potential values\n",
    "- Super Learner Ensemble\n",
    "    - we use the Super Learner ensemble method developed by Polley et al. (2011), which finds an optimal combination of individual prediction models by minimizing the cross-validated out-of-bag risk of these predic- tions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e335012",
   "metadata": {},
   "source": [
    "## Protocol\n",
    "- We divide our dataset into 70% as our training set and 30% as our testing set\n",
    "- In our training set, we perform a 5-fold cross-validation procedure in order to train our models and choose the optimal combination of parameters\n",
    "- The previous step is repeated 10 times with different random partitions. Hence, we obtain 10 “optimal parameters” and we use as our optimal parameter the average of them. For the case of integer parameters, we round it to the closest integer\n",
    "- Using these optimal parameters, we assess the performance of our models in the testing set that has never been used for training purposes\n",
    "- We standardize the data by the mean and standard deviation of the training set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da8ffde0",
   "metadata": {},
   "source": [
    "## Assessing Models’ Performance\n",
    "- We use as a first performance measure of interest the area under the ROC (Receiver Operating Characteristic) curve (AUC)\n",
    "- We also present each model’s level of accuracy, which corresponds to the proportion of municipalities correctly predicted as corrupt; models’ precision, which is the proportion of positive identifications that are correct (or true positives over true positives plus false positives); models’ recall, which is the proportion of actual positives identified correctly (true positives over true positives plus false negatives), and models’ F1, which is the harmonic mean of precision and recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "10c662d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#general\n",
    "import pyreadr\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "#random forests\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "#gradient boosting\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "#plotting\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e412c4f",
   "metadata": {},
   "source": [
    "## Get Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "id": "6f27ac2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = pyreadr.read_r('/Users/annieulichney/Desktop/Deforestation/analysis.Rdata') #read R dataframe into Python\n",
    "df1 = pd.DataFrame(result['forest_full'])\n",
    "df = df1.sample(1000) #trim because this frame is quite large, test that all runs before doing entire dataset!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4990f390",
   "metadata": {},
   "source": [
    "## Random Forests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "id": "1a487838",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df['forest.l']\n",
    "X = df.drop('forest.l', axis =1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "id": "42b7e9d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_unscaled, X_test_unscaled, y_train, y_test = train_test_split(X, y, test_size = 0.3) #30/70 data split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "id": "ac07c14b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#normalize our explanatory variables\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train_unscaled)\n",
    "X_test = sc.transform(X_test_unscaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "id": "8fa7f3c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#use random search cross validation to tune model hyperparameters\n",
    "\n",
    "# number of trees in random forest\n",
    "n_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]\n",
    "\n",
    "# number of features to consider at every split\n",
    "max_features = ['auto', 'sqrt']\n",
    "\n",
    "# maximum number of levels in tree\n",
    "max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\n",
    "max_depth.append(None)\n",
    "\n",
    "# minimum number of samples required to split a node\n",
    "min_samples_split = [2, 5, 10]\n",
    "\n",
    "# minimum number of samples required at each leaf node\n",
    "min_samples_leaf = [1, 2, 4]\n",
    "\n",
    "# method of selecting samples for training each tree\n",
    "bootstrap = [True, False]\n",
    "\n",
    "#create random grid, all poss combos of these variables\n",
    "random_grid = {'n_estimators': n_estimators,\n",
    "               'max_features': max_features,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf,\n",
    "               'bootstrap': bootstrap}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "id": "31d363fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 100 candidates, totalling 500 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=5, estimator=RandomForestRegressor(), n_iter=100,\n",
       "                   n_jobs=-1,\n",
       "                   param_distributions={'bootstrap': [True, False],\n",
       "                                        'max_depth': [10, 20, 30, 40, 50, 60,\n",
       "                                                      70, 80, 90, 100, 110,\n",
       "                                                      None],\n",
       "                                        'max_features': ['auto', 'sqrt'],\n",
       "                                        'min_samples_leaf': [1, 2, 4],\n",
       "                                        'min_samples_split': [2, 5, 10],\n",
       "                                        'n_estimators': [200, 400, 600, 800,\n",
       "                                                         1000, 1200, 1400, 1600,\n",
       "                                                         1800, 2000]},\n",
       "                   random_state=42, scoring='r2', verbose=2)"
      ]
     },
     "execution_count": 262,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use the random grid to search for best hyperparameters\n",
    "\n",
    "#create initial model to tune\n",
    "rf = RandomForestRegressor()\n",
    "\n",
    "# Random search of parameters, using 5 fold cross validation, \n",
    "# search across 1000 different combinations\n",
    "#note that we're using r2 scoring here since the goal of the project is to explain deforestation\n",
    "#todo: research additional scoring methods \n",
    "\n",
    "rf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, n_iter = 100, scoring = 'r2', cv = 5, verbose=2, random_state=42, n_jobs = -1)\n",
    "\n",
    "# Fit the random search model\n",
    "rf_random.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "id": "6c67d566",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_estimators': 200,\n",
       " 'min_samples_split': 5,\n",
       " 'min_samples_leaf': 4,\n",
       " 'max_features': 'auto',\n",
       " 'max_depth': 10,\n",
       " 'bootstrap': True}"
      ]
     },
     "execution_count": 263,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#let's see what the best parameters are\n",
    "rf_random.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "id": "6ef4e84f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.934627575167231"
      ]
     },
     "execution_count": 264,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check how our best score did-- 93% best_score\n",
    "rf_random.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "id": "aaea3785",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model(model, test_features, test_labels):\n",
    "    predictions = model.predict(test_features)\n",
    "    \n",
    "    print('Mean Absolute Error (MAE):', metrics.mean_absolute_error(test_labels, predictions))\n",
    "    print('Mean Squared Error (MSE):', metrics.mean_squared_error(test_labels, predictions))\n",
    "    print('Root Mean Squared Error (RMSE):', metrics.mean_squared_error(test_labels, predictions, squared=False))\n",
    "    print('Mean Absolute Percentage Error (MAPE):', metrics.mean_absolute_percentage_error(test_labels, predictions))\n",
    "    print('Explained Variance Score:', metrics.explained_variance_score(test_labels, predictions))\n",
    "    print('Max Error:', metrics.max_error(test_labels, predictions))\n",
    "    print('Mean Squared Log Error:', metrics.mean_squared_log_error(test_labels, predictions))\n",
    "    print('Median Absolute Error:', metrics.median_absolute_error(test_labels, predictions))\n",
    "    print('R^2:', metrics.r2_score(test_labels, predictions))\n",
    "    print('Mean Poisson Deviance:', metrics.mean_poisson_deviance(test_labels, predictions))\n",
    "    print('Mean Gamma Deviance:', metrics.mean_gamma_deviance(test_labels, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "id": "5b6b1746",
   "metadata": {},
   "outputs": [],
   "source": [
    "def diff(previous, current):\n",
    "    return ((float(current)-previous)/previous)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fceec53e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "id": "2e6f3a1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_2_models(model1, model2, test_features, test_labels):\n",
    "    \n",
    "    predictions1 = model1.predict(test_features)\n",
    "    predictions2 = model2.predict(test_features)\n",
    "    \n",
    "    print('% Diff Mean Absolute Error (MAE):', diff(metrics.mean_absolute_error(test_labels, predictions1), metrics.mean_absolute_error(test_labels, predictions2)))\n",
    "    print('% Diff Mean Squared Error (MSE):', diff(metrics.mean_squared_error(test_labels, predictions1), metrics.mean_squared_error(test_labels, predictions2)))\n",
    "    print('% Diff Root Mean Squared Error (RMSE):', diff(metrics.mean_squared_error(test_labels, predictions1, squared=False), metrics.mean_squared_error(test_labels, predictions2, squared=False)))\n",
    "    print('% Diff Mean Absolute Percentage Error (MAPE):', diff(metrics.mean_absolute_percentage_error(test_labels, predictions1), metrics.mean_absolute_percentage_error(test_labels, predictions2)))\n",
    "    print('% Diff Explained Variance Score:', diff(metrics.explained_variance_score(test_labels, predictions1), metrics.explained_variance_score(test_labels, predictions2)))\n",
    "    print('% Diff Max Error:', diff(metrics.max_error(test_labels, predictions1), metrics.max_error(test_labels, predictions2)))\n",
    "    print('% Diff Mean Squared Log Error:', diff(metrics.mean_squared_log_error(test_labels, predictions1), metrics.mean_squared_log_error(test_labels, predictions2)))\n",
    "    print('% Diff Median Absolute Error:', diff(metrics.median_absolute_error(test_labels, predictions1), metrics.median_absolute_error(test_labels, predictions2)))\n",
    "    print('% Diff R^2:', diff(metrics.r2_score(test_labels, predictions1), metrics.r2_score(test_labels, predictions2)))\n",
    "    print('% Diff Mean Poisson Deviance:', diff(metrics.mean_poisson_deviance(test_labels, predictions1), metrics.mean_poisson_deviance(test_labels, predictions2)))\n",
    "    print('% Diff Mean Gamma Deviance:', diff(metrics.mean_gamma_deviance(test_labels, predictions1), metrics.mean_gamma_deviance(test_labels, predictions2)))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "id": "fb172747",
   "metadata": {},
   "outputs": [],
   "source": [
    "#compare arbitrarily selected base model to our cv searched model \n",
    "base_model = RandomForestRegressor(n_estimators = 10, random_state = 42)\n",
    "base_model.fit(X_train, y_train)\n",
    "best_random = rf_random.best_estimator_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "id": "15585c28",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error (MAE): 3.9469999999999996\n",
      "Mean Squared Error (MSE): 41.66683333333333\n",
      "Root Mean Squared Error (RMSE): 6.454985153610605\n",
      "Mean Absolute Percentage Error (MAPE): 0.11975761147974257\n",
      "Explained Variance Score: 0.9495971290169967\n",
      "Max Error: 37.1\n",
      "Mean Squared Log Error: 0.0355894472013568\n",
      "Median Absolute Error: 2.299999999999997\n",
      "R^2: 0.9495179345503043\n",
      "Mean Poisson Deviance: 0.9153915028647823\n",
      "Mean Gamma Deviance: 0.040746085633077106\n"
     ]
    }
   ],
   "source": [
    "eval_model(base_model, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "id": "22d24d8b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error (MAE): 3.7969506487005926\n",
      "Mean Squared Error (MSE): 40.193962265261916\n",
      "Root Mean Squared Error (RMSE): 6.33987083979334\n",
      "Mean Absolute Percentage Error (MAPE): 0.11093781953163609\n",
      "Explained Variance Score: 0.9513482528523591\n",
      "Max Error: 36.44068888056384\n",
      "Mean Squared Log Error: 0.0316863556357667\n",
      "Median Absolute Error: 1.8540487813674105\n",
      "R^2: 0.9513024131801661\n",
      "Mean Poisson Deviance: 0.8661864255008385\n",
      "Mean Gamma Deviance: 0.036936638399714186\n"
     ]
    }
   ],
   "source": [
    "eval_model(best_random, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "id": "41b24c1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "% Diff Mean Absolute Error (MAE): -3.801605049389587\n",
      "% Diff Mean Squared Error (MSE): -3.534876423865696\n",
      "% Diff Root Mean Squared Error (RMSE): -1.783339714621581\n",
      "% Diff Mean Absolute Percentage Error (MAPE): -7.364702618170016\n",
      "% Diff Explained Variance Score: 0.18440702713320062\n",
      "% Diff Max Error: -1.7771189203131028\n",
      "% Diff Mean Squared Log Error: -10.966991264313034\n",
      "% Diff Median Absolute Error: -19.38918341880814\n",
      "% Diff R^2: 0.187935221119008\n",
      "% Diff Mean Poisson Deviance: -5.375304141446922\n",
      "% Diff Mean Gamma Deviance: -9.349234838574196\n"
     ]
    }
   ],
   "source": [
    "#compare the above\n",
    "eval_2_models(base_model, best_random, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c26694c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "id": "4b5501dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#n_estimators should be 500 as in the paper? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "id": "6add199b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cv': 5,\n",
       " 'error_score': nan,\n",
       " 'estimator__bootstrap': True,\n",
       " 'estimator__ccp_alpha': 0.0,\n",
       " 'estimator__criterion': 'mse',\n",
       " 'estimator__max_depth': None,\n",
       " 'estimator__max_features': 'auto',\n",
       " 'estimator__max_leaf_nodes': None,\n",
       " 'estimator__max_samples': None,\n",
       " 'estimator__min_impurity_decrease': 0.0,\n",
       " 'estimator__min_impurity_split': None,\n",
       " 'estimator__min_samples_leaf': 1,\n",
       " 'estimator__min_samples_split': 2,\n",
       " 'estimator__min_weight_fraction_leaf': 0.0,\n",
       " 'estimator__n_estimators': 100,\n",
       " 'estimator__n_jobs': None,\n",
       " 'estimator__oob_score': False,\n",
       " 'estimator__random_state': None,\n",
       " 'estimator__verbose': 0,\n",
       " 'estimator__warm_start': False,\n",
       " 'estimator': RandomForestRegressor(),\n",
       " 'n_iter': 100,\n",
       " 'n_jobs': -1,\n",
       " 'param_distributions': {'n_estimators': [200,\n",
       "   400,\n",
       "   600,\n",
       "   800,\n",
       "   1000,\n",
       "   1200,\n",
       "   1400,\n",
       "   1600,\n",
       "   1800,\n",
       "   2000],\n",
       "  'max_features': ['auto', 'sqrt'],\n",
       "  'max_depth': [10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, None],\n",
       "  'min_samples_split': [2, 5, 10],\n",
       "  'min_samples_leaf': [1, 2, 4],\n",
       "  'bootstrap': [True, False]},\n",
       " 'pre_dispatch': '2*n_jobs',\n",
       " 'random_state': 42,\n",
       " 'refit': True,\n",
       " 'return_train_score': False,\n",
       " 'scoring': 'r2',\n",
       " 'verbose': 2}"
      ]
     },
     "execution_count": 273,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_random.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cc1fb13",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0b4cf0a2",
   "metadata": {},
   "source": [
    "## Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "id": "88ddfd40",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "id": "b704ab9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df['forest.l']\n",
    "X = df.drop('forest.l', axis =1)\n",
    "\n",
    "X_train_unscaled, X_test_unscaled, y_train, y_test = train_test_split(X, y, test_size = 0.3) #30/70 data split\n",
    "\n",
    "#normalize our explanatory variables\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train_unscaled)\n",
    "X_test = sc.transform(X_test_unscaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "id": "4e57d8e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#use random search cross validation to tune model hyperparameters\n",
    "\n",
    "# trade-off between learning_rate and n_estimators\n",
    "learning_rate = [0.1, 0.2, 0.3]\n",
    "\n",
    "# \n",
    "n_estimators = [80, 100, 200, 300]\n",
    "\n",
    "# maximum number of levels in tree\n",
    "criterion = ['friedman_mse', 'squared_error', 'mse', 'mae']\n",
    "\n",
    "# minimum number of samples required to split a node\n",
    "min_samples_split = [2, 5, 10]\n",
    "\n",
    "# minimum number of samples required at each leaf node\n",
    "min_samples_leaf = [1, 2, 4]\n",
    "\n",
    "\n",
    "#create random grid, all poss combos of these variables\n",
    "random_grid = {'n_estimators': n_estimators,\n",
    "               'criterion': criterion,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf,}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "id": "7901858b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 100 candidates, totalling 500 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=5, estimator=GradientBoostingRegressor(), n_iter=100,\n",
       "                   n_jobs=-1,\n",
       "                   param_distributions={'criterion': ['friedman_mse',\n",
       "                                                      'squared_error', 'mse',\n",
       "                                                      'mae'],\n",
       "                                        'min_samples_leaf': [1, 2, 4],\n",
       "                                        'min_samples_split': [2, 5, 10],\n",
       "                                        'n_estimators': [80, 100, 200, 300]},\n",
       "                   random_state=42, scoring='r2', verbose=2)"
      ]
     },
     "execution_count": 279,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use the random grid to search for best hyperparameters\n",
    "\n",
    "#create initial model to tune\n",
    "gbr = GradientBoostingRegressor()\n",
    "\n",
    "# Random search of parameters, using 5 fold cross validation, \n",
    "# search across 1000 different combinations\n",
    "#note that we're using r2 scoring here since the goal of the project is to explain deforestation\n",
    "#todo: research additional scoring methods \n",
    "\n",
    "gbr_random = RandomizedSearchCV(estimator = gbr, param_distributions = random_grid, n_iter = 100, scoring = 'r2', cv = 5, verbose=2, random_state=42, n_jobs = -1)\n",
    "\n",
    "# Fit the random search model\n",
    "gbr_random.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "id": "2232ef0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9382840589585462"
      ]
     },
     "execution_count": 280,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#let's see what the best parameters are\n",
    "gbr_random.best_params_\n",
    "\n",
    "#check how our best score did-- 93% best_score\n",
    "gbr_random.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "id": "d4081374",
   "metadata": {},
   "outputs": [],
   "source": [
    "#compare arbitrarily selected base model to our cv searched model \n",
    "base_model = GradientBoostingRegressor(n_estimators = 10, random_state = 42)\n",
    "base_model.fit(X_train, y_train)\n",
    "best_random = gbr_random.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "id": "83287be5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error (MAE): 9.772165351232049\n",
      "Mean Squared Error (MSE): 128.27886736785445\n",
      "Root Mean Squared Error (RMSE): 11.326026106620734\n",
      "Mean Absolute Percentage Error (MAPE): 0.281517844010598\n",
      "Explained Variance Score: 0.8252335750411198\n",
      "Max Error: 33.316413779436715\n",
      "Mean Squared Log Error: 0.10286085452865466\n",
      "Median Absolute Error: 9.733925785954824\n",
      "R^2: 0.8223620607249653\n",
      "Mean Poisson Deviance: 2.7176813225222265\n",
      "Mean Gamma Deviance: 0.08179499776663553\n"
     ]
    }
   ],
   "source": [
    "eval_model(base_model, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "id": "089df2b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error (MAE): 4.123193117563597\n",
      "Mean Squared Error (MSE): 48.66928630795911\n",
      "Root Mean Squared Error (RMSE): 6.976337599912945\n",
      "Mean Absolute Percentage Error (MAPE): 0.09257356693063805\n",
      "Explained Variance Score: 0.9328057381136158\n",
      "Max Error: 33.01603282796255\n",
      "Mean Squared Log Error: 0.024240642536835784\n",
      "Median Absolute Error: 2.422492705412793\n",
      "R^2: 0.9326037725220904\n",
      "Mean Poisson Deviance: 0.9192367878696442\n",
      "Mean Gamma Deviance: 0.025388282134810456\n"
     ]
    }
   ],
   "source": [
    "eval_model(best_random, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "id": "188d1161",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "% Diff Mean Absolute Error (MAE): -57.806760637305885\n",
      "% Diff Mean Squared Error (MSE): -62.05977858504603\n",
      "% Diff Root Mean Squared Error (RMSE): -38.4043658893311\n",
      "% Diff Mean Absolute Percentage Error (MAPE): -67.11627028262086\n",
      "% Diff Explained Variance Score: 13.035359481966768\n",
      "% Diff Max Error: -0.9016004947674356\n",
      "% Diff Mean Squared Log Error: -76.4335590561491\n",
      "% Diff Median Absolute Error: -75.1128911532464\n",
      "% Diff R^2: 13.405495834759188\n",
      "% Diff Mean Poisson Deviance: -66.17569616232566\n",
      "% Diff Mean Gamma Deviance: -68.9610821834799\n"
     ]
    }
   ],
   "source": [
    "eval_2_models(base_model, best_random, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "id": "04f0268b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#great changes on this one, much better effects of using cross validation with GBR than RF\n",
    "#Is this excpected? compare my results here to other findings. How much do these metrics usually change? \n",
    "#What metrics do we care most about given our applications? \n",
    "#How can each model translate to variable importance estimates?\n",
    "#How do the important variables change when we use Machine learning vs. Simple Linear Regression? Why? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d00d6a23",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9b0b4f42",
   "metadata": {},
   "source": [
    "## Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "id": "47a2faa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "id": "ce2defa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df['forest.l']\n",
    "X = df.drop('forest.l', axis =1)\n",
    "\n",
    "X_train_unscaled, X_test_unscaled, y_train, y_test = train_test_split(X, y, test_size = 0.3) #30/70 data split\n",
    "\n",
    "#normalize our explanatory variables\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train_unscaled)\n",
    "X_test = sc.transform(X_test_unscaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "id": "fb10c988",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLPRegressor(max_iter=500)"
      ]
     },
     "execution_count": 290,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp = MLPRegressor(max_iter=500)\n",
    "mlp.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "id": "73da9d25",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = MLPRegressor(max_iter=500)\n",
    "mlp.fit(X_train,y_train)\n",
    "\n",
    "predict_train = mlp.predict(X_train)\n",
    "predict_test = mlp.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb8359cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = mlp.predict(y_test)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d48fb08c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8ffe7ff4",
   "metadata": {},
   "source": [
    "## Variable Importance Things to look into: \n",
    "https://mljar.com/blog/feature-importance-in-random-forest/\n",
    "\n",
    "https://arxiv.org/pdf/1407.7502.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9f97565",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
