{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 547,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mdutils.mdutils import MdUtils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 548,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# Links to Code\\n  \\n[Models and Utils: deforestutils.py](deforestutils.py)  \\n[Notebook for models and analysis: ResultsParallel.ipynb](ResultsParallel.ipynb)  \\n[Notebook for markdown: ResultsMarkdownGenerator.ipynb](ResultsMarkdownGenerator.ipynb)  \\n'"
      ]
     },
     "execution_count": 548,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mdFile = MdUtils(file_name='Results',title='Results')\n",
    "\n",
    "mdFile.new_header(level=1, title=\"Links to Code\")\n",
    "mdFile.new_line()\n",
    "mdFile.write(\"[Models and Utils: deforestutils.py](deforestutils.py)\")\n",
    "mdFile.new_line()\n",
    "mdFile.write(\"[Notebook for models and analysis: ResultsParallel.ipynb](ResultsParallel.ipynb)\")\n",
    "mdFile.new_line()\n",
    "mdFile.write(\"[Notebook for markdown: ResultsMarkdownGenerator.ipynb](ResultsMarkdownGenerator.ipynb)\")\n",
    "mdFile.new_line()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 549,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdFile.new_header(level=1, title=\"Motivating Questions\")\n",
    "\n",
    "items = ['What are important features in explaining variance in deforestation?', \n",
    "         'How do the features change across the time interval 2004-2016?', \n",
    "         'How much information leakage occurrs if spatial dependence is not considered?', \n",
    "         'Do important feature vary accross the region?']\n",
    "\n",
    "mdFile.new_list(items=items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 550,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# Links to Code\\n  \\n[Models and Utils: deforestutils.py](deforestutils.py)  \\n[Notebook for models and analysis: ResultsParallel.ipynb](ResultsParallel.ipynb)  \\n[Notebook for markdown: ResultsMarkdownGenerator.ipynb](ResultsMarkdownGenerator.ipynb)  \\n\\n# Motivating Questions\\n\\n- What are important features in explaining variance in deforestation?\\n- How do the features change across the time interval 2004-2016?\\n- How much information leakage occurrs if spatial dependence is not considered?\\n- Do important feature vary accross the region?\\n\\n# Test Train Split\\n\\n\\nData split 70/30 between Train and Test data sets by municipality where municipalities are selected at random. Splitting by municipality is done to account for the spatial dependence of the data.  \\n![Test/Train split plot](FeatureImportanceResults/2004_2005_2006_PREDICT_2007/EntirePlot.png)  \\nNote that I use this same split for each panel of the sliding window and for each method within each panel.'"
      ]
     },
     "execution_count": 550,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mdFile.new_header(level=1, title='Test Train Split')\n",
    "\n",
    "mdFile.new_paragraph(\"Data split 70/30 between Train and Test data sets by municipality where municipalities are selected at random. \" + \n",
    "                \"Splitting by municipality is done to account for the spatial dependence of the data.\")\n",
    "\n",
    "mdFile.new_line(mdFile.new_inline_image(text='Test/Train split plot', path='FeatureImportanceResults/2004_2005_2006_PREDICT_2007/EntirePlot.png'))\n",
    "\n",
    "mdFile.new_line(\"Note that I use this same split for each panel of the sliding window and for each method within each panel.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 551,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n# Links to Code\\n  \\n[Models and Utils: deforestutils.py](deforestutils.py)  \\n[Notebook for models and analysis: ResultsParallel.ipynb](ResultsParallel.ipynb)  \\n[Notebook for markdown: ResultsMarkdownGenerator.ipynb](ResultsMarkdownGenerator.ipynb)  \\n\\n# Motivating Questions\\n\\n- What are important features in explaining variance in deforestation?\\n- How do the features change across the time interval 2004-2016?\\n- How much information leakage occurrs if spatial dependence is not considered?\\n- Do important feature vary accross the region?\\n\\n# Test Train Split\\n\\n\\nData split 70/30 between Train and Test data sets by municipality where municipalities are selected at random. Splitting by municipality is done to account for the spatial dependence of the data.  \\n![Test/Train split plot](FeatureImportanceResults/2004_2005_2006_PREDICT_2007/EntirePlot.png)  \\nNote that I use this same split for each panel of the sliding window and for each method within each panel.\\n# Models & Fit\\n\\n- Random Forest\\n    - Model: RandomForestRegressor in Sklearn\\n    - Hyperparam: {'model__max_depth': np.arange(3, 11, 8)}\\n    - coefficients: best_model._final_estimator.feature_importances_\\n        - unsigned feature importances are calculated based on the Gini importance or mean decrease impurity method\\n- Lasso\\n    - Model: Lasso in Sklearn\\n    - Hyperparam: {'model__alpha':[1e-15,1e-10,1e-8,1e-3,1e-2,1,5,10,20,30,35,40,45,50,55,100]}\\n    - signed coefficients: best_model.named_steps['model'].coef_\\n        - feature importances calculated as coeffs resulting from iterative coordinate descent optimization algo where each updated one at a time while others held fixed\\n- Gradient Boosting\\n    - Model: GradientBoostingRegressor(learning_rate = 0.1, min_samples_leaf = 2) in Sklearn\\n    - Hyperparam: {'model__max_depth': np.arange(3, 11, 8)}\\n    - coefficients: best_model._final_estimator.feature_importances_\\n        - unsigned feature importances, normalized by design: total reduction in the loss function achieved by splits on a given feature, summed over all trees in the ensemble of decision trees\\n- Neural Network\\n    - Model: MLPRegressor(activation = 'logistic') in Sklearn\\n    - Hyperparam: {'model__hidden_layer_sizes':[(50,),(100,)], 'model__alpha':np.arange(0.00001, 0.001, 0.001)}\\n    - coefficients: shap.KernelExplainer(best_model.predict,shap.sample(X_train, 100), nsamples = 100), explainer.shap_values(shap.sample(X_test, 1000), nsamples=100)\\n        - signed feature importances: shap approximates Shapley values as coefficients of linear regression of perturbed inputs\\n- Super Learner\\n    - Model: I implement by combining the above. predictions from best models from each of the above are input to Ridge() meta learner\\n    - Hyperparam: {'model__max_depth': np.arange(3, 11, 8)}\\n    - coefficients: best_model._final_estimator.feature_importances_\\n        - signed feature importances: mean of feature importance of each input model weighted by importance of each base model in meta model\\n  \\nI use 3 years of data to predict the  following year's **deforest_diff** variable.  \\nParallelized gridsearchCV used to tune hyperparameters of each model.  \\nAll hyperparameters are selected with 5-fold cross validation where folds are selected on municipality level to control for spatial dependence.  \\nStandardScaler transform applied\""
      ]
     },
     "execution_count": 551,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "mdFile.new_header(level=1, title=\"Models & Fit\")\n",
    "items = [\"Random Forest\", \n",
    "            [\"Model: RandomForestRegressor in Sklearn\" , \"Hyperparam: {'model__max_depth': np.arange(3, 11, 8)}\", \"coefficients: best_model._final_estimator.feature_importances_\", [\"unsigned feature importances are calculated based on the Gini importance or mean decrease impurity method\"]],\n",
    "         \"Lasso\", \n",
    "            [\"Model: Lasso in Sklearn\" , \"Hyperparam: {'model__alpha':[1e-15,1e-10,1e-8,1e-3,1e-2,1,5,10,20,30,35,40,45,50,55,100]}\", \"signed coefficients: best_model.named_steps['model'].coef_\", \n",
    "               [\"feature importances calculated as coeffs resulting from iterative coordinate descent optimization algo where each updated one at a time while others held fixed\"]],\n",
    "         \"Gradient Boosting\", \n",
    "            [\"Model: GradientBoostingRegressor(learning_rate = 0.1, min_samples_leaf = 2) in Sklearn\" , \"Hyperparam: {'model__max_depth': np.arange(3, 11, 8)}\", \"coefficients: best_model._final_estimator.feature_importances_\", [\"unsigned feature importances, normalized by design: total reduction in the loss function achieved by splits on a given feature, summed over all trees in the ensemble of decision trees\"]],\n",
    "         \"Neural Network\", \n",
    "            [\"Model: MLPRegressor(activation = 'logistic') in Sklearn\" , \"Hyperparam: {'model__hidden_layer_sizes':[(50,),(100,)], 'model__alpha':np.arange(0.00001, 0.001, 0.001)}\", \"coefficients: shap.KernelExplainer(best_model.predict,shap.sample(X_train, 100), nsamples = 100), explainer.shap_values(shap.sample(X_test, 1000), nsamples=100)\", [\"signed feature importances: shap approximates Shapley values as coefficients of linear regression of perturbed inputs\"]],\n",
    "         \"Super Learner\", \n",
    "            [\"Model: I implement by combining the above. predictions from best models from each of the above are input to Ridge() meta learner\" , \"Hyperparam: {'model__max_depth': np.arange(3, 11, 8)}\", \"coefficients: best_model._final_estimator.feature_importances_\", [\"signed feature importances: mean of feature importance of each input model weighted by importance of each base model in meta model\"]]\n",
    "            ]\n",
    "\n",
    "mdFile.new_list(items=items)\n",
    "\n",
    "mdFile.new_line(\"I use 3 years of data to predict the  following year's **deforest_diff** variable.\")\n",
    "mdFile.new_line(\"Parallelized gridsearchCV used to tune hyperparameters of each model.\")\n",
    "mdFile.new_line(\"All hyperparameters are selected with 5-fold cross validation where folds are selected on municipality level to control for spatial dependence.\")\n",
    "mdFile.new_line(\"StandardScaler transform applied\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 552,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n# Links to Code\\n  \\n[Models and Utils: deforestutils.py](deforestutils.py)  \\n[Notebook for models and analysis: ResultsParallel.ipynb](ResultsParallel.ipynb)  \\n[Notebook for markdown: ResultsMarkdownGenerator.ipynb](ResultsMarkdownGenerator.ipynb)  \\n\\n# Motivating Questions\\n\\n- What are important features in explaining variance in deforestation?\\n- How do the features change across the time interval 2004-2016?\\n- How much information leakage occurrs if spatial dependence is not considered?\\n- Do important feature vary accross the region?\\n\\n# Test Train Split\\n\\n\\nData split 70/30 between Train and Test data sets by municipality where municipalities are selected at random. Splitting by municipality is done to account for the spatial dependence of the data.  \\n![Test/Train split plot](FeatureImportanceResults/2004_2005_2006_PREDICT_2007/EntirePlot.png)  \\nNote that I use this same split for each panel of the sliding window and for each method within each panel.\\n# Models & Fit\\n\\n- Random Forest\\n    - Model: RandomForestRegressor in Sklearn\\n    - Hyperparam: {'model__max_depth': np.arange(3, 11, 8)}\\n    - coefficients: best_model._final_estimator.feature_importances_\\n        - unsigned feature importances are calculated based on the Gini importance or mean decrease impurity method\\n- Lasso\\n    - Model: Lasso in Sklearn\\n    - Hyperparam: {'model__alpha':[1e-15,1e-10,1e-8,1e-3,1e-2,1,5,10,20,30,35,40,45,50,55,100]}\\n    - signed coefficients: best_model.named_steps['model'].coef_\\n        - feature importances calculated as coeffs resulting from iterative coordinate descent optimization algo where each updated one at a time while others held fixed\\n- Gradient Boosting\\n    - Model: GradientBoostingRegressor(learning_rate = 0.1, min_samples_leaf = 2) in Sklearn\\n    - Hyperparam: {'model__max_depth': np.arange(3, 11, 8)}\\n    - coefficients: best_model._final_estimator.feature_importances_\\n        - unsigned feature importances, normalized by design: total reduction in the loss function achieved by splits on a given feature, summed over all trees in the ensemble of decision trees\\n- Neural Network\\n    - Model: MLPRegressor(activation = 'logistic') in Sklearn\\n    - Hyperparam: {'model__hidden_layer_sizes':[(50,),(100,)], 'model__alpha':np.arange(0.00001, 0.001, 0.001)}\\n    - coefficients: shap.KernelExplainer(best_model.predict,shap.sample(X_train, 100), nsamples = 100), explainer.shap_values(shap.sample(X_test, 1000), nsamples=100)\\n        - signed feature importances: shap approximates Shapley values as coefficients of linear regression of perturbed inputs\\n- Super Learner\\n    - Model: I implement by combining the above. predictions from best models from each of the above are input to Ridge() meta learner\\n    - Hyperparam: {'model__max_depth': np.arange(3, 11, 8)}\\n    - coefficients: best_model._final_estimator.feature_importances_\\n        - signed feature importances: mean of feature importance of each input model weighted by importance of each base model in meta model\\n  \\nI use 3 years of data to predict the  following year's **deforest_diff** variable.  \\nParallelized gridsearchCV used to tune hyperparameters of each model.  \\nAll hyperparameters are selected with 5-fold cross validation where folds are selected on municipality level to control for spatial dependence.  \\nStandardScaler transform applied\\n# Feature Importance\\n  \\nFeature importance scores for each method are normalized to sum to 1 here to understand their relative differences. Absolute vals are taken when necessary since some methods return signed feature importance and some don't.\""
      ]
     },
     "execution_count": 552,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mdFile.new_header(level=1, title='Feature Importance')\n",
    "\n",
    "mdFile.new_line(\"Feature importance scores for each method are normalized to sum to 1 here to understand their relative differences. Absolute vals are taken when necessary since some methods return signed feature importance and some don't.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 553,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, this_start_year in enumerate([2004, 2005, 2006, 2007, 2008, 2009, 2010, 2011, 2012, 2013]):\n",
    "\tSTART_YEAR_TRAIN = this_start_year\n",
    "\tNUMBER_YEARS_TRAIN = 3\n",
    "\tYEARS_TO_TRAIN = [START_YEAR_TRAIN + i  for i in range(NUMBER_YEARS_TRAIN + 1)]\n",
    "\tPREDICT_YEAR = START_YEAR_TRAIN + NUMBER_YEARS_TRAIN\n",
    "\tFOLDER_NAME = ''.join([f'{START_YEAR_TRAIN + i}_' for i in list(range(NUMBER_YEARS_TRAIN))]) + f'PREDICT_{PREDICT_YEAR}'\n",
    "\tmdFile.new_line(mdFile.new_inline_image(text='', path=f'FeatureImportanceResults/{FOLDER_NAME}/FeatureImportance/features_all.png'))\n",
    "\n",
    "        \n",
    "mdFile.new_header(level=2, title='Feature Importance without the **forest** variables.')\n",
    "mdFile.new_line(\"'''forest_lag''' and '''forest_formation''' are both directly proportional to the response variable forest_diff, so it was expected that they would dominate the important features consistently.\")\n",
    "mdFile.new_line('I repeat the above plots but exclude forest vars so others are visible.')\n",
    "\n",
    "for this_start_year in [2004, 2005, 2006, 2007, 2008, 2009, 2010, 2011, 2012, 2013]:\n",
    "\tSTART_YEAR_TRAIN = this_start_year\n",
    "\tNUMBER_YEARS_TRAIN = 3\n",
    "\tYEARS_TO_TRAIN = [START_YEAR_TRAIN + i  for i in range(NUMBER_YEARS_TRAIN + 1)]\n",
    "\tPREDICT_YEAR = START_YEAR_TRAIN + NUMBER_YEARS_TRAIN\n",
    "\tFOLDER_NAME = ''.join([f'{START_YEAR_TRAIN + i}_' for i in list(range(NUMBER_YEARS_TRAIN))]) + f'PREDICT_{PREDICT_YEAR}'\n",
    "\tmdFile.new_line(mdFile.new_inline_image(text='', path=f'FeatureImportanceResults/{FOLDER_NAME}/FeatureImportance/features_all_forest_exclude.png'))\n",
    "\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 554,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdFile.new_header(level=1, title='Predicted Deforestation')\n",
    "\n",
    "\n",
    "for this_start_year in [2004, 2005, 2006, 2007, 2008, 2009, 2010, 2011, 2012, 2013]:\n",
    "\tSTART_YEAR_TRAIN = this_start_year\n",
    "\tNUMBER_YEARS_TRAIN = 3\n",
    "\tYEARS_TO_TRAIN = [START_YEAR_TRAIN + i  for i in range(NUMBER_YEARS_TRAIN + 1)]\n",
    "\tPREDICT_YEAR = START_YEAR_TRAIN + NUMBER_YEARS_TRAIN\n",
    "\tFOLDER_NAME = ''.join([f'{START_YEAR_TRAIN + i}_' for i in list(range(NUMBER_YEARS_TRAIN))]) + f'PREDICT_{PREDICT_YEAR}'\n",
    "\tmdFile.new_line(mdFile.new_inline_image(text='', path=f'FeatureImportanceResults/{FOLDER_NAME}/DeforestPlot_all.png'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 555,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n# Links to Code\\n  \\n[Models and Utils: deforestutils.py](deforestutils.py)  \\n[Notebook for models and analysis: ResultsParallel.ipynb](ResultsParallel.ipynb)  \\n[Notebook for markdown: ResultsMarkdownGenerator.ipynb](ResultsMarkdownGenerator.ipynb)  \\n\\n# Motivating Questions\\n\\n- What are important features in explaining variance in deforestation?\\n- How do the features change across the time interval 2004-2016?\\n- How much information leakage occurrs if spatial dependence is not considered?\\n- Do important feature vary accross the region?\\n\\n# Test Train Split\\n\\n\\nData split 70/30 between Train and Test data sets by municipality where municipalities are selected at random. Splitting by municipality is done to account for the spatial dependence of the data.  \\n![Test/Train split plot](FeatureImportanceResults/2004_2005_2006_PREDICT_2007/EntirePlot.png)  \\nNote that I use this same split for each panel of the sliding window and for each method within each panel.\\n# Models & Fit\\n\\n- Random Forest\\n    - Model: RandomForestRegressor in Sklearn\\n    - Hyperparam: {'model__max_depth': np.arange(3, 11, 8)}\\n    - coefficients: best_model._final_estimator.feature_importances_\\n        - unsigned feature importances are calculated based on the Gini importance or mean decrease impurity method\\n- Lasso\\n    - Model: Lasso in Sklearn\\n    - Hyperparam: {'model__alpha':[1e-15,1e-10,1e-8,1e-3,1e-2,1,5,10,20,30,35,40,45,50,55,100]}\\n    - signed coefficients: best_model.named_steps['model'].coef_\\n        - feature importances calculated as coeffs resulting from iterative coordinate descent optimization algo where each updated one at a time while others held fixed\\n- Gradient Boosting\\n    - Model: GradientBoostingRegressor(learning_rate = 0.1, min_samples_leaf = 2) in Sklearn\\n    - Hyperparam: {'model__max_depth': np.arange(3, 11, 8)}\\n    - coefficients: best_model._final_estimator.feature_importances_\\n        - unsigned feature importances, normalized by design: total reduction in the loss function achieved by splits on a given feature, summed over all trees in the ensemble of decision trees\\n- Neural Network\\n    - Model: MLPRegressor(activation = 'logistic') in Sklearn\\n    - Hyperparam: {'model__hidden_layer_sizes':[(50,),(100,)], 'model__alpha':np.arange(0.00001, 0.001, 0.001)}\\n    - coefficients: shap.KernelExplainer(best_model.predict,shap.sample(X_train, 100), nsamples = 100), explainer.shap_values(shap.sample(X_test, 1000), nsamples=100)\\n        - signed feature importances: shap approximates Shapley values as coefficients of linear regression of perturbed inputs\\n- Super Learner\\n    - Model: I implement by combining the above. predictions from best models from each of the above are input to Ridge() meta learner\\n    - Hyperparam: {'model__max_depth': np.arange(3, 11, 8)}\\n    - coefficients: best_model._final_estimator.feature_importances_\\n        - signed feature importances: mean of feature importance of each input model weighted by importance of each base model in meta model\\n  \\nI use 3 years of data to predict the  following year's **deforest_diff** variable.  \\nParallelized gridsearchCV used to tune hyperparameters of each model.  \\nAll hyperparameters are selected with 5-fold cross validation where folds are selected on municipality level to control for spatial dependence.  \\nStandardScaler transform applied\\n# Feature Importance\\n  \\nFeature importance scores for each method are normalized to sum to 1 here to understand their relative differences. Absolute vals are taken when necessary since some methods return signed feature importance and some don't.  \\n![](FeatureImportanceResults/2004_2005_2006_PREDICT_2007/FeatureImportance/features_all.png)  \\n![](FeatureImportanceResults/2005_2006_2007_PREDICT_2008/FeatureImportance/features_all.png)  \\n![](FeatureImportanceResults/2006_2007_2008_PREDICT_2009/FeatureImportance/features_all.png)  \\n![](FeatureImportanceResults/2007_2008_2009_PREDICT_2010/FeatureImportance/features_all.png)  \\n![](FeatureImportanceResults/2008_2009_2010_PREDICT_2011/FeatureImportance/features_all.png)  \\n![](FeatureImportanceResults/2009_2010_2011_PREDICT_2012/FeatureImportance/features_all.png)  \\n![](FeatureImportanceResults/2010_2011_2012_PREDICT_2013/FeatureImportance/features_all.png)  \\n![](FeatureImportanceResults/2011_2012_2013_PREDICT_2014/FeatureImportance/features_all.png)  \\n![](FeatureImportanceResults/2012_2013_2014_PREDICT_2015/FeatureImportance/features_all.png)  \\n![](FeatureImportanceResults/2013_2014_2015_PREDICT_2016/FeatureImportance/features_all.png)\\n## Feature Importance without the **forest** variables.\\n  \\n'''forest_lag''' and '''forest_formation''' are both directly proportional to the response variable forest_diff, so it was expected that they would dominate the important features consistently.  \\nI repeat the above plots but exclude forest vars so others are visible.  \\n![](FeatureImportanceResults/2004_2005_2006_PREDICT_2007/FeatureImportance/features_all_forest_exclude.png)  \\n![](FeatureImportanceResults/2005_2006_2007_PREDICT_2008/FeatureImportance/features_all_forest_exclude.png)  \\n![](FeatureImportanceResults/2006_2007_2008_PREDICT_2009/FeatureImportance/features_all_forest_exclude.png)  \\n![](FeatureImportanceResults/2007_2008_2009_PREDICT_2010/FeatureImportance/features_all_forest_exclude.png)  \\n![](FeatureImportanceResults/2008_2009_2010_PREDICT_2011/FeatureImportance/features_all_forest_exclude.png)  \\n![](FeatureImportanceResults/2009_2010_2011_PREDICT_2012/FeatureImportance/features_all_forest_exclude.png)  \\n![](FeatureImportanceResults/2010_2011_2012_PREDICT_2013/FeatureImportance/features_all_forest_exclude.png)  \\n![](FeatureImportanceResults/2011_2012_2013_PREDICT_2014/FeatureImportance/features_all_forest_exclude.png)  \\n![](FeatureImportanceResults/2012_2013_2014_PREDICT_2015/FeatureImportance/features_all_forest_exclude.png)  \\n![](FeatureImportanceResults/2013_2014_2015_PREDICT_2016/FeatureImportance/features_all_forest_exclude.png)\\n# Predicted Deforestation\\n  \\n![](FeatureImportanceResults/2004_2005_2006_PREDICT_2007/DeforestPlot_all.png)  \\n![](FeatureImportanceResults/2005_2006_2007_PREDICT_2008/DeforestPlot_all.png)  \\n![](FeatureImportanceResults/2006_2007_2008_PREDICT_2009/DeforestPlot_all.png)  \\n![](FeatureImportanceResults/2007_2008_2009_PREDICT_2010/DeforestPlot_all.png)  \\n![](FeatureImportanceResults/2008_2009_2010_PREDICT_2011/DeforestPlot_all.png)  \\n![](FeatureImportanceResults/2009_2010_2011_PREDICT_2012/DeforestPlot_all.png)  \\n![](FeatureImportanceResults/2010_2011_2012_PREDICT_2013/DeforestPlot_all.png)  \\n![](FeatureImportanceResults/2011_2012_2013_PREDICT_2014/DeforestPlot_all.png)  \\n![](FeatureImportanceResults/2012_2013_2014_PREDICT_2015/DeforestPlot_all.png)  \\n![](FeatureImportanceResults/2013_2014_2015_PREDICT_2016/DeforestPlot_all.png)\\n# MSE\\n  \\n![Average of all methods](FeatureImportanceResults/MSE.png)\""
      ]
     },
     "execution_count": 555,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mdFile.new_header(level=1, title='MSE')\n",
    "mdFile.new_line(mdFile.new_inline_image(text='Average of all methods', path='FeatureImportanceResults/MSE.png'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 556,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n# Links to Code\\n  \\n[Models and Utils: deforestutils.py](deforestutils.py)  \\n[Notebook for models and analysis: ResultsParallel.ipynb](ResultsParallel.ipynb)  \\n[Notebook for markdown: ResultsMarkdownGenerator.ipynb](ResultsMarkdownGenerator.ipynb)  \\n\\n# Motivating Questions\\n\\n- What are important features in explaining variance in deforestation?\\n- How do the features change across the time interval 2004-2016?\\n- How much information leakage occurrs if spatial dependence is not considered?\\n- Do important feature vary accross the region?\\n\\n# Test Train Split\\n\\n\\nData split 70/30 between Train and Test data sets by municipality where municipalities are selected at random. Splitting by municipality is done to account for the spatial dependence of the data.  \\n![Test/Train split plot](FeatureImportanceResults/2004_2005_2006_PREDICT_2007/EntirePlot.png)  \\nNote that I use this same split for each panel of the sliding window and for each method within each panel.\\n# Models & Fit\\n\\n- Random Forest\\n    - Model: RandomForestRegressor in Sklearn\\n    - Hyperparam: {'model__max_depth': np.arange(3, 11, 8)}\\n    - coefficients: best_model._final_estimator.feature_importances_\\n        - unsigned feature importances are calculated based on the Gini importance or mean decrease impurity method\\n- Lasso\\n    - Model: Lasso in Sklearn\\n    - Hyperparam: {'model__alpha':[1e-15,1e-10,1e-8,1e-3,1e-2,1,5,10,20,30,35,40,45,50,55,100]}\\n    - signed coefficients: best_model.named_steps['model'].coef_\\n        - feature importances calculated as coeffs resulting from iterative coordinate descent optimization algo where each updated one at a time while others held fixed\\n- Gradient Boosting\\n    - Model: GradientBoostingRegressor(learning_rate = 0.1, min_samples_leaf = 2) in Sklearn\\n    - Hyperparam: {'model__max_depth': np.arange(3, 11, 8)}\\n    - coefficients: best_model._final_estimator.feature_importances_\\n        - unsigned feature importances, normalized by design: total reduction in the loss function achieved by splits on a given feature, summed over all trees in the ensemble of decision trees\\n- Neural Network\\n    - Model: MLPRegressor(activation = 'logistic') in Sklearn\\n    - Hyperparam: {'model__hidden_layer_sizes':[(50,),(100,)], 'model__alpha':np.arange(0.00001, 0.001, 0.001)}\\n    - coefficients: shap.KernelExplainer(best_model.predict,shap.sample(X_train, 100), nsamples = 100), explainer.shap_values(shap.sample(X_test, 1000), nsamples=100)\\n        - signed feature importances: shap approximates Shapley values as coefficients of linear regression of perturbed inputs\\n- Super Learner\\n    - Model: I implement by combining the above. predictions from best models from each of the above are input to Ridge() meta learner\\n    - Hyperparam: {'model__max_depth': np.arange(3, 11, 8)}\\n    - coefficients: best_model._final_estimator.feature_importances_\\n        - signed feature importances: mean of feature importance of each input model weighted by importance of each base model in meta model\\n  \\nI use 3 years of data to predict the  following year's **deforest_diff** variable.  \\nParallelized gridsearchCV used to tune hyperparameters of each model.  \\nAll hyperparameters are selected with 5-fold cross validation where folds are selected on municipality level to control for spatial dependence.  \\nStandardScaler transform applied\\n# Feature Importance\\n  \\nFeature importance scores for each method are normalized to sum to 1 here to understand their relative differences. Absolute vals are taken when necessary since some methods return signed feature importance and some don't.  \\n![](FeatureImportanceResults/2004_2005_2006_PREDICT_2007/FeatureImportance/features_all.png)  \\n![](FeatureImportanceResults/2005_2006_2007_PREDICT_2008/FeatureImportance/features_all.png)  \\n![](FeatureImportanceResults/2006_2007_2008_PREDICT_2009/FeatureImportance/features_all.png)  \\n![](FeatureImportanceResults/2007_2008_2009_PREDICT_2010/FeatureImportance/features_all.png)  \\n![](FeatureImportanceResults/2008_2009_2010_PREDICT_2011/FeatureImportance/features_all.png)  \\n![](FeatureImportanceResults/2009_2010_2011_PREDICT_2012/FeatureImportance/features_all.png)  \\n![](FeatureImportanceResults/2010_2011_2012_PREDICT_2013/FeatureImportance/features_all.png)  \\n![](FeatureImportanceResults/2011_2012_2013_PREDICT_2014/FeatureImportance/features_all.png)  \\n![](FeatureImportanceResults/2012_2013_2014_PREDICT_2015/FeatureImportance/features_all.png)  \\n![](FeatureImportanceResults/2013_2014_2015_PREDICT_2016/FeatureImportance/features_all.png)\\n## Feature Importance without the **forest** variables.\\n  \\n'''forest_lag''' and '''forest_formation''' are both directly proportional to the response variable forest_diff, so it was expected that they would dominate the important features consistently.  \\nI repeat the above plots but exclude forest vars so others are visible.  \\n![](FeatureImportanceResults/2004_2005_2006_PREDICT_2007/FeatureImportance/features_all_forest_exclude.png)  \\n![](FeatureImportanceResults/2005_2006_2007_PREDICT_2008/FeatureImportance/features_all_forest_exclude.png)  \\n![](FeatureImportanceResults/2006_2007_2008_PREDICT_2009/FeatureImportance/features_all_forest_exclude.png)  \\n![](FeatureImportanceResults/2007_2008_2009_PREDICT_2010/FeatureImportance/features_all_forest_exclude.png)  \\n![](FeatureImportanceResults/2008_2009_2010_PREDICT_2011/FeatureImportance/features_all_forest_exclude.png)  \\n![](FeatureImportanceResults/2009_2010_2011_PREDICT_2012/FeatureImportance/features_all_forest_exclude.png)  \\n![](FeatureImportanceResults/2010_2011_2012_PREDICT_2013/FeatureImportance/features_all_forest_exclude.png)  \\n![](FeatureImportanceResults/2011_2012_2013_PREDICT_2014/FeatureImportance/features_all_forest_exclude.png)  \\n![](FeatureImportanceResults/2012_2013_2014_PREDICT_2015/FeatureImportance/features_all_forest_exclude.png)  \\n![](FeatureImportanceResults/2013_2014_2015_PREDICT_2016/FeatureImportance/features_all_forest_exclude.png)\\n# Predicted Deforestation\\n  \\n![](FeatureImportanceResults/2004_2005_2006_PREDICT_2007/DeforestPlot_all.png)  \\n![](FeatureImportanceResults/2005_2006_2007_PREDICT_2008/DeforestPlot_all.png)  \\n![](FeatureImportanceResults/2006_2007_2008_PREDICT_2009/DeforestPlot_all.png)  \\n![](FeatureImportanceResults/2007_2008_2009_PREDICT_2010/DeforestPlot_all.png)  \\n![](FeatureImportanceResults/2008_2009_2010_PREDICT_2011/DeforestPlot_all.png)  \\n![](FeatureImportanceResults/2009_2010_2011_PREDICT_2012/DeforestPlot_all.png)  \\n![](FeatureImportanceResults/2010_2011_2012_PREDICT_2013/DeforestPlot_all.png)  \\n![](FeatureImportanceResults/2011_2012_2013_PREDICT_2014/DeforestPlot_all.png)  \\n![](FeatureImportanceResults/2012_2013_2014_PREDICT_2015/DeforestPlot_all.png)  \\n![](FeatureImportanceResults/2013_2014_2015_PREDICT_2016/DeforestPlot_all.png)\\n# MSE\\n  \\n![Average of all methods](FeatureImportanceResults/MSE.png)\\n# Feature Importance Changing in Time\\n  \\n![Average of all methods](FeatureImportanceResults/Evolution/evolution_exclude_forest_avg.png)  \\n![Random Forest](FeatureImportanceResults/Evolution/evolution_exclude_forest_randomforest.png)  \\n![Lasso](FeatureImportanceResults/Evolution/evolution_exclude_forest_lasso.png)  \\n![Gradient Boosting](FeatureImportanceResults/Evolution/evolution_exclude_forest_gradientboosting.png)  \\n![Neural Network](FeatureImportanceResults/Evolution/evolution_exclude_forest_neuralnetwork.png)  \\n![Superlearner](FeatureImportanceResults/Evolution/evolution_exclude_forest_superlearner.png)\""
      ]
     },
     "execution_count": 556,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mdFile.new_header(level=1, title='Feature Importance Changing in Time')\n",
    "\n",
    "mdFile.new_line(mdFile.new_inline_image(text='Average of all methods', path='FeatureImportanceResults/Evolution/evolution_exclude_forest_avg.png'))\n",
    "mdFile.new_line(mdFile.new_inline_image(text='Random Forest', path='FeatureImportanceResults/Evolution/evolution_exclude_forest_randomforest.png'))\n",
    "mdFile.new_line(mdFile.new_inline_image(text='Lasso', path='FeatureImportanceResults/Evolution/evolution_exclude_forest_lasso.png'))\n",
    "mdFile.new_line(mdFile.new_inline_image(text='Gradient Boosting', path='FeatureImportanceResults/Evolution/evolution_exclude_forest_gradientboosting.png'))\n",
    "mdFile.new_line(mdFile.new_inline_image(text='Neural Network', path='FeatureImportanceResults/Evolution/evolution_exclude_forest_neuralnetwork.png'))\n",
    "mdFile.new_line(mdFile.new_inline_image(text='Superlearner', path='FeatureImportanceResults/Evolution/evolution_exclude_forest_superlearner.png'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 557,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdFile.new_header(level=1, title=\"Remaining Questions\")\n",
    "\n",
    "items = ['Should I keep the deforest variables in? ', \n",
    "         'Should I run again with random cross validation and see if the mse changes?', \n",
    "         'Should I do the same analysis with a sliding window to see if important features change over the area?', \n",
    "         'Should I exclude year?', \n",
    "         'Should I use the same test/train split for all fits?']\n",
    "\n",
    "mdFile.new_list(items=items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 558,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdFile.new_header(level=1, title='Appendix: Bigger Feature Importance Plots')\n",
    "\n",
    "for i, this_start_year in enumerate([2004, 2005, 2006, 2007, 2008, 2009, 2010, 2011, 2012, 2013]):\n",
    "    START_YEAR_TRAIN = this_start_year\n",
    "    NUMBER_YEARS_TRAIN = 3\n",
    "    YEARS_TO_TRAIN = [START_YEAR_TRAIN + i  for i in range(NUMBER_YEARS_TRAIN + 1)]\n",
    "    PREDICT_YEAR = START_YEAR_TRAIN + NUMBER_YEARS_TRAIN\n",
    "    FOLDER_NAME = ''.join([f'{START_YEAR_TRAIN + i}_' for i in list(range(NUMBER_YEARS_TRAIN))]) + f'PREDICT_{PREDICT_YEAR}'\n",
    "    mdFile.new_header(level=2, title=f'{FOLDER_NAME} feature importances')\n",
    "    mdFile.new_line(mdFile.new_inline_image(text='Random Forest', path=f'FeatureImportanceResults/{FOLDER_NAME}/FeatureImportance/features_randomforest.png'))\n",
    "    mdFile.new_line(mdFile.new_inline_image(text='Lasso', path=f'FeatureImportanceResults/{FOLDER_NAME}/FeatureImportance/features_lasso.png'))\n",
    "    mdFile.new_line(mdFile.new_inline_image(text='GradientBoosting', path=f'FeatureImportanceResults/{FOLDER_NAME}/FeatureImportance/features_gradientboosting.png'))\n",
    "    mdFile.new_line(mdFile.new_inline_image(text='NeuralNetwork', path=f'FeatureImportanceResults/{FOLDER_NAME}/FeatureImportance/features_neuralnetwork.png'))\n",
    "    mdFile.new_line(mdFile.new_inline_image(text='SuperLearner', path=f'FeatureImportanceResults/{FOLDER_NAME}/FeatureImportance/features_superlearner.png'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 559,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdFile.new_header(level=1, title='Appendix: All Features Input')\n",
    "feature_list = ['year', 'rain1', 'elevation', 'slope', 'aspect', 'near_mines',\n",
    "        'near_roads', 'near_hidrovia', 'indigenous_homol',\n",
    "        'mun_election_year', 'new_forest_code', 'lula', 'dilma', 'temer',\n",
    "        'bolsonaro', 'fed_election_year', 'populacao', 'pib_pc', 'ironore',\n",
    "        'silver', 'copper', 'gold', 'soy_price', 'beef_price', 'ag_jobs',\n",
    "        'mining_jobs', 'public_jobs', 'construction_jobs', 'PIB',\n",
    "        'n_companies_PUBLIC ADMIN', 'n_companies_AGRICULTURE',\n",
    "        'n_companies_FOOD AND DRINKS', 'n_companies_ACCOMODATION AND FOOD',\n",
    "        'n_companies_EQUIPMENT RENTAL', 'n_companies_WHOLESALE',\n",
    "        'n_companies_ASSOCIATIVE ACTIVITIES',\n",
    "        'n_companies_AUTOMOBILES AND TRANSPORT',\n",
    "        'n_companies_FINANCIAL ASSISTANCE',\n",
    "        'n_companies_TRADE REP VEHICLES', 'n_companies_CONSTRUCTION',\n",
    "        'n_companies_MAIL AND TELECOM', 'n_companies_CULTURE AND SPORT',\n",
    "        'n_companies_EDITING AND PRINTING', 'n_companies_EDUCATION',\n",
    "        'n_companies_ELECTRICITY AND GAS', 'n_companies_FINANCES',\n",
    "        'n_companies_CLEANING AND SEWAGE', 'n_companies_MACHINERY',\n",
    "        'n_companies_BASIC METALLURGY', 'n_companies_MINING',\n",
    "        'n_companies_WOOD PROD',\n",
    "        'n_companies_NON-METALLIC MINERAL PRODUCTS', 'n_companies_HEALTH',\n",
    "        'n_companies_SERVICES FOR COMPANIES',\n",
    "        'n_companies_PERSONAL SERVICES', 'n_companies_TRANSPORTATION',\n",
    "        'n_companies_GROUND TRANSPORT',\n",
    "        'n_companies_WATER TREATMENT AND DISTRIBUTION',\n",
    "        'n_companies_RETAIL', 'n_companies_COMPUTING',\n",
    "        'n_companies_INSURANCE AND SOCIAL SECURITY',\n",
    "        'n_companies_METALLIC PRODUCTS', 'n_companies_DOMESTIC SERVICES',\n",
    "        'n_companies_FORESTRY', 'n_companies_CLOTHING',\n",
    "        'n_companies_PAPER', 'n_companies_INTERNATIONAL BODIES',\n",
    "        'n_companies_OIL AND GAS', 'n_companies_FISHING AND AQUACULTURE',\n",
    "        'n_companies_CHEMICALS', 'n_companies_WATER-BASED TRANSPORTATION',\n",
    "        'n_companies_REAL ESTATE', 'n_companies_RECYCLING',\n",
    "        'n_companies_LEATHERS AND FOOTWEAR',\n",
    "        'n_companies_RUBBER AND PLASTIC', 'n_companies_TEXTILES',\n",
    "        'n_companies_RESEARCH AND DEVELOPMENT',\n",
    "        'n_companies_AERO TRANSPORT', 'n_companies_SMOKE',\n",
    "        'n_companies_PETROLEUM REFINING', 'n_companies_',\n",
    "        'n_jobs_PUBLIC ADMIN', 'n_jobs_AGRICULTURE',\n",
    "        'n_jobs_FOOD AND DRINKS', 'n_jobs_ACCOMODATION AND FOOD',\n",
    "        'n_jobs_EQUIPMENT RENTAL', 'n_jobs_WHOLESALE',\n",
    "        'n_jobs_ASSOCIATIVE ACTIVITIES',\n",
    "        'n_jobs_AUTOMOBILES AND TRANSPORT', 'n_jobs_FINANCIAL ASSISTANCE',\n",
    "        'n_jobs_TRADE REP VEHICLES', 'n_jobs_CONSTRUCTION',\n",
    "        'n_jobs_MAIL AND TELECOM', 'n_jobs_CULTURE AND SPORT',\n",
    "        'n_jobs_EDITING AND PRINTING', 'n_jobs_EDUCATION',\n",
    "        'n_jobs_ELECTRICITY AND GAS', 'n_jobs_FINANCES',\n",
    "        'n_jobs_CLEANING AND SEWAGE', 'n_jobs_MACHINERY',\n",
    "        'n_jobs_BASIC METALLURGY', 'n_jobs_MINING', 'n_jobs_WOOD PROD',\n",
    "        'n_jobs_NON-METALLIC MINERAL PRODUCTS', 'n_jobs_HEALTH',\n",
    "        'n_jobs_SERVICES FOR COMPANIES', 'n_jobs_PERSONAL SERVICES',\n",
    "        'n_jobs_TRANSPORTATION', 'n_jobs_GROUND TRANSPORT',\n",
    "        'n_jobs_WATER TREATMENT AND DISTRIBUTION', 'n_jobs_RETAIL',\n",
    "        'n_jobs_COMPUTING', 'n_jobs_INSURANCE AND SOCIAL SECURITY',\n",
    "        'n_jobs_METALLIC PRODUCTS', 'n_jobs_DOMESTIC SERVICES',\n",
    "        'n_jobs_FORESTRY', 'n_jobs_CLOTHING', 'n_jobs_PAPER',\n",
    "        'n_jobs_INTERNATIONAL BODIES', 'n_jobs_OIL AND GAS',\n",
    "        'n_jobs_FISHING AND AQUACULTURE', 'n_jobs_CHEMICALS',\n",
    "        'n_jobs_WATER-BASED TRANSPORTATION', 'n_jobs_REAL ESTATE',\n",
    "        'n_jobs_RECYCLING', 'n_jobs_LEATHERS AND FOOTWEAR',\n",
    "        'n_jobs_RUBBER AND PLASTIC', 'n_jobs_TEXTILES',\n",
    "        'n_jobs_RESEARCH AND DEVELOPMENT', 'n_jobs_AERO TRANSPORT',\n",
    "        'n_jobs_SMOKE', 'n_jobs_PETROLEUM REFINING', 'n_jobs_',\n",
    "        'n_jobs_TOTAL INDUSTRIAL', 'n_jobs_TOTAL SERVICE',\n",
    "        'n_companies_TOTAL INDUSTRIAL', 'n_companies_TOTAL SERVICE',\n",
    "        'n_companies_TOTAL', 'n_jobs_TOTAL', 'murder_threats',\n",
    "        'assassination', 'assassination_attempt', 'f_emitted_count',\n",
    "        'expen_agri', 'expen_env_man', 'expen_agr_org', 'expen_mining',\n",
    "        'expen_petrol', 'expen_prom_ani_pro', 'expen_prom_veg_pro',\n",
    "        'expen_other_agr', 'expen_agr_defense', 'expen_min_fuel',\n",
    "        'illegal_mining', 'illegal_other', 'illegal_industry', 'audits',\n",
    "        'emiss_pec_full', 'emiss_agr_full', 'emiss_agropec_full',\n",
    "        'incumbant', 'term_limited_seat', 'special',\n",
    "        'overall_winner_complete_college', \n",
    "        'overall_winner_feminino', 'overall_winner_agriculture_job',\n",
    "        'overall_winner_public_service_job', 'overall_winner_health_job',\n",
    "        'overall_winner_corporate_job', 'overall_winner_law_job',\n",
    "        'overall_winner_technical_job', 'overall_winner_professional_job',\n",
    "        'overall_winner_mining_job', 'overall_winner_partido_PT',\n",
    "        'overall_winner_partido_PMDB_MDB', 'overall_winner_partido_PSDB',\n",
    "        'overall_winner_partido_DEM', 'overall_winner_partido_PL',\n",
    "        'overall_winner_partido_other', 'runnerup_partido_PT',\n",
    "        'runnerup_partido_PMDB_MDB', 'runnerup_partido_PSDB',\n",
    "        'runnerup_partido_DEM', 'runnerup_partido_PL',\n",
    "        'runnerup_partido_other', 'winner_votes_proportion',\n",
    "        'vote_participation_proportion',\n",
    "        'forest_formation', 'savanna', 'mangrove', 'silvicultura',\n",
    "        'pasture', 'sugarcane', 'mosaic_ag', 'urban', 'mining', 'water',\n",
    "        'soybean', 'rice', 'other_crop', 'coffee', 'citrus',\n",
    "        'other_perennial', 'forest_lag'] \n",
    "\n",
    "mdFile.new_list(items=feature_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 560,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<mdutils.fileutils.fileutils.MarkDownFile at 0x1022c1070>"
      ]
     },
     "execution_count": 560,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mdFile.new_table_of_contents(table_title='Contents', depth=2)\n",
    "mdFile.create_md_file()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "annieulichney-xxpO7m7Z",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
