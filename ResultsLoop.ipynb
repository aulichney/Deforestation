{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pyreadr\n",
    "from scipy.spatial import distance\n",
    "from tqdm import tqdm\n",
    "\n",
    "import shapefile as shp  # Requires the pyshp package\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "from sklearn.model_selection import GroupKFold, cross_val_predict\n",
    "import geopandas as gpd\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "import shap\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "import matplotlib.gridspec as gridspec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Global Variables\n",
    "\n",
    "SUBSET = False\n",
    "SUBSET_SIZE = 100\n",
    "\n",
    "NEW_INDICES = True\n",
    "\n",
    "NEW_CV_INDICES = True\n",
    "\n",
    "PLOT_ENTIRE_AREA = True\n",
    "PLOT_FOLDS = False\n",
    "\n",
    "START_YEAR_TRAIN = 2004\n",
    "NUMBER_YEARS_TRAIN = 3\n",
    "YEARS_TO_TRAIN = [START_YEAR_TRAIN + i  for i in range(NUMBER_YEARS_TRAIN + 1)]\n",
    "\n",
    "PREDICT_YEAR = START_YEAR_TRAIN + NUMBER_YEARS_TRAIN\n",
    "\n",
    "FOLDER_NAME = ''.join([f'{START_YEAR_TRAIN + i}_' for i in list(range(NUMBER_YEARS_TRAIN))]) + f'PREDICT_{PREDICT_YEAR}'\n",
    "\n",
    "\n",
    "FILE_PATH = f'FeatureImportanceResults/{FOLDER_NAME}/'\n",
    "\n",
    "if not os.path.exists(f'FeatureImportanceResults/{FOLDER_NAME}'):\n",
    "    os.makedirs(f'FeatureImportanceResults/{FOLDER_NAME}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_full = pd.read_csv(f'/Users/annieulichney/Documents/GitHub/Deforestation/FinalData/FinalData{START_YEAR_TRAIN}_1.csv')\n",
    "df_full = pd.concat([df_full, pd.read_csv(f'/Users/annieulichney/Documents/GitHub/Deforestation/FinalData/FinalData{START_YEAR_TRAIN}_2.csv')])\n",
    "\n",
    "for year in YEARS_TO_TRAIN[1:]:\n",
    "    filename = f'/Users/annieulichney/Documents/GitHub/Deforestation/FinalData/FinalData{str(year)}_1.csv'\n",
    "    df_full = pd.concat([df_full, pd.read_csv(filename)])\n",
    "    filename = f'/Users/annieulichney/Documents/GitHub/Deforestation/FinalData/FinalData{str(year)}_2.csv'\n",
    "    df_full = pd.concat([df_full, pd.read_csv(filename)])\n",
    "\n",
    "print(np.unique(df_full.year))\n",
    "print(df_full.shape)\n",
    "\n",
    "\n",
    "X_cols  = ['year', 'rain1', 'elevation', 'slope', 'aspect', 'near_mines',\n",
    "       'near_roads', 'near_hidrovia', 'indigenous_homol',\n",
    "       'mun_election_year', 'new_forest_code', 'lula', 'dilma', 'temer',\n",
    "       'bolsonaro', 'fed_election_year', 'populacao', 'pib_pc', 'ironore',\n",
    "       'silver', 'copper', 'gold', 'soy_price', 'beef_price', 'ag_jobs',\n",
    "       'mining_jobs', 'public_jobs', 'construction_jobs', 'PIB',\n",
    "       'n_companies_PUBLIC ADMIN', 'n_companies_AGRICULTURE',\n",
    "       'n_companies_FOOD AND DRINKS', 'n_companies_ACCOMODATION AND FOOD',\n",
    "       'n_companies_EQUIPMENT RENTAL', 'n_companies_WHOLESALE',\n",
    "       'n_companies_ASSOCIATIVE ACTIVITIES',\n",
    "       'n_companies_AUTOMOBILES AND TRANSPORT',\n",
    "       'n_companies_FINANCIAL ASSISTANCE',\n",
    "       'n_companies_TRADE REP VEHICLES', 'n_companies_CONSTRUCTION',\n",
    "       'n_companies_MAIL AND TELECOM', 'n_companies_CULTURE AND SPORT',\n",
    "       'n_companies_EDITING AND PRINTING', 'n_companies_EDUCATION',\n",
    "       'n_companies_ELECTRICITY AND GAS', 'n_companies_FINANCES',\n",
    "       'n_companies_CLEANING AND SEWAGE', 'n_companies_MACHINERY',\n",
    "       'n_companies_BASIC METALLURGY', 'n_companies_MINING',\n",
    "       'n_companies_WOOD PROD',\n",
    "       'n_companies_NON-METALLIC MINERAL PRODUCTS', 'n_companies_HEALTH',\n",
    "       'n_companies_SERVICES FOR COMPANIES',\n",
    "       'n_companies_PERSONAL SERVICES', 'n_companies_TRANSPORTATION',\n",
    "       'n_companies_GROUND TRANSPORT',\n",
    "       'n_companies_WATER TREATMENT AND DISTRIBUTION',\n",
    "       'n_companies_RETAIL', 'n_companies_COMPUTING',\n",
    "       'n_companies_INSURANCE AND SOCIAL SECURITY',\n",
    "       'n_companies_METALLIC PRODUCTS', 'n_companies_DOMESTIC SERVICES',\n",
    "       'n_companies_FORESTRY', 'n_companies_CLOTHING',\n",
    "       'n_companies_PAPER', 'n_companies_INTERNATIONAL BODIES',\n",
    "       'n_companies_OIL AND GAS', 'n_companies_FISHING AND AQUACULTURE',\n",
    "       'n_companies_CHEMICALS', 'n_companies_WATER-BASED TRANSPORTATION',\n",
    "       'n_companies_REAL ESTATE', 'n_companies_RECYCLING',\n",
    "       'n_companies_LEATHERS AND FOOTWEAR',\n",
    "       'n_companies_RUBBER AND PLASTIC', 'n_companies_TEXTILES',\n",
    "       'n_companies_RESEARCH AND DEVELOPMENT',\n",
    "       'n_companies_AERO TRANSPORT', 'n_companies_SMOKE',\n",
    "       'n_companies_PETROLEUM REFINING', 'n_companies_',\n",
    "       'n_jobs_PUBLIC ADMIN', 'n_jobs_AGRICULTURE',\n",
    "       'n_jobs_FOOD AND DRINKS', 'n_jobs_ACCOMODATION AND FOOD',\n",
    "       'n_jobs_EQUIPMENT RENTAL', 'n_jobs_WHOLESALE',\n",
    "       'n_jobs_ASSOCIATIVE ACTIVITIES',\n",
    "       'n_jobs_AUTOMOBILES AND TRANSPORT', 'n_jobs_FINANCIAL ASSISTANCE',\n",
    "       'n_jobs_TRADE REP VEHICLES', 'n_jobs_CONSTRUCTION',\n",
    "       'n_jobs_MAIL AND TELECOM', 'n_jobs_CULTURE AND SPORT',\n",
    "       'n_jobs_EDITING AND PRINTING', 'n_jobs_EDUCATION',\n",
    "       'n_jobs_ELECTRICITY AND GAS', 'n_jobs_FINANCES',\n",
    "       'n_jobs_CLEANING AND SEWAGE', 'n_jobs_MACHINERY',\n",
    "       'n_jobs_BASIC METALLURGY', 'n_jobs_MINING', 'n_jobs_WOOD PROD',\n",
    "       'n_jobs_NON-METALLIC MINERAL PRODUCTS', 'n_jobs_HEALTH',\n",
    "       'n_jobs_SERVICES FOR COMPANIES', 'n_jobs_PERSONAL SERVICES',\n",
    "       'n_jobs_TRANSPORTATION', 'n_jobs_GROUND TRANSPORT',\n",
    "       'n_jobs_WATER TREATMENT AND DISTRIBUTION', 'n_jobs_RETAIL',\n",
    "       'n_jobs_COMPUTING', 'n_jobs_INSURANCE AND SOCIAL SECURITY',\n",
    "       'n_jobs_METALLIC PRODUCTS', 'n_jobs_DOMESTIC SERVICES',\n",
    "       'n_jobs_FORESTRY', 'n_jobs_CLOTHING', 'n_jobs_PAPER',\n",
    "       'n_jobs_INTERNATIONAL BODIES', 'n_jobs_OIL AND GAS',\n",
    "       'n_jobs_FISHING AND AQUACULTURE', 'n_jobs_CHEMICALS',\n",
    "       'n_jobs_WATER-BASED TRANSPORTATION', 'n_jobs_REAL ESTATE',\n",
    "       'n_jobs_RECYCLING', 'n_jobs_LEATHERS AND FOOTWEAR',\n",
    "       'n_jobs_RUBBER AND PLASTIC', 'n_jobs_TEXTILES',\n",
    "       'n_jobs_RESEARCH AND DEVELOPMENT', 'n_jobs_AERO TRANSPORT',\n",
    "       'n_jobs_SMOKE', 'n_jobs_PETROLEUM REFINING', 'n_jobs_',\n",
    "       'n_jobs_TOTAL INDUSTRIAL', 'n_jobs_TOTAL SERVICE',\n",
    "       'n_companies_TOTAL INDUSTRIAL', 'n_companies_TOTAL SERVICE',\n",
    "       'n_companies_TOTAL', 'n_jobs_TOTAL', 'murder_threats',\n",
    "       'assassination', 'assassination_attempt', 'f_emitted_count',\n",
    "       'expen_agri', 'expen_env_man', 'expen_agr_org', 'expen_mining',\n",
    "       'expen_petrol', 'expen_prom_ani_pro', 'expen_prom_veg_pro',\n",
    "       'expen_other_agr', 'expen_agr_defense', 'expen_min_fuel',\n",
    "       'illegal_mining', 'illegal_other', 'illegal_industry', 'audits',\n",
    "       'emiss_pec_full', 'emiss_agr_full', 'emiss_agropec_full',\n",
    "       'incumbant', 'term_limited_seat', 'special',\n",
    "       'overall_winner_complete_college', \n",
    "       'overall_winner_feminino', 'overall_winner_agriculture_job',\n",
    "       'overall_winner_public_service_job', 'overall_winner_health_job',\n",
    "       'overall_winner_corporate_job', 'overall_winner_law_job',\n",
    "       'overall_winner_technical_job', 'overall_winner_professional_job',\n",
    "       'overall_winner_mining_job', 'overall_winner_partido_PT',\n",
    "       'overall_winner_partido_PMDB_MDB', 'overall_winner_partido_PSDB',\n",
    "       'overall_winner_partido_DEM', 'overall_winner_partido_PL',\n",
    "       'overall_winner_partido_other', 'runnerup_partido_PT',\n",
    "       'runnerup_partido_PMDB_MDB', 'runnerup_partido_PSDB',\n",
    "       'runnerup_partido_DEM', 'runnerup_partido_PL',\n",
    "       'runnerup_partido_other', 'winner_votes_proportion',\n",
    "       'vote_participation_proportion',\n",
    "       'forest_formation', 'savanna', 'mangrove', 'silvicultura',\n",
    "       'pasture', 'sugarcane', 'mosaic_ag', 'urban', 'mining', 'water',\n",
    "       'soybean', 'rice', 'other_crop', 'coffee', 'citrus',\n",
    "       'other_perennial', 'forest_lag']\n",
    "\n",
    "\n",
    "#'runnerup_votes_proportion', \n",
    "#'overall_winner_idade',\n",
    "\n",
    "\n",
    "## Test train split\n",
    "#split into two groups where no muni in train set is tested\n",
    "#then do a second split by year so that the train years are year n, n+1, n+2 and the test set uses n+3.\n",
    "if SUBSET:\n",
    "    df_full = df_full.sample(SUBSET_SIZE).reset_index(drop=True)\n",
    "\n",
    "\n",
    "Y = df_full['forest_diff']\n",
    "X = df_full[X_cols]\n",
    "# gdf = gpd.GeoDataFrame(X, geometry = gpd.points_from_xy(df_full.x, df_full.y))\n",
    "# XYs = gdf['geometry']\n",
    "\n",
    "\n",
    "if NEW_INDICES:\n",
    "\n",
    "    #Select Test/Train Indices\n",
    "    n_folds = 10 \n",
    "    munis = df_full['ID'].values\n",
    "    group_kfold = GroupKFold(n_splits = n_folds)\n",
    "    muni_kfold = group_kfold.split(X, Y, munis) \n",
    "    train_indices, test_indices = [list(traintest) for traintest in zip(*muni_kfold)]\n",
    "    city_cv = [*zip(train_indices,test_indices)]\n",
    "\n",
    "    test_inds = []\n",
    "    for i in range(3):\n",
    "        test_inds.extend(city_cv[i][1])\n",
    "\n",
    "    train_inds = []\n",
    "    for i in range(3, 10):\n",
    "        train_inds.extend(city_cv[i][1])\n",
    "\n",
    "    print(f'Test set pct of data: {len(test_inds)/(len(train_inds) + len(test_inds)) * 100}')\n",
    "\n",
    "\n",
    "    np.save('FeatureImportanceResults/test_inds.npy', test_inds)\n",
    "    np.save('FeatureImportanceResults/train_inds.npy', train_inds)\n",
    "    print('New test/train indices generated and read in')\n",
    "\n",
    "if not NEW_INDICES:\n",
    "    test_inds = np.load('test_inds.npy')\n",
    "    train_inds = np.load('train_inds.npy')\n",
    "    print('Existing test/train indices read in from previous iteration')\n",
    "\n",
    "\n",
    "#Split data into test/train sets\n",
    "\n",
    "df_full_test = df_full.iloc[test_inds].reset_index(drop=True)\n",
    "df_full_train = df_full.iloc[train_inds].reset_index(drop=True)\n",
    "\n",
    "#test data has only the last year with unseen spatial samples\n",
    "df_full_test = df_full_test[df_full_test.year == PREDICT_YEAR]\n",
    "\n",
    "#train data has only the 3 train years \n",
    "df_full_train = df_full_train[df_full_train.year < PREDICT_YEAR]\n",
    "\n",
    "Y_test = df_full_test['forest_diff']\n",
    "Y_train = df_full_train['forest_diff']\n",
    "\n",
    "X_test = df_full_test[X_cols]\n",
    "X_train = df_full_train[X_cols]\n",
    "\n",
    "gdf_test = gpd.GeoDataFrame(X_test, geometry = gpd.points_from_xy(df_full_test.x, df_full_test.y))\n",
    "gdf_train = gpd.GeoDataFrame(X_train, geometry = gpd.points_from_xy(df_full_train.x, df_full_train.y))\n",
    "\n",
    "XYs_test = gdf_test['geometry']\n",
    "XYs_train = gdf_train['geometry']\n",
    "\n",
    "if PLOT_ENTIRE_AREA and NEW_INDICES:\n",
    "    fig, axs = plt.subplots(1, 1, figsize=(15, 12))\n",
    "    marker_size = 0.1\n",
    "    marker_size = 1\n",
    "    XYs_test.plot(ax=axs, color = 'red', markersize=marker_size, label = 'Test')\n",
    "    XYs_train.plot(ax=axs, color = 'black', markersize=marker_size, label = 'Train')\n",
    "\n",
    "    plt.legend(markerscale=1)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Save the figure\n",
    "    plt.savefig(FILE_PATH + 'EntirePlot')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "print(np.unique(df_full_test.year))\n",
    "print(np.unique(df_full_train.year))\n",
    "\n",
    "\n",
    "NEW_CV_INDICES = True\n",
    "\n",
    "if NEW_CV_INDICES:\n",
    "    #Select Cross Validation Fold Indices: \n",
    "\n",
    "    n_folds = 5\n",
    "    munis = df_full_train['ID'].values\n",
    "    group_kfold = GroupKFold(n_splits = n_folds)\n",
    "\n",
    "    # Generator for the train/test indices\n",
    "    muni_kfold = group_kfold.split(X_train, Y_train, munis) \n",
    "\n",
    "    # Create a nested list of train and test indices for each fold\n",
    "    train_indices, test_indices = [list(traintest) for traintest in zip(*muni_kfold)]\n",
    "    muni_cv = [*zip(train_indices,test_indices)]\n",
    "\n",
    "    np.save('FeatureImportanceResults/muni_cv.npy', muni_cv)\n",
    "    print('New test/train indices generated and read in')\n",
    "\n",
    "if not NEW_CV_INDICES:\n",
    "    muni_cv = np.load('muni_cv.npy')\n",
    "    print('Existing test/train cross validation indices read in from previous iteration')\n",
    "\n",
    "\n",
    "if PLOT_FOLDS and NEW_CV_INDICES: \n",
    "    fig, axs = plt.subplots(1, n_folds, figsize=(25, 16))\n",
    "    marker_size = 0.01\n",
    "\n",
    "    for i in range(n_folds):\n",
    "        ax = axs[i]\n",
    "\n",
    "        this_train_inds = muni_cv[i][0]\n",
    "        this_test_inds = muni_cv[i][1]\n",
    "        XYs_train[this_test_inds].plot(ax=ax, color = 'red', markersize=marker_size, label = 'Test')\n",
    "        XYs_train[this_train_inds].plot(ax=ax, color = 'black', markersize=marker_size, label = 'Train')\n",
    "        ax.set_title(f\"Fold {i+1}\")\n",
    "\n",
    "    #plt.suptitle(f'{n_folds}-Fold Spatial Cross Validation ') \n",
    "    # handles, labels = ax.get_legend_handles_labels()\n",
    "    # fig.legend(handles, labels)   \n",
    "\n",
    "    for ax in axs.flat:\n",
    "        ax.set_axis_off()\n",
    "\n",
    "\n",
    "    plt.legend(markerscale=100)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(FILE_PATH + 'FoldPlot')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "# Count null values in each column\n",
    "null_counts = {col: X_train[col].isnull().sum() for col in X_train.columns}\n",
    "\n",
    "# Sort the dictionary in descending order based on the values\n",
    "sorted_null_counts = dict(sorted(null_counts.items(), key=lambda item: item[1], reverse=True))\n",
    "\n",
    "print(sorted_null_counts)\n",
    "\n",
    "X_train = X_train.drop('geometry', axis = 1)\n",
    "X_test = X_test.drop('geometry', axis = 1)\n",
    "\n",
    "with open(FILE_PATH + 'performance.txt', 'w+') as f:\n",
    "        f.write(f'MODEL PERFORMANCES\\n')\n",
    "\n",
    "def generate_results_table(coef_input, key_input, name_input, yhat, normalized = True):\n",
    "    if normalized: \n",
    "        coef_input = coef_input / sum(coef_input)\n",
    "\n",
    "    #write MSE to file \n",
    "    mse = mean_squared_error(Y_test, yhat)\n",
    "    print(f'{name_input} MSE: {mse}')\n",
    "\n",
    "    with open(FILE_PATH + 'performance.txt', 'a') as f:\n",
    "        f.write(f'\\n{name_input} MSE: {mse}')\n",
    "\n",
    "\n",
    "    features_df = pd.DataFrame([key_input, coef_input]).T\n",
    "    features_df.columns = ['Feature', 'Coeff']\n",
    "\n",
    "    features_df = features_df.iloc[features_df['Coeff'].abs().argsort()[::-1]]\n",
    "    features_df.to_csv(f'{FILE_PATH}{name_input}.csv')\n",
    "\n",
    "    return features_df\n",
    "\n",
    "\n",
    "base_learners = []\n",
    "\n",
    "yhat_list = []\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#random forest\n",
    "pipeline = Pipeline([\n",
    "                     ('scaler',StandardScaler()),\n",
    "                     ('model',RandomForestRegressor(n_estimators = 500))\n",
    "])\n",
    "\n",
    "search = GridSearchCV(pipeline,\n",
    "                      {'model__max_depth': np.arange(3,11,8) },\n",
    "                      cv = muni_cv, scoring = \"neg_mean_squared_error\",verbose = 3\n",
    "                      )\n",
    "\n",
    "search.fit(X_train,Y_train)\n",
    "\n",
    "base_learners.append(('randomforest', search.best_estimator_))\n",
    "\n",
    "coefficients = search.best_estimator_._final_estimator.feature_importances_\n",
    "importance = np.abs(coefficients)\n",
    "\n",
    "yhat = search.best_estimator_.predict(X_test)\n",
    "yhat_list.append(yhat)\n",
    "\n",
    "randomforest_features_df = generate_results_table(coefficients, X_train.columns, 'randomforest', yhat)\n",
    "\n",
    "X_train.columns[np.argsort(np.array(abs( importance )))[::-1][0:9]]\n",
    "\n",
    "\n",
    "\n",
    "#lasso \n",
    "\n",
    "pipeline = Pipeline([\n",
    "                     ('scaler',StandardScaler()),\n",
    "                     ('model',Lasso())\n",
    "])\n",
    "\n",
    "search = GridSearchCV(pipeline,\n",
    "                      {'model__alpha':[1e-15,1e-10,1e-8,1e-3,1e-2,1,5,10,20,30,35,40,45,50,55,100]},\n",
    "                      cv = muni_cv, scoring = \"neg_mean_squared_error\",verbose = 3\n",
    "                      )\n",
    "\n",
    "search.fit(X_train,Y_train)\n",
    "base_learners.append(('lasso', search.best_estimator_))\n",
    "\n",
    "coefficients = search.best_estimator_.named_steps['model'].coef_\n",
    "importance = np.abs(coefficients)\n",
    "\n",
    "yhat = search.best_estimator_.predict(X_test)\n",
    "yhat_list.append(yhat)\n",
    "\n",
    "lasso_features_df = generate_results_table(coefficients, X_train.columns, 'lasso', yhat = yhat)\n",
    "\n",
    "X_train.columns[np.argsort(np.array(abs( importance )))[::-1][0:9]]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Gradient boosting\n",
    "\n",
    "pipeline = Pipeline([\n",
    "                     ('scaler',StandardScaler()),\n",
    "                     ('model',GradientBoostingRegressor(learning_rate = 0.1, min_samples_leaf = 2))\n",
    "])\n",
    "\n",
    "search = GridSearchCV(pipeline,\n",
    "                      {'model__n_estimators':np.arange(50, 150, 50), 'model__max_depth':np.arange(3, 5, 1)},\n",
    "                      cv = muni_cv, scoring = \"neg_mean_squared_error\",verbose = 3\n",
    "                      )\n",
    "\n",
    "search.fit(X_train,Y_train)\n",
    "\n",
    "search.best_params_\n",
    "\n",
    "base_learners.append(('gradientboosting', search.best_estimator_.named_steps['model']))\n",
    "\n",
    "coefficients = search.best_estimator_.named_steps['model'].feature_importances_\n",
    "importance = np.abs(coefficients)\n",
    "\n",
    "yhat = search.best_estimator_.predict(X_test)\n",
    "yhat_list.append(yhat)\n",
    "\n",
    "gradient_boosting_features_df = generate_results_table(coefficients, X_train.columns, 'gradientboosting', yhat)\n",
    "\n",
    "\n",
    "X.columns[np.argsort(np.array(abs( importance )))[::-1][0:9]]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#neural network\n",
    "\n",
    "pipeline = Pipeline([\n",
    "                     ('scaler',StandardScaler()),\n",
    "                     ('model', MLPRegressor(activation = 'logistic', random_state=42))\n",
    "])\n",
    "\n",
    "search = GridSearchCV(pipeline,\n",
    "                      {'model__hidden_layer_sizes':[(50,),(100,)], 'model__alpha':np.arange(0.00001, 0.001, 0.001)},\n",
    "                      cv = muni_cv, scoring = \"neg_mean_squared_error\",verbose = 3\n",
    "                      )\n",
    "\n",
    "search.fit(X_train,Y_train)\n",
    "\n",
    "base_learners.append(('neuralnetwork', search.best_estimator_))\n",
    "\n",
    "explainer = shap.KernelExplainer(search.best_estimator_.predict, X_train)\n",
    "\n",
    "shap_values = explainer.shap_values(X_test, nsamples=10)\n",
    "\n",
    "shap.summary_plot(shap_values,X_test,feature_names=X_test.columns)\n",
    "\n",
    "feature_names = X_train.columns\n",
    "\n",
    "rf_resultX = pd.DataFrame(shap_values, columns = feature_names)\n",
    "\n",
    "vals = np.abs(rf_resultX.values).mean(0)\n",
    "\n",
    "shap_importance = pd.DataFrame(list(zip(feature_names, vals)),\n",
    "                                  columns=['col_name','feature_importance_vals'])\n",
    "shap_importance.sort_values(by=['feature_importance_vals'],\n",
    "                               ascending=False, inplace=True)\n",
    "\n",
    "#shap_importance.to_csv('FeatureInportanceResults/neuralnetwork.csv')\n",
    "\n",
    "yhat = search.best_estimator_.predict(X_test)\n",
    "yhat_list.append(yhat)\n",
    "\n",
    "nn_features_df = generate_results_table(np.array(shap_importance.feature_importance_vals), np.array(shap_importance.col_name), 'neuralnetwork', yhat)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#super learner ensemble\n",
    "def get_models():\n",
    "    base_models = []\n",
    "    #Random forest regressor\n",
    "    base_models.append(base_learners[0][1][1])\n",
    "    #Lasso\n",
    "    base_models.append(base_learners[1][1][1])\n",
    "    #Gradient Boosting\n",
    "    base_models.append(base_learners[2][1])\n",
    "    #NeuralNetwork\n",
    "    base_models.append(base_learners[3][1][1])\n",
    "    return base_models\n",
    "\n",
    "def get_out_of_fold_predictions(X_train, Y_train, base_models):\n",
    "    meta_X = []\n",
    "    meta_Y = []\n",
    "\n",
    "    # enumerate splits\n",
    "    for train_ix, test_ix in muni_cv:\n",
    "        fold_yhats = []\n",
    "        meta_train_X, meta_test_X = X_train.iloc[train_ix], X_train.iloc[test_ix]\n",
    "        meta_train_Y, meta_test_Y = Y_train.iloc[train_ix], Y_train.iloc[test_ix]\n",
    "        meta_Y.extend(meta_test_Y)\n",
    "\n",
    "        # fit and make predictions with each sub-model\n",
    "        for model in base_models:\n",
    "            model.fit(meta_train_X, meta_train_Y)\n",
    "            yhat = model.predict(meta_test_X)\n",
    "            # store columns\n",
    "            fold_yhats.append(yhat.reshape(len(yhat),1))\n",
    "       \n",
    "        meta_X.append(np.hstack(fold_yhats))\n",
    "            \n",
    "    return np.vstack(meta_X), np.asarray(meta_Y)\n",
    "\n",
    "def super_learner_predictions(X, models, meta_model):\n",
    "\tmeta_X = []\n",
    "\tfor model in models:\n",
    "\t\tyhat = model.predict(X) \n",
    "\t\tmeta_X.append(yhat)\n",
    "\t# predict\n",
    "\treturn meta_model.predict(pd.DataFrame(meta_X).T)\n",
    "    \n",
    "def fit_base_models(X, y, models):\n",
    "\tfor model in models:\n",
    "\t\tmodel.fit(X, y)\n",
    " \n",
    "\n",
    "def fit_meta_model(X, y):\n",
    "\tmodel = Ridge()\n",
    "\tmodel.fit(X, y)\n",
    "\treturn model\n",
    "\n",
    "def evaluate_models(X, y, models):\n",
    "\tfor model in models:\n",
    "\t\tyhat = model.predict(X)\n",
    "\t\tmse = mean_squared_error(y, yhat)\n",
    "\t\tprint('%s: %.3f' % (model.__class__.__name__, mse))\n",
    "\n",
    "# get models\n",
    "models = get_models()\n",
    "# get out of fold predictions\n",
    "meta_X, meta_y = get_out_of_fold_predictions(X_train, Y_train, models)\n",
    "print('Meta Data Shape: ', meta_X.shape, meta_y.shape)\n",
    "\n",
    "fit_base_models(X_train, Y_train, models)\n",
    "meta_model = fit_meta_model(meta_X, meta_y)\n",
    "\n",
    "evaluate_models(X_test, Y_test, models)\n",
    "\n",
    "yhat = super_learner_predictions(X_test, models, meta_model)\n",
    "yhat_list.append(yhat)\n",
    "\n",
    "# Evaluate the performance of the model\n",
    "mse = mean_squared_error(Y_test, yhat)\n",
    "print(\"MSE:\", mse)\n",
    "\n",
    "\n",
    "#Super Learner Feature Importance\n",
    "\n",
    "#Random forest \n",
    "random_forest_weighted_importance = models[0].feature_importances_ * meta_model.coef_[0]\n",
    "\n",
    "#Lasso \n",
    "lasso_weighted_importance = models[1].coef_ * meta_model.coef_[1]\n",
    "\n",
    "#GradientBoostingRegressor\n",
    "gradient_boosting_weighted_importance = models[2].feature_importances_ * meta_model.coef_[2]\n",
    "\n",
    "#NeuralNetwork\n",
    "explainer = shap.KernelExplainer(models[2].predict, X_train)\n",
    "shap_values = explainer.shap_values(X_test, nsamples=100)\n",
    "\n",
    "feature_names = X_train.columns\n",
    "\n",
    "rf_resultX = pd.DataFrame(shap_values, columns = feature_names)\n",
    "\n",
    "vals = np.abs(rf_resultX.values).mean(0)\n",
    "\n",
    "shap_importance = pd.DataFrame(list(zip(feature_names, vals)),\n",
    "                                  columns=['col_name','feature_importance_vals'])\n",
    "\n",
    "\n",
    "nn_weighted_importance = shap_importance.feature_importance_vals * meta_model.coef_[3]\n",
    "\n",
    "\n",
    "super_learner_feature_importance = np.mean([random_forest_weighted_importance, lasso_weighted_importance, gradient_boosting_weighted_importance, nn_weighted_importance], axis = 0)\n",
    "\n",
    "\n",
    "super_learner_features_df = generate_results_table(super_learner_feature_importance, X_train.columns, 'superlearner', yhat)\n",
    "\n",
    "\n",
    "\n",
    "#visualize\n",
    "\n",
    "prediction_df = -pd.DataFrame(yhat_list).T\n",
    "prediction_df.columns = ['randomforest', 'lasso', 'gradientboosting', 'nn', 'superlearner']\n",
    "prediction_df['avg'] = prediction_df.mean(axis=1)\n",
    "prediction_df['x'] = np.array(df_full_test['x'])\n",
    "prediction_df['y'] = np.array(df_full_test['y'])\n",
    "prediction_df['actual']  = -np.array(Y_test)\n",
    "prediction_df.to_csv(FILE_PATH + 'predictions.csv')\n",
    "\n",
    "for col_name in ['randomforest', 'lasso', 'gradientboosting', 'nn', 'superlearner', 'avg']:\n",
    "    gdf_yhat = gpd.GeoDataFrame(prediction_df, geometry = gpd.points_from_xy(prediction_df.x, prediction_df.y))\n",
    "    fig, axs = plt.subplots(1, 1, figsize=(10, 8))\n",
    "    marker_size = 1\n",
    "    gdf_yhat.plot(column = col_name, cmap = 'Reds', ax=axs, markersize = marker_size)\n",
    "\n",
    "    #plt.legend(markerscale=1)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    \n",
    "\n",
    "    # Show the colorbar\n",
    "    sm = plt.cm.ScalarMappable(cmap = 'Reds')\n",
    "    sm.set_array(prediction_df['avg'])\n",
    "    cbar = plt.colorbar(sm)\n",
    "    \n",
    "\n",
    "    # Save the figure\n",
    "    plt.savefig(FILE_PATH + 'DeforestPlot_' + col_name)\n",
    "\n",
    "    plt.show()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
