{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pyreadr\n",
    "from scipy.spatial import distance\n",
    "from tqdm import tqdm\n",
    "\n",
    "import shapefile as shp  # Requires the pyshp package\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_full = pd.read_csv('/Users/annieulichney/Documents/GitHub/Deforestation/FinalData/FinalData2004.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "for year in [ 2005, 2006, 2007, 2008, 2009, 2010, 2011, 2012, 2013, 2014, 2015, 2016]:\n",
    "    filename = f'/Users/annieulichney/Documents/GitHub/Deforestation/FinalData/FinalData{str(year)}.csv'\n",
    "    df_full = pd.concat([df_full, pd.read_csv(filename)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_conditional(cols_list, df):\n",
    "    for col in cols_list:\n",
    "        if col in df.columns:\n",
    "            df = df.drop(col, axis = 1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_full = drop_conditional(['Unnamed: 0', 'ID', ], df_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2133501, 216)"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_full.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         1100015.0\n",
       "1         1100015.0\n",
       "2         1100015.0\n",
       "3         1100015.0\n",
       "4         1100015.0\n",
       "            ...    \n",
       "160553    5108956.0\n",
       "160554    5108956.0\n",
       "160555    5108956.0\n",
       "160556    5108956.0\n",
       "160557    5108956.0\n",
       "Name: ID, Length: 2133501, dtype: float64"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_full.ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "#should year be in it? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_cols  = ['year', 'rain1', 'elevation', 'slope', 'aspect', 'near_mines',\n",
    "       'near_roads', 'near_hidrovia', 'indigenous_homol',\n",
    "       'mun_election_year', 'new_forest_code', 'lula', 'dilma', 'temer',\n",
    "       'bolsonaro', 'fed_election_year', 'populacao', 'pib_pc', 'ironore',\n",
    "       'silver', 'copper', 'gold', 'soy_price', 'beef_price', 'ag_jobs',\n",
    "       'mining_jobs', 'public_jobs', 'construction_jobs', 'PIB',\n",
    "       'n_companies_PUBLIC ADMIN', 'n_companies_AGRICULTURE',\n",
    "       'n_companies_FOOD AND DRINKS', 'n_companies_ACCOMODATION AND FOOD',\n",
    "       'n_companies_EQUIPMENT RENTAL', 'n_companies_WHOLESALE',\n",
    "       'n_companies_ASSOCIATIVE ACTIVITIES',\n",
    "       'n_companies_AUTOMOBILES AND TRANSPORT',\n",
    "       'n_companies_FINANCIAL ASSISTANCE',\n",
    "       'n_companies_TRADE REP VEHICLES', 'n_companies_CONSTRUCTION',\n",
    "       'n_companies_MAIL AND TELECOM', 'n_companies_CULTURE AND SPORT',\n",
    "       'n_companies_EDITING AND PRINTING', 'n_companies_EDUCATION',\n",
    "       'n_companies_ELECTRICITY AND GAS', 'n_companies_FINANCES',\n",
    "       'n_companies_CLEANING AND SEWAGE', 'n_companies_MACHINERY',\n",
    "       'n_companies_BASIC METALLURGY', 'n_companies_MINING',\n",
    "       'n_companies_WOOD PROD',\n",
    "       'n_companies_NON-METALLIC MINERAL PRODUCTS', 'n_companies_HEALTH',\n",
    "       'n_companies_SERVICES FOR COMPANIES',\n",
    "       'n_companies_PERSONAL SERVICES', 'n_companies_TRANSPORTATION',\n",
    "       'n_companies_GROUND TRANSPORT',\n",
    "       'n_companies_WATER TREATMENT AND DISTRIBUTION',\n",
    "       'n_companies_RETAIL', 'n_companies_COMPUTING',\n",
    "       'n_companies_INSURANCE AND SOCIAL SECURITY',\n",
    "       'n_companies_METALLIC PRODUCTS', 'n_companies_DOMESTIC SERVICES',\n",
    "       'n_companies_FORESTRY', 'n_companies_CLOTHING',\n",
    "       'n_companies_PAPER', 'n_companies_INTERNATIONAL BODIES',\n",
    "       'n_companies_OIL AND GAS', 'n_companies_FISHING AND AQUACULTURE',\n",
    "       'n_companies_CHEMICALS', 'n_companies_WATER-BASED TRANSPORTATION',\n",
    "       'n_companies_REAL ESTATE', 'n_companies_RECYCLING',\n",
    "       'n_companies_LEATHERS AND FOOTWEAR',\n",
    "       'n_companies_RUBBER AND PLASTIC', 'n_companies_TEXTILES',\n",
    "       'n_companies_RESEARCH AND DEVELOPMENT',\n",
    "       'n_companies_AERO TRANSPORT', 'n_companies_SMOKE',\n",
    "       'n_companies_PETROLEUM REFINING', 'n_companies_',\n",
    "       'n_jobs_PUBLIC ADMIN', 'n_jobs_AGRICULTURE',\n",
    "       'n_jobs_FOOD AND DRINKS', 'n_jobs_ACCOMODATION AND FOOD',\n",
    "       'n_jobs_EQUIPMENT RENTAL', 'n_jobs_WHOLESALE',\n",
    "       'n_jobs_ASSOCIATIVE ACTIVITIES',\n",
    "       'n_jobs_AUTOMOBILES AND TRANSPORT', 'n_jobs_FINANCIAL ASSISTANCE',\n",
    "       'n_jobs_TRADE REP VEHICLES', 'n_jobs_CONSTRUCTION',\n",
    "       'n_jobs_MAIL AND TELECOM', 'n_jobs_CULTURE AND SPORT',\n",
    "       'n_jobs_EDITING AND PRINTING', 'n_jobs_EDUCATION',\n",
    "       'n_jobs_ELECTRICITY AND GAS', 'n_jobs_FINANCES',\n",
    "       'n_jobs_CLEANING AND SEWAGE', 'n_jobs_MACHINERY',\n",
    "       'n_jobs_BASIC METALLURGY', 'n_jobs_MINING', 'n_jobs_WOOD PROD',\n",
    "       'n_jobs_NON-METALLIC MINERAL PRODUCTS', 'n_jobs_HEALTH',\n",
    "       'n_jobs_SERVICES FOR COMPANIES', 'n_jobs_PERSONAL SERVICES',\n",
    "       'n_jobs_TRANSPORTATION', 'n_jobs_GROUND TRANSPORT',\n",
    "       'n_jobs_WATER TREATMENT AND DISTRIBUTION', 'n_jobs_RETAIL',\n",
    "       'n_jobs_COMPUTING', 'n_jobs_INSURANCE AND SOCIAL SECURITY',\n",
    "       'n_jobs_METALLIC PRODUCTS', 'n_jobs_DOMESTIC SERVICES',\n",
    "       'n_jobs_FORESTRY', 'n_jobs_CLOTHING', 'n_jobs_PAPER',\n",
    "       'n_jobs_INTERNATIONAL BODIES', 'n_jobs_OIL AND GAS',\n",
    "       'n_jobs_FISHING AND AQUACULTURE', 'n_jobs_CHEMICALS',\n",
    "       'n_jobs_WATER-BASED TRANSPORTATION', 'n_jobs_REAL ESTATE',\n",
    "       'n_jobs_RECYCLING', 'n_jobs_LEATHERS AND FOOTWEAR',\n",
    "       'n_jobs_RUBBER AND PLASTIC', 'n_jobs_TEXTILES',\n",
    "       'n_jobs_RESEARCH AND DEVELOPMENT', 'n_jobs_AERO TRANSPORT',\n",
    "       'n_jobs_SMOKE', 'n_jobs_PETROLEUM REFINING', 'n_jobs_',\n",
    "       'n_jobs_TOTAL INDUSTRIAL', 'n_jobs_TOTAL SERVICE',\n",
    "       'n_companies_TOTAL INDUSTRIAL', 'n_companies_TOTAL SERVICE',\n",
    "       'n_companies_TOTAL', 'n_jobs_TOTAL', 'murder_threats',\n",
    "       'assassination', 'assassination_attempt', 'f_emitted_count',\n",
    "       'expen_agri', 'expen_env_man', 'expen_agr_org', 'expen_mining',\n",
    "       'expen_petrol', 'expen_prom_ani_pro', 'expen_prom_veg_pro',\n",
    "       'expen_other_agr', 'expen_agr_defense', 'expen_min_fuel',\n",
    "       'illegal_mining', 'illegal_other', 'illegal_industry', 'audits',\n",
    "       'emiss_pec_full', 'emiss_agr_full', 'emiss_agropec_full',\n",
    "       'incumbant', 'term_limited_seat', 'special',\n",
    "       'overall_winner_complete_college', \n",
    "       'overall_winner_feminino', 'overall_winner_agriculture_job',\n",
    "       'overall_winner_public_service_job', 'overall_winner_health_job',\n",
    "       'overall_winner_corporate_job', 'overall_winner_law_job',\n",
    "       'overall_winner_technical_job', 'overall_winner_professional_job',\n",
    "       'overall_winner_mining_job', 'overall_winner_partido_PT',\n",
    "       'overall_winner_partido_PMDB_MDB', 'overall_winner_partido_PSDB',\n",
    "       'overall_winner_partido_DEM', 'overall_winner_partido_PL',\n",
    "       'overall_winner_partido_other', 'runnerup_partido_PT',\n",
    "       'runnerup_partido_PMDB_MDB', 'runnerup_partido_PSDB',\n",
    "       'runnerup_partido_DEM', 'runnerup_partido_PL',\n",
    "       'runnerup_partido_other', 'winner_votes_proportion',\n",
    "       'vote_participation_proportion',\n",
    "       'forest_formation', 'savanna', 'mangrove', 'silvicultura',\n",
    "       'pasture', 'sugarcane', 'mosaic_ag', 'urban', 'mining', 'water',\n",
    "       'soybean', 'rice', 'other_crop', 'coffee', 'citrus',\n",
    "       'other_perennial', 'forest_lag']\n",
    "\n",
    "\n",
    "#'runnerup_votes_proportion', \n",
    "#'overall_winner_idade',"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Train Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GroupKFold, cross_val_predict\n",
    "\n",
    "import geopandas as gpd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2133501"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_full.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_full = df_full.sample(100).reset_index(drop=True)\n",
    "Y = df_full['forest_diff']\n",
    "X = df_full[X_cols]\n",
    "# gdf = gpd.GeoDataFrame(X, geometry = gpd.points_from_xy(df_full.x, df_full.y))\n",
    "# XYs = gdf['geometry']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3\n"
     ]
    }
   ],
   "source": [
    "#Select Test/Train Indices\n",
    "n_folds = 10 \n",
    "munis = df_full['ID'].values\n",
    "group_kfold = GroupKFold(n_splits = n_folds)\n",
    "muni_kfold = group_kfold.split(X, Y, munis) \n",
    "train_indices, test_indices = [list(traintest) for traintest in zip(*muni_kfold)]\n",
    "city_cv = [*zip(train_indices,test_indices)]\n",
    "\n",
    "test_inds = []\n",
    "for i in range(3):\n",
    "    test_inds.extend(city_cv[i][1])\n",
    "\n",
    "train_inds = []\n",
    "for i in range(3, 10):\n",
    "    train_inds.extend(city_cv[i][1])\n",
    "\n",
    "\n",
    "print(len(test_inds)/(len(train_inds) + len(test_inds)) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "#partition data into test/train sets\n",
    "\n",
    "df_full_test = df_full.iloc[test_inds].reset_index(drop=True)\n",
    "df_full_train = df_full.iloc[train_inds].reset_index(drop=True)\n",
    "\n",
    "Y_test = df_full_test['forest_diff']\n",
    "Y_train = df_full_train['forest_diff']\n",
    "\n",
    "X_test = df_full_test[X_cols]\n",
    "X_train = df_full_train[X_cols]\n",
    "\n",
    "gdf_test = gpd.GeoDataFrame(X_test, geometry = gpd.points_from_xy(df_full_test.x, df_full_test.y))\n",
    "gdf_train = gpd.GeoDataFrame(X_train, geometry = gpd.points_from_xy(df_full_train.x, df_full_train.y))\n",
    "\n",
    "XYs_test = gdf_test['geometry']\n",
    "XYs_train = gdf_train['geometry']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1080x864 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "PLOT = True\n",
    "\n",
    "if PLOT:\n",
    "    fig, axs = plt.subplots(1, 1, figsize=(15, 12))\n",
    "    marker_size = 0.1\n",
    "    marker_size = 1\n",
    "    XYs_test.plot(ax=axs, color = 'red', markersize=marker_size, label = 'Test')\n",
    "    XYs_train.plot(ax=axs, color = 'black', markersize=marker_size, label = 'Train')\n",
    "\n",
    "    plt.legend(markerscale=1)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Select Cross Validation Fold Indices: \n",
    "\n",
    "n_folds = 5\n",
    "munis = df_full_train['ID'].values\n",
    "group_kfold = GroupKFold(n_splits = n_folds)\n",
    "\n",
    "# Generator for the train/test indices\n",
    "muni_kfold = group_kfold.split(X_train, Y_train, munis) \n",
    "\n",
    "# Create a nested list of train and test indices for each fold\n",
    "train_indices, test_indices = [list(traintest) for traintest in zip(*muni_kfold)]\n",
    "muni_cv = [*zip(train_indices,test_indices)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "PLOT_FOLDS = False\n",
    "if PLOT_FOLDS: \n",
    "    fig, axs = plt.subplots(1, n_folds, figsize=(25, 16))\n",
    "    marker_size = 0.01\n",
    "\n",
    "    for i in range(n_folds):\n",
    "        ax = axs[i]\n",
    "\n",
    "        this_train_inds = muni_cv[i][0]\n",
    "        this_test_inds = muni_cv[i][1]\n",
    "        XYs_train[this_test_inds].plot(ax=ax, color = 'red', markersize=marker_size, label = 'Test')\n",
    "        XYs_train[this_train_inds].plot(ax=ax, color = 'black', markersize=marker_size, label = 'Train')\n",
    "        ax.set_title(f\"Fold {i+1}\")\n",
    "\n",
    "    #plt.suptitle(f'{n_folds}-Fold Spatial Cross Validation ') \n",
    "    # handles, labels = ax.get_legend_handles_labels()\n",
    "    # fig.legend(handles, labels)   \n",
    "\n",
    "    for ax in axs.flat:\n",
    "        ax.set_axis_off()\n",
    "\n",
    "\n",
    "    plt.legend(markerscale=100)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>rain1</th>\n",
       "      <th>elevation</th>\n",
       "      <th>slope</th>\n",
       "      <th>aspect</th>\n",
       "      <th>near_mines</th>\n",
       "      <th>near_roads</th>\n",
       "      <th>near_hidrovia</th>\n",
       "      <th>indigenous_homol</th>\n",
       "      <th>mun_election_year</th>\n",
       "      <th>...</th>\n",
       "      <th>mining</th>\n",
       "      <th>water</th>\n",
       "      <th>soybean</th>\n",
       "      <th>rice</th>\n",
       "      <th>other_crop</th>\n",
       "      <th>coffee</th>\n",
       "      <th>citrus</th>\n",
       "      <th>other_perennial</th>\n",
       "      <th>forest_lag</th>\n",
       "      <th>geometry</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2004</td>\n",
       "      <td>26.69474</td>\n",
       "      <td>170.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>43823.49</td>\n",
       "      <td>62284.85</td>\n",
       "      <td>70087.130</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.052162</td>\n",
       "      <td>POINT (-62.52500 -9.42500)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2006</td>\n",
       "      <td>28.92715</td>\n",
       "      <td>291.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>44220.20</td>\n",
       "      <td>35965.72</td>\n",
       "      <td>63467.210</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.100305</td>\n",
       "      <td>POINT (-55.27500 -6.22500)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2004</td>\n",
       "      <td>26.03838</td>\n",
       "      <td>450.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>116.0</td>\n",
       "      <td>46226.31</td>\n",
       "      <td>81908.83</td>\n",
       "      <td>140703.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.016141</td>\n",
       "      <td>POINT (-55.02500 -14.32500)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2009</td>\n",
       "      <td>30.17732</td>\n",
       "      <td>84.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>238.0</td>\n",
       "      <td>290771.50</td>\n",
       "      <td>31920.89</td>\n",
       "      <td>3657.568</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.996202</td>\n",
       "      <td>POINT (-44.62500 -4.67500)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2006</td>\n",
       "      <td>35.24723</td>\n",
       "      <td>273.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>209.0</td>\n",
       "      <td>75217.83</td>\n",
       "      <td>106856.10</td>\n",
       "      <td>83014.370</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.001487</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.055551</td>\n",
       "      <td>POINT (-56.02500 -8.82500)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 205 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   year     rain1  elevation  slope  aspect  near_mines  near_roads  \\\n",
       "0  2004  26.69474      170.0    0.0    34.0    43823.49    62284.85   \n",
       "1  2006  28.92715      291.0    0.0    21.0    44220.20    35965.72   \n",
       "2  2004  26.03838      450.0    0.0   116.0    46226.31    81908.83   \n",
       "3  2009  30.17732       84.0    0.0   238.0   290771.50    31920.89   \n",
       "4  2006  35.24723      273.0    1.0   209.0    75217.83   106856.10   \n",
       "\n",
       "   near_hidrovia  indigenous_homol  mun_election_year  ...  mining     water  \\\n",
       "0      70087.130               0.0                1.0  ...     0.0  0.000000   \n",
       "1      63467.210               0.0                0.0  ...     0.0  0.000000   \n",
       "2     140703.000               0.0                1.0  ...     0.0  0.000000   \n",
       "3       3657.568               0.0                0.0  ...     0.0  0.000000   \n",
       "4      83014.370               0.0                0.0  ...     0.0  0.001487   \n",
       "\n",
       "   soybean  rice  other_crop  coffee  citrus  other_perennial  forest_lag  \\\n",
       "0      0.0   0.0         0.0       0       0              0.0    3.052162   \n",
       "1      0.0   0.0         0.0       0       0              0.0    3.100305   \n",
       "2      0.0   0.0         0.0       0       0              0.0    0.016141   \n",
       "3      0.0   0.0         0.0       0       0              0.0    2.996202   \n",
       "4      0.0   0.0         0.0       0       0              0.0    3.055551   \n",
       "\n",
       "                      geometry  \n",
       "0   POINT (-62.52500 -9.42500)  \n",
       "1   POINT (-55.27500 -6.22500)  \n",
       "2  POINT (-55.02500 -14.32500)  \n",
       "3   POINT (-44.62500 -4.67500)  \n",
       "4   POINT (-56.02500 -8.82500)  \n",
       "\n",
       "[5 rows x 205 columns]"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'year': 0, 'rain1': 0, 'elevation': 0, 'slope': 0, 'aspect': 0, 'near_mines': 0, 'near_roads': 0, 'near_hidrovia': 0, 'indigenous_homol': 0, 'mun_election_year': 0, 'new_forest_code': 0, 'lula': 0, 'dilma': 0, 'temer': 0, 'bolsonaro': 0, 'fed_election_year': 0, 'populacao': 0, 'pib_pc': 0, 'ironore': 0, 'silver': 0, 'copper': 0, 'gold': 0, 'soy_price': 0, 'beef_price': 0, 'ag_jobs': 0, 'mining_jobs': 0, 'public_jobs': 0, 'construction_jobs': 0, 'PIB': 0, 'n_companies_PUBLIC ADMIN': 0, 'n_companies_AGRICULTURE': 0, 'n_companies_FOOD AND DRINKS': 0, 'n_companies_ACCOMODATION AND FOOD': 0, 'n_companies_EQUIPMENT RENTAL': 0, 'n_companies_WHOLESALE': 0, 'n_companies_ASSOCIATIVE ACTIVITIES': 0, 'n_companies_AUTOMOBILES AND TRANSPORT': 0, 'n_companies_FINANCIAL ASSISTANCE': 0, 'n_companies_TRADE REP VEHICLES': 0, 'n_companies_CONSTRUCTION': 0, 'n_companies_MAIL AND TELECOM': 0, 'n_companies_CULTURE AND SPORT': 0, 'n_companies_EDITING AND PRINTING': 0, 'n_companies_EDUCATION': 0, 'n_companies_ELECTRICITY AND GAS': 0, 'n_companies_FINANCES': 0, 'n_companies_CLEANING AND SEWAGE': 0, 'n_companies_MACHINERY': 0, 'n_companies_BASIC METALLURGY': 0, 'n_companies_MINING': 0, 'n_companies_WOOD PROD': 0, 'n_companies_NON-METALLIC MINERAL PRODUCTS': 0, 'n_companies_HEALTH': 0, 'n_companies_SERVICES FOR COMPANIES': 0, 'n_companies_PERSONAL SERVICES': 0, 'n_companies_TRANSPORTATION': 0, 'n_companies_GROUND TRANSPORT': 0, 'n_companies_WATER TREATMENT AND DISTRIBUTION': 0, 'n_companies_RETAIL': 0, 'n_companies_COMPUTING': 0, 'n_companies_INSURANCE AND SOCIAL SECURITY': 0, 'n_companies_METALLIC PRODUCTS': 0, 'n_companies_DOMESTIC SERVICES': 0, 'n_companies_FORESTRY': 0, 'n_companies_CLOTHING': 0, 'n_companies_PAPER': 0, 'n_companies_INTERNATIONAL BODIES': 0, 'n_companies_OIL AND GAS': 0, 'n_companies_FISHING AND AQUACULTURE': 0, 'n_companies_CHEMICALS': 0, 'n_companies_WATER-BASED TRANSPORTATION': 0, 'n_companies_REAL ESTATE': 0, 'n_companies_RECYCLING': 0, 'n_companies_LEATHERS AND FOOTWEAR': 0, 'n_companies_RUBBER AND PLASTIC': 0, 'n_companies_TEXTILES': 0, 'n_companies_RESEARCH AND DEVELOPMENT': 0, 'n_companies_AERO TRANSPORT': 0, 'n_companies_SMOKE': 0, 'n_companies_PETROLEUM REFINING': 0, 'n_companies_': 0, 'n_jobs_PUBLIC ADMIN': 0, 'n_jobs_AGRICULTURE': 0, 'n_jobs_FOOD AND DRINKS': 0, 'n_jobs_ACCOMODATION AND FOOD': 0, 'n_jobs_EQUIPMENT RENTAL': 0, 'n_jobs_WHOLESALE': 0, 'n_jobs_ASSOCIATIVE ACTIVITIES': 0, 'n_jobs_AUTOMOBILES AND TRANSPORT': 0, 'n_jobs_FINANCIAL ASSISTANCE': 0, 'n_jobs_TRADE REP VEHICLES': 0, 'n_jobs_CONSTRUCTION': 0, 'n_jobs_MAIL AND TELECOM': 0, 'n_jobs_CULTURE AND SPORT': 0, 'n_jobs_EDITING AND PRINTING': 0, 'n_jobs_EDUCATION': 0, 'n_jobs_ELECTRICITY AND GAS': 0, 'n_jobs_FINANCES': 0, 'n_jobs_CLEANING AND SEWAGE': 0, 'n_jobs_MACHINERY': 0, 'n_jobs_BASIC METALLURGY': 0, 'n_jobs_MINING': 0, 'n_jobs_WOOD PROD': 0, 'n_jobs_NON-METALLIC MINERAL PRODUCTS': 0, 'n_jobs_HEALTH': 0, 'n_jobs_SERVICES FOR COMPANIES': 0, 'n_jobs_PERSONAL SERVICES': 0, 'n_jobs_TRANSPORTATION': 0, 'n_jobs_GROUND TRANSPORT': 0, 'n_jobs_WATER TREATMENT AND DISTRIBUTION': 0, 'n_jobs_RETAIL': 0, 'n_jobs_COMPUTING': 0, 'n_jobs_INSURANCE AND SOCIAL SECURITY': 0, 'n_jobs_METALLIC PRODUCTS': 0, 'n_jobs_DOMESTIC SERVICES': 0, 'n_jobs_FORESTRY': 0, 'n_jobs_CLOTHING': 0, 'n_jobs_PAPER': 0, 'n_jobs_INTERNATIONAL BODIES': 0, 'n_jobs_OIL AND GAS': 0, 'n_jobs_FISHING AND AQUACULTURE': 0, 'n_jobs_CHEMICALS': 0, 'n_jobs_WATER-BASED TRANSPORTATION': 0, 'n_jobs_REAL ESTATE': 0, 'n_jobs_RECYCLING': 0, 'n_jobs_LEATHERS AND FOOTWEAR': 0, 'n_jobs_RUBBER AND PLASTIC': 0, 'n_jobs_TEXTILES': 0, 'n_jobs_RESEARCH AND DEVELOPMENT': 0, 'n_jobs_AERO TRANSPORT': 0, 'n_jobs_SMOKE': 0, 'n_jobs_PETROLEUM REFINING': 0, 'n_jobs_': 0, 'n_jobs_TOTAL INDUSTRIAL': 0, 'n_jobs_TOTAL SERVICE': 0, 'n_companies_TOTAL INDUSTRIAL': 0, 'n_companies_TOTAL SERVICE': 0, 'n_companies_TOTAL': 0, 'n_jobs_TOTAL': 0, 'murder_threats': 0, 'assassination': 0, 'assassination_attempt': 0, 'f_emitted_count': 0, 'expen_agri': 0, 'expen_env_man': 0, 'expen_agr_org': 0, 'expen_mining': 0, 'expen_petrol': 0, 'expen_prom_ani_pro': 0, 'expen_prom_veg_pro': 0, 'expen_other_agr': 0, 'expen_agr_defense': 0, 'expen_min_fuel': 0, 'illegal_mining': 0, 'illegal_other': 0, 'illegal_industry': 0, 'audits': 0, 'emiss_pec_full': 0, 'emiss_agr_full': 0, 'emiss_agropec_full': 0, 'incumbant': 0, 'term_limited_seat': 0, 'special': 0, 'overall_winner_complete_college': 0, 'overall_winner_feminino': 0, 'overall_winner_agriculture_job': 0, 'overall_winner_public_service_job': 0, 'overall_winner_health_job': 0, 'overall_winner_corporate_job': 0, 'overall_winner_law_job': 0, 'overall_winner_technical_job': 0, 'overall_winner_professional_job': 0, 'overall_winner_mining_job': 0, 'overall_winner_partido_PT': 0, 'overall_winner_partido_PMDB_MDB': 0, 'overall_winner_partido_PSDB': 0, 'overall_winner_partido_DEM': 0, 'overall_winner_partido_PL': 0, 'overall_winner_partido_other': 0, 'runnerup_partido_PT': 0, 'runnerup_partido_PMDB_MDB': 0, 'runnerup_partido_PSDB': 0, 'runnerup_partido_DEM': 0, 'runnerup_partido_PL': 0, 'runnerup_partido_other': 0, 'winner_votes_proportion': 0, 'vote_participation_proportion': 0, 'forest_formation': 0, 'savanna': 0, 'mangrove': 0, 'silvicultura': 0, 'pasture': 0, 'sugarcane': 0, 'mosaic_ag': 0, 'urban': 0, 'mining': 0, 'water': 0, 'soybean': 0, 'rice': 0, 'other_crop': 0, 'coffee': 0, 'citrus': 0, 'other_perennial': 0, 'forest_lag': 0, 'geometry': 0}\n"
     ]
    }
   ],
   "source": [
    "# Count null values in each column\n",
    "null_counts = {col: X_train[col].isnull().sum() for col in X_train.columns}\n",
    "\n",
    "# Sort the dictionary in descending order based on the values\n",
    "sorted_null_counts = dict(sorted(null_counts.items(), key=lambda item: item[1], reverse=True))\n",
    "\n",
    "print(sorted_null_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.drop('geometry', axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = X_test.drop('geometry', axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_results_table(coef_input, key_input, name_input):\n",
    "    features_df = pd.DataFrame([key_input, coef_input]).T\n",
    "    features_df.columns = ['Feature', 'Coeff']\n",
    "\n",
    "    features_df = features_df.iloc[features_df['Coeff'].abs().argsort()[::-1]]\n",
    "    features_df.to_csv(f'FeatureImportanceResults/{name_input}.csv')\n",
    "\n",
    "\n",
    "    # Open the existing spreadsheet file\n",
    "    #file_path = 'ResultsMuniCv.xlsx'\n",
    "    #book = pd.read_excel(file_path, sheet_name=None)\n",
    "\n",
    "    # Add the new sheet to the spreadsheet\n",
    "    #sheet_name = name_input\n",
    "    #book[sheet_name] = features_df\n",
    "\n",
    "    # Save the updated spreadsheet\n",
    "    #with pd.ExcelWriter(file_path) as writer:\n",
    "    #    for sheet, df in book.items():\n",
    "    #        features_df.to_excel(writer, sheet_name=sheet, index=False)\n",
    "\n",
    "\n",
    "    # # Open the existing Excel file\n",
    "    # writer = pd.ExcelWriter('ResultsMuniCV.xlsx', engine='openpyxl')\n",
    "\n",
    "    # # Write the DataFrame to a specific sheet\n",
    "    # features_df.to_excel(writer, sheet_name=name_input)\n",
    "\n",
    "    # # Save the changes\n",
    "    # writer.save()\n",
    "\n",
    "    return features_df\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "#base_learners = [('ls', ls), ('el', el), ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_learners = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "#from sklearn.inspection import permutation_importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline([\n",
    "                     ('scaler',StandardScaler()),\n",
    "                     ('model',RandomForestRegressor(n_estimators = 500))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "search = GridSearchCV(pipeline,\n",
    "                      {'model__max_depth': np.arange(3,11,8) },\n",
    "                      cv = muni_cv, scoring = \"neg_mean_squared_error\",verbose = 3\n",
    "                      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "[CV 1/5] END ...............model__max_depth=3;, score=-0.031 total time=   0.8s\n",
      "[CV 2/5] END ...............model__max_depth=3;, score=-0.011 total time=   0.7s\n",
      "[CV 3/5] END ...............model__max_depth=3;, score=-0.005 total time=   0.7s\n",
      "[CV 4/5] END ...............model__max_depth=3;, score=-0.000 total time=   0.7s\n",
      "[CV 5/5] END ...............model__max_depth=3;, score=-0.002 total time=   0.7s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-10 {color: black;background-color: white;}#sk-container-id-10 pre{padding: 0;}#sk-container-id-10 div.sk-toggleable {background-color: white;}#sk-container-id-10 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-10 label.sk-toggleable__label-arrow:before {content: \"â–¸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-10 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-10 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-10 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-10 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-10 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-10 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"â–¾\";}#sk-container-id-10 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-10 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-10 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-10 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-10 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-10 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-10 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-10 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-10 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-10 div.sk-item {position: relative;z-index: 1;}#sk-container-id-10 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-10 div.sk-item::before, #sk-container-id-10 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-10 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-10 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-10 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-10 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-10 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-10 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-10 div.sk-label-container {text-align: center;}#sk-container-id-10 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-10 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-10\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GridSearchCV(cv=[(array([ 0,  1,  2,  3,  4,  5,  6,  8, 10, 11, 12, 13, 14, 15, 16, 17, 18,\n",
       "       19, 21, 22, 23, 24, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37,\n",
       "       38, 40, 42, 43, 47, 48, 49, 50, 51, 52, 53, 54, 57, 58, 60, 61, 63,\n",
       "       64, 65, 67, 68, 69]),\n",
       "                  array([ 7,  9, 20, 25, 39, 41, 44, 45, 46, 55, 56, 59, 62, 66])),\n",
       "                 (array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 12, 13, 14, 15, 16, 17, 20,\n",
       "       21, 22, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 36, 37, 38, 39,\n",
       "       40, 41, 42, 44, 4...\n",
       "       21, 23, 24, 25, 26, 27, 28, 29, 31, 33, 34, 35, 36, 38, 39, 41, 43,\n",
       "       44, 45, 46, 47, 48, 50, 51, 52, 53, 55, 56, 57, 58, 59, 60, 61, 62,\n",
       "       63, 64, 66, 67, 68]),\n",
       "                  array([13, 14, 15, 16, 22, 30, 32, 37, 40, 42, 49, 54, 65, 69]))],\n",
       "             estimator=Pipeline(steps=[(&#x27;scaler&#x27;, StandardScaler()),\n",
       "                                       (&#x27;model&#x27;,\n",
       "                                        RandomForestRegressor(n_estimators=500))]),\n",
       "             param_grid={&#x27;model__max_depth&#x27;: array([3])},\n",
       "             scoring=&#x27;neg_mean_squared_error&#x27;, verbose=3)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-19\" type=\"checkbox\" ><label for=\"sk-estimator-id-19\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GridSearchCV</label><div class=\"sk-toggleable__content\"><pre>GridSearchCV(cv=[(array([ 0,  1,  2,  3,  4,  5,  6,  8, 10, 11, 12, 13, 14, 15, 16, 17, 18,\n",
       "       19, 21, 22, 23, 24, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37,\n",
       "       38, 40, 42, 43, 47, 48, 49, 50, 51, 52, 53, 54, 57, 58, 60, 61, 63,\n",
       "       64, 65, 67, 68, 69]),\n",
       "                  array([ 7,  9, 20, 25, 39, 41, 44, 45, 46, 55, 56, 59, 62, 66])),\n",
       "                 (array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 12, 13, 14, 15, 16, 17, 20,\n",
       "       21, 22, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 36, 37, 38, 39,\n",
       "       40, 41, 42, 44, 4...\n",
       "       21, 23, 24, 25, 26, 27, 28, 29, 31, 33, 34, 35, 36, 38, 39, 41, 43,\n",
       "       44, 45, 46, 47, 48, 50, 51, 52, 53, 55, 56, 57, 58, 59, 60, 61, 62,\n",
       "       63, 64, 66, 67, 68]),\n",
       "                  array([13, 14, 15, 16, 22, 30, 32, 37, 40, 42, 49, 54, 65, 69]))],\n",
       "             estimator=Pipeline(steps=[(&#x27;scaler&#x27;, StandardScaler()),\n",
       "                                       (&#x27;model&#x27;,\n",
       "                                        RandomForestRegressor(n_estimators=500))]),\n",
       "             param_grid={&#x27;model__max_depth&#x27;: array([3])},\n",
       "             scoring=&#x27;neg_mean_squared_error&#x27;, verbose=3)</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-20\" type=\"checkbox\" ><label for=\"sk-estimator-id-20\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;scaler&#x27;, StandardScaler()),\n",
       "                (&#x27;model&#x27;, RandomForestRegressor(n_estimators=500))])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-21\" type=\"checkbox\" ><label for=\"sk-estimator-id-21\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">StandardScaler</label><div class=\"sk-toggleable__content\"><pre>StandardScaler()</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-22\" type=\"checkbox\" ><label for=\"sk-estimator-id-22\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestRegressor</label><div class=\"sk-toggleable__content\"><pre>RandomForestRegressor(n_estimators=500)</pre></div></div></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "GridSearchCV(cv=[(array([ 0,  1,  2,  3,  4,  5,  6,  8, 10, 11, 12, 13, 14, 15, 16, 17, 18,\n",
       "       19, 21, 22, 23, 24, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37,\n",
       "       38, 40, 42, 43, 47, 48, 49, 50, 51, 52, 53, 54, 57, 58, 60, 61, 63,\n",
       "       64, 65, 67, 68, 69]),\n",
       "                  array([ 7,  9, 20, 25, 39, 41, 44, 45, 46, 55, 56, 59, 62, 66])),\n",
       "                 (array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 12, 13, 14, 15, 16, 17, 20,\n",
       "       21, 22, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 36, 37, 38, 39,\n",
       "       40, 41, 42, 44, 4...\n",
       "       21, 23, 24, 25, 26, 27, 28, 29, 31, 33, 34, 35, 36, 38, 39, 41, 43,\n",
       "       44, 45, 46, 47, 48, 50, 51, 52, 53, 55, 56, 57, 58, 59, 60, 61, 62,\n",
       "       63, 64, 66, 67, 68]),\n",
       "                  array([13, 14, 15, 16, 22, 30, 32, 37, 40, 42, 49, 54, 65, 69]))],\n",
       "             estimator=Pipeline(steps=[('scaler', StandardScaler()),\n",
       "                                       ('model',\n",
       "                                        RandomForestRegressor(n_estimators=500))]),\n",
       "             param_grid={'model__max_depth': array([3])},\n",
       "             scoring='neg_mean_squared_error', verbose=3)"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search.fit(X_train,Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = RandomForestRegressor(n_estimators = 500, max_depth = search.best_params_['model__max_depth'])\n",
    "#model.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_learners.append(('randomforest', search.best_estimator_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['n_companies_FOOD AND DRINKS', 'n_jobs_COMPUTING', 'emiss_agropec_full',\n",
       "       'n_companies_TOTAL SERVICE', 'n_jobs_TOTAL SERVICE', 'n_jobs_TOTAL',\n",
       "       'n_jobs_RESEARCH AND DEVELOPMENT', 'n_jobs_METALLIC PRODUCTS',\n",
       "       'n_jobs_INTERNATIONAL BODIES'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coefficients = search.best_estimator_._final_estimator.feature_importances_\n",
    "importance = np.abs(coefficients)\n",
    "\n",
    "randomforest_features_df = generate_results_table(coefficients, X_train.columns, 'randomforest')\n",
    "\n",
    "X_train.columns[np.argsort(np.array(abs( importance )))[::-1][0:9]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline([\n",
    "                     ('scaler',StandardScaler()),\n",
    "                     ('model',Lasso())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "search = GridSearchCV(pipeline,\n",
    "                      {'model__alpha':[1e-15,1e-10,1e-8,1e-3,1e-2,1,5,10,20,30,35,40,45,50,55,100]},\n",
    "                      cv = muni_cv, scoring = \"neg_mean_squared_error\",verbose = 3\n",
    "                      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 16 candidates, totalling 80 fits\n",
      "[CV 1/5] END ...............model__alpha=1e-15;, score=-0.022 total time=   0.0s\n",
      "[CV 2/5] END ...............model__alpha=1e-15;, score=-0.017 total time=   0.0s\n",
      "[CV 3/5] END ...............model__alpha=1e-15;, score=-0.027 total time=   0.0s\n",
      "[CV 4/5] END ...............model__alpha=1e-15;, score=-0.018 total time=   0.0s\n",
      "[CV 5/5] END ...............model__alpha=1e-15;, score=-0.012 total time=   0.0s\n",
      "[CV 1/5] END ...............model__alpha=1e-10;, score=-0.022 total time=   0.0s\n",
      "[CV 2/5] END ...............model__alpha=1e-10;, score=-0.017 total time=   0.0s\n",
      "[CV 3/5] END ...............model__alpha=1e-10;, score=-0.027 total time=   0.0s\n",
      "[CV 4/5] END ...............model__alpha=1e-10;, score=-0.018 total time=   0.0s\n",
      "[CV 5/5] END ...............model__alpha=1e-10;, score=-0.012 total time=   0.0s\n",
      "[CV 1/5] END ...............model__alpha=1e-08;, score=-0.022 total time=   0.0s\n",
      "[CV 2/5] END ...............model__alpha=1e-08;, score=-0.017 total time=   0.0s\n",
      "[CV 3/5] END ...............model__alpha=1e-08;, score=-0.027 total time=   0.0s\n",
      "[CV 4/5] END ...............model__alpha=1e-08;, score=-0.018 total time=   0.0s\n",
      "[CV 5/5] END ...............model__alpha=1e-08;, score=-0.012 total time=   0.0s\n",
      "[CV 1/5] END ...............model__alpha=0.001;, score=-0.004 total time=   0.0s\n",
      "[CV 2/5] END ...............model__alpha=0.001;, score=-0.036 total time=   0.0s\n",
      "[CV 3/5] END ...............model__alpha=0.001;, score=-0.008 total time=   0.0s\n",
      "[CV 4/5] END ...............model__alpha=0.001;, score=-0.008 total time=   0.0s\n",
      "[CV 5/5] END ...............model__alpha=0.001;, score=-0.008 total time=   0.0s\n",
      "[CV 1/5] END ................model__alpha=0.01;, score=-0.003 total time=   0.0s\n",
      "[CV 2/5] END ................model__alpha=0.01;, score=-0.037 total time=   0.0s\n",
      "[CV 3/5] END ................model__alpha=0.01;, score=-0.004 total time=   0.0s\n",
      "[CV 4/5] END ................model__alpha=0.01;, score=-0.006 total time=   0.0s\n",
      "[CV 5/5] END ................model__alpha=0.01;, score=-0.002 total time=   0.0s\n",
      "[CV 1/5] END ...................model__alpha=1;, score=-0.039 total time=   0.0s\n",
      "[CV 2/5] END ...................model__alpha=1;, score=-0.009 total time=   0.0s\n",
      "[CV 3/5] END ...................model__alpha=1;, score=-0.004 total time=   0.0s\n",
      "[CV 4/5] END ...................model__alpha=1;, score=-0.005 total time=   0.0s\n",
      "[CV 5/5] END ...................model__alpha=1;, score=-0.002 total time=   0.0s\n",
      "[CV 1/5] END ...................model__alpha=5;, score=-0.039 total time=   0.0s\n",
      "[CV 2/5] END ...................model__alpha=5;, score=-0.009 total time=   0.0s\n",
      "[CV 3/5] END ...................model__alpha=5;, score=-0.004 total time=   0.0s\n",
      "[CV 4/5] END ...................model__alpha=5;, score=-0.005 total time=   0.0s\n",
      "[CV 5/5] END ...................model__alpha=5;, score=-0.002 total time=   0.0s\n",
      "[CV 1/5] END ..................model__alpha=10;, score=-0.039 total time=   0.0s\n",
      "[CV 2/5] END ..................model__alpha=10;, score=-0.009 total time=   0.0s\n",
      "[CV 3/5] END ..................model__alpha=10;, score=-0.004 total time=   0.0s\n",
      "[CV 4/5] END ..................model__alpha=10;, score=-0.005 total time=   0.0s\n",
      "[CV 5/5] END ..................model__alpha=10;, score=-0.002 total time=   0.0s\n",
      "[CV 1/5] END ..................model__alpha=20;, score=-0.039 total time=   0.0s\n",
      "[CV 2/5] END ..................model__alpha=20;, score=-0.009 total time=   0.0s\n",
      "[CV 3/5] END ..................model__alpha=20;, score=-0.004 total time=   0.0s\n",
      "[CV 4/5] END ..................model__alpha=20;, score=-0.005 total time=   0.0s\n",
      "[CV 5/5] END ..................model__alpha=20;, score=-0.002 total time=   0.0s\n",
      "[CV 1/5] END ..................model__alpha=30;, score=-0.039 total time=   0.0s\n",
      "[CV 2/5] END ..................model__alpha=30;, score=-0.009 total time=   0.0s\n",
      "[CV 3/5] END ..................model__alpha=30;, score=-0.004 total time=   0.0s\n",
      "[CV 4/5] END ..................model__alpha=30;, score=-0.005 total time=   0.0s\n",
      "[CV 5/5] END ..................model__alpha=30;, score=-0.002 total time=   0.0s\n",
      "[CV 1/5] END ..................model__alpha=35;, score=-0.039 total time=   0.0s\n",
      "[CV 2/5] END ..................model__alpha=35;, score=-0.009 total time=   0.0s\n",
      "[CV 3/5] END ..................model__alpha=35;, score=-0.004 total time=   0.0s\n",
      "[CV 4/5] END ..................model__alpha=35;, score=-0.005 total time=   0.0s\n",
      "[CV 5/5] END ..................model__alpha=35;, score=-0.002 total time=   0.0s\n",
      "[CV 1/5] END ..................model__alpha=40;, score=-0.039 total time=   0.0s\n",
      "[CV 2/5] END ..................model__alpha=40;, score=-0.009 total time=   0.0s\n",
      "[CV 3/5] END ..................model__alpha=40;, score=-0.004 total time=   0.0s\n",
      "[CV 4/5] END ..................model__alpha=40;, score=-0.005 total time=   0.0s\n",
      "[CV 5/5] END ..................model__alpha=40;, score=-0.002 total time=   0.0s\n",
      "[CV 1/5] END ..................model__alpha=45;, score=-0.039 total time=   0.0s\n",
      "[CV 2/5] END ..................model__alpha=45;, score=-0.009 total time=   0.0s\n",
      "[CV 3/5] END ..................model__alpha=45;, score=-0.004 total time=   0.0s\n",
      "[CV 4/5] END ..................model__alpha=45;, score=-0.005 total time=   0.0s\n",
      "[CV 5/5] END ..................model__alpha=45;, score=-0.002 total time=   0.0s\n",
      "[CV 1/5] END ..................model__alpha=50;, score=-0.039 total time=   0.0s\n",
      "[CV 2/5] END ..................model__alpha=50;, score=-0.009 total time=   0.0s\n",
      "[CV 3/5] END ..................model__alpha=50;, score=-0.004 total time=   0.0s\n",
      "[CV 4/5] END ..................model__alpha=50;, score=-0.005 total time=   0.0s\n",
      "[CV 5/5] END ..................model__alpha=50;, score=-0.002 total time=   0.0s\n",
      "[CV 1/5] END ..................model__alpha=55;, score=-0.039 total time=   0.0s\n",
      "[CV 2/5] END ..................model__alpha=55;, score=-0.009 total time=   0.0s\n",
      "[CV 3/5] END ..................model__alpha=55;, score=-0.004 total time=   0.0s\n",
      "[CV 4/5] END ..................model__alpha=55;, score=-0.005 total time=   0.0s\n",
      "[CV 5/5] END ..................model__alpha=55;, score=-0.002 total time=   0.0s\n",
      "[CV 1/5] END .................model__alpha=100;, score=-0.039 total time=   0.0s\n",
      "[CV 2/5] END .................model__alpha=100;, score=-0.009 total time=   0.0s\n",
      "[CV 3/5] END .................model__alpha=100;, score=-0.004 total time=   0.0s\n",
      "[CV 4/5] END .................model__alpha=100;, score=-0.005 total time=   0.0s\n",
      "[CV 5/5] END .................model__alpha=100;, score=-0.002 total time=   0.0s\n"
     ]
    }
   ],
   "source": [
    "search.fit(X_train,Y_train)\n",
    "base_learners.append(('lasso', search.best_estimator_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['n_jobs_SMOKE', 'n_jobs_METALLIC PRODUCTS', 'emiss_agropec_full',\n",
       "       'n_jobs_COMPUTING', 'n_companies_FOOD AND DRINKS',\n",
       "       'overall_winner_partido_DEM', 'runnerup_partido_PSDB',\n",
       "       'n_companies_METALLIC PRODUCTS', 'audits'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coefficients = search.best_estimator_.named_steps['model'].coef_\n",
    "importance = np.abs(coefficients)\n",
    "\n",
    "lasso_features_df = generate_results_table(coefficients, X_train.columns, 'lasso')\n",
    "\n",
    "X_train.columns[np.argsort(np.array(abs( importance )))[::-1][0:9]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "from sklearn.ensemble import GradientBoostingRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline([\n",
    "                     ('scaler',StandardScaler()),\n",
    "                     ('model',GradientBoostingRegressor(learning_rate = 0.1, min_samples_leaf = 2))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "search = GridSearchCV(pipeline,\n",
    "                      {'model__n_estimators':np.arange(50, 150, 50), 'model__max_depth':np.arange(3, 5, 1)},\n",
    "                      cv = muni_cv, scoring = \"neg_mean_squared_error\",verbose = 3\n",
    "                      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 4 candidates, totalling 20 fits\n",
      "[CV 1/5] END model__max_depth=3, model__n_estimators=50;, score=-0.040 total time=   0.1s\n",
      "[CV 2/5] END model__max_depth=3, model__n_estimators=50;, score=-0.010 total time=   0.1s\n",
      "[CV 3/5] END model__max_depth=3, model__n_estimators=50;, score=-0.007 total time=   0.1s\n",
      "[CV 4/5] END model__max_depth=3, model__n_estimators=50;, score=-0.002 total time=   0.1s\n",
      "[CV 5/5] END model__max_depth=3, model__n_estimators=50;, score=-0.002 total time=   0.1s\n",
      "[CV 1/5] END model__max_depth=3, model__n_estimators=100;, score=-0.040 total time=   0.1s\n",
      "[CV 2/5] END model__max_depth=3, model__n_estimators=100;, score=-0.010 total time=   0.1s\n",
      "[CV 3/5] END model__max_depth=3, model__n_estimators=100;, score=-0.010 total time=   0.1s\n",
      "[CV 4/5] END model__max_depth=3, model__n_estimators=100;, score=-0.002 total time=   0.1s\n",
      "[CV 5/5] END model__max_depth=3, model__n_estimators=100;, score=-0.001 total time=   0.1s\n",
      "[CV 1/5] END model__max_depth=4, model__n_estimators=50;, score=-0.041 total time=   0.1s\n",
      "[CV 2/5] END model__max_depth=4, model__n_estimators=50;, score=-0.010 total time=   0.1s\n",
      "[CV 3/5] END model__max_depth=4, model__n_estimators=50;, score=-0.011 total time=   0.1s\n",
      "[CV 4/5] END model__max_depth=4, model__n_estimators=50;, score=-0.003 total time=   0.1s\n",
      "[CV 5/5] END model__max_depth=4, model__n_estimators=50;, score=-0.003 total time=   0.1s\n",
      "[CV 1/5] END model__max_depth=4, model__n_estimators=100;, score=-0.041 total time=   0.1s\n",
      "[CV 2/5] END model__max_depth=4, model__n_estimators=100;, score=-0.014 total time=   0.1s\n",
      "[CV 3/5] END model__max_depth=4, model__n_estimators=100;, score=-0.007 total time=   0.1s\n",
      "[CV 4/5] END model__max_depth=4, model__n_estimators=100;, score=-0.003 total time=   0.1s\n",
      "[CV 5/5] END model__max_depth=4, model__n_estimators=100;, score=-0.002 total time=   0.2s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-11 {color: black;background-color: white;}#sk-container-id-11 pre{padding: 0;}#sk-container-id-11 div.sk-toggleable {background-color: white;}#sk-container-id-11 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-11 label.sk-toggleable__label-arrow:before {content: \"â–¸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-11 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-11 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-11 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-11 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-11 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-11 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"â–¾\";}#sk-container-id-11 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-11 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-11 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-11 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-11 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-11 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-11 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-11 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-11 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-11 div.sk-item {position: relative;z-index: 1;}#sk-container-id-11 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-11 div.sk-item::before, #sk-container-id-11 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-11 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-11 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-11 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-11 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-11 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-11 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-11 div.sk-label-container {text-align: center;}#sk-container-id-11 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-11 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-11\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GridSearchCV(cv=[(array([ 0,  1,  2,  3,  4,  5,  6,  8, 10, 11, 12, 13, 14, 15, 16, 17, 18,\n",
       "       19, 21, 22, 23, 24, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37,\n",
       "       38, 40, 42, 43, 47, 48, 49, 50, 51, 52, 53, 54, 57, 58, 60, 61, 63,\n",
       "       64, 65, 67, 68, 69]),\n",
       "                  array([ 7,  9, 20, 25, 39, 41, 44, 45, 46, 55, 56, 59, 62, 66])),\n",
       "                 (array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 12, 13, 14, 15, 16, 17, 20,\n",
       "       21, 22, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 36, 37, 38, 39,\n",
       "       40, 41, 42, 44, 4...\n",
       "       44, 45, 46, 47, 48, 50, 51, 52, 53, 55, 56, 57, 58, 59, 60, 61, 62,\n",
       "       63, 64, 66, 67, 68]),\n",
       "                  array([13, 14, 15, 16, 22, 30, 32, 37, 40, 42, 49, 54, 65, 69]))],\n",
       "             estimator=Pipeline(steps=[(&#x27;scaler&#x27;, StandardScaler()),\n",
       "                                       (&#x27;model&#x27;,\n",
       "                                        GradientBoostingRegressor(min_samples_leaf=2))]),\n",
       "             param_grid={&#x27;model__max_depth&#x27;: array([3, 4]),\n",
       "                         &#x27;model__n_estimators&#x27;: array([ 50, 100])},\n",
       "             scoring=&#x27;neg_mean_squared_error&#x27;, verbose=3)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-23\" type=\"checkbox\" ><label for=\"sk-estimator-id-23\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GridSearchCV</label><div class=\"sk-toggleable__content\"><pre>GridSearchCV(cv=[(array([ 0,  1,  2,  3,  4,  5,  6,  8, 10, 11, 12, 13, 14, 15, 16, 17, 18,\n",
       "       19, 21, 22, 23, 24, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37,\n",
       "       38, 40, 42, 43, 47, 48, 49, 50, 51, 52, 53, 54, 57, 58, 60, 61, 63,\n",
       "       64, 65, 67, 68, 69]),\n",
       "                  array([ 7,  9, 20, 25, 39, 41, 44, 45, 46, 55, 56, 59, 62, 66])),\n",
       "                 (array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 12, 13, 14, 15, 16, 17, 20,\n",
       "       21, 22, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 36, 37, 38, 39,\n",
       "       40, 41, 42, 44, 4...\n",
       "       44, 45, 46, 47, 48, 50, 51, 52, 53, 55, 56, 57, 58, 59, 60, 61, 62,\n",
       "       63, 64, 66, 67, 68]),\n",
       "                  array([13, 14, 15, 16, 22, 30, 32, 37, 40, 42, 49, 54, 65, 69]))],\n",
       "             estimator=Pipeline(steps=[(&#x27;scaler&#x27;, StandardScaler()),\n",
       "                                       (&#x27;model&#x27;,\n",
       "                                        GradientBoostingRegressor(min_samples_leaf=2))]),\n",
       "             param_grid={&#x27;model__max_depth&#x27;: array([3, 4]),\n",
       "                         &#x27;model__n_estimators&#x27;: array([ 50, 100])},\n",
       "             scoring=&#x27;neg_mean_squared_error&#x27;, verbose=3)</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-24\" type=\"checkbox\" ><label for=\"sk-estimator-id-24\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;scaler&#x27;, StandardScaler()),\n",
       "                (&#x27;model&#x27;, GradientBoostingRegressor(min_samples_leaf=2))])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-25\" type=\"checkbox\" ><label for=\"sk-estimator-id-25\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">StandardScaler</label><div class=\"sk-toggleable__content\"><pre>StandardScaler()</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-26\" type=\"checkbox\" ><label for=\"sk-estimator-id-26\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GradientBoostingRegressor</label><div class=\"sk-toggleable__content\"><pre>GradientBoostingRegressor(min_samples_leaf=2)</pre></div></div></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "GridSearchCV(cv=[(array([ 0,  1,  2,  3,  4,  5,  6,  8, 10, 11, 12, 13, 14, 15, 16, 17, 18,\n",
       "       19, 21, 22, 23, 24, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37,\n",
       "       38, 40, 42, 43, 47, 48, 49, 50, 51, 52, 53, 54, 57, 58, 60, 61, 63,\n",
       "       64, 65, 67, 68, 69]),\n",
       "                  array([ 7,  9, 20, 25, 39, 41, 44, 45, 46, 55, 56, 59, 62, 66])),\n",
       "                 (array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 12, 13, 14, 15, 16, 17, 20,\n",
       "       21, 22, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 36, 37, 38, 39,\n",
       "       40, 41, 42, 44, 4...\n",
       "       44, 45, 46, 47, 48, 50, 51, 52, 53, 55, 56, 57, 58, 59, 60, 61, 62,\n",
       "       63, 64, 66, 67, 68]),\n",
       "                  array([13, 14, 15, 16, 22, 30, 32, 37, 40, 42, 49, 54, 65, 69]))],\n",
       "             estimator=Pipeline(steps=[('scaler', StandardScaler()),\n",
       "                                       ('model',\n",
       "                                        GradientBoostingRegressor(min_samples_leaf=2))]),\n",
       "             param_grid={'model__max_depth': array([3, 4]),\n",
       "                         'model__n_estimators': array([ 50, 100])},\n",
       "             scoring='neg_mean_squared_error', verbose=3)"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search.fit(X_train,Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model__max_depth': 3, 'model__n_estimators': 50}"
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_learners.append(('gradientboosting', search.best_estimator_.named_steps['model']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['n_jobs_SMOKE', 'populacao', 'n_jobs_TOTAL SERVICE', 'f_emitted_count',\n",
       "       'emiss_agropec_full', 'n_companies_BASIC METALLURGY',\n",
       "       'n_companies_TOTAL SERVICE', 'n_companies_WHOLESALE',\n",
       "       'n_companies_TOTAL INDUSTRIAL'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coefficients = search.best_estimator_.named_steps['model'].feature_importances_\n",
    "importance = np.abs(coefficients)\n",
    "\n",
    "gradient_boosting_features_df = generate_results_table(coefficients, X_train.columns, 'gradientboosting')\n",
    "\n",
    "\n",
    "\n",
    "X.columns[np.argsort(np.array(abs( importance )))[::-1][0:9]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPRegressor\n",
    "import shap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline([\n",
    "                     ('scaler',StandardScaler()),\n",
    "                     ('model', MLPRegressor(activation = 'logistic', random_state=42))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "search = GridSearchCV(pipeline,\n",
    "                      {'model__hidden_layer_sizes':[(50,),(100,)], 'model__alpha':np.arange(0.00001, 0.001, 0.001)},\n",
    "                      cv = muni_cv, scoring = \"neg_mean_squared_error\",verbose = 3\n",
    "                      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n",
      "[CV 1/5] END model__alpha=1e-05, model__hidden_layer_sizes=(50,);, score=-0.049 total time=   0.1s\n",
      "[CV 2/5] END model__alpha=1e-05, model__hidden_layer_sizes=(50,);, score=-0.016 total time=   0.0s\n",
      "[CV 3/5] END model__alpha=1e-05, model__hidden_layer_sizes=(50,);, score=-0.053 total time=   0.0s\n",
      "[CV 4/5] END model__alpha=1e-05, model__hidden_layer_sizes=(50,);, score=-0.020 total time=   0.0s\n",
      "[CV 5/5] END model__alpha=1e-05, model__hidden_layer_sizes=(50,);, score=-0.019 total time=   0.0s\n",
      "[CV 1/5] END model__alpha=1e-05, model__hidden_layer_sizes=(100,);, score=-0.053 total time=   0.2s\n",
      "[CV 2/5] END model__alpha=1e-05, model__hidden_layer_sizes=(100,);, score=-0.019 total time=   0.0s\n",
      "[CV 3/5] END model__alpha=1e-05, model__hidden_layer_sizes=(100,);, score=-0.032 total time=   0.0s\n",
      "[CV 4/5] END model__alpha=1e-05, model__hidden_layer_sizes=(100,);, score=-0.041 total time=   0.0s\n",
      "[CV 5/5] END model__alpha=1e-05, model__hidden_layer_sizes=(100,);, score=-0.045 total time=   0.0s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-12 {color: black;background-color: white;}#sk-container-id-12 pre{padding: 0;}#sk-container-id-12 div.sk-toggleable {background-color: white;}#sk-container-id-12 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-12 label.sk-toggleable__label-arrow:before {content: \"â–¸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-12 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-12 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-12 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-12 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-12 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-12 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"â–¾\";}#sk-container-id-12 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-12 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-12 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-12 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-12 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-12 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-12 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-12 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-12 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-12 div.sk-item {position: relative;z-index: 1;}#sk-container-id-12 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-12 div.sk-item::before, #sk-container-id-12 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-12 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-12 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-12 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-12 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-12 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-12 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-12 div.sk-label-container {text-align: center;}#sk-container-id-12 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-12 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-12\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GridSearchCV(cv=[(array([ 0,  1,  2,  3,  4,  5,  6,  8, 10, 11, 12, 13, 14, 15, 16, 17, 18,\n",
       "       19, 21, 22, 23, 24, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37,\n",
       "       38, 40, 42, 43, 47, 48, 49, 50, 51, 52, 53, 54, 57, 58, 60, 61, 63,\n",
       "       64, 65, 67, 68, 69]),\n",
       "                  array([ 7,  9, 20, 25, 39, 41, 44, 45, 46, 55, 56, 59, 62, 66])),\n",
       "                 (array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 12, 13, 14, 15, 16, 17, 20,\n",
       "       21, 22, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 36, 37, 38, 39,\n",
       "       40, 41, 42, 44, 4...\n",
       "       44, 45, 46, 47, 48, 50, 51, 52, 53, 55, 56, 57, 58, 59, 60, 61, 62,\n",
       "       63, 64, 66, 67, 68]),\n",
       "                  array([13, 14, 15, 16, 22, 30, 32, 37, 40, 42, 49, 54, 65, 69]))],\n",
       "             estimator=Pipeline(steps=[(&#x27;scaler&#x27;, StandardScaler()),\n",
       "                                       (&#x27;model&#x27;,\n",
       "                                        MLPRegressor(activation=&#x27;logistic&#x27;,\n",
       "                                                     random_state=42))]),\n",
       "             param_grid={&#x27;model__alpha&#x27;: array([1.e-05]),\n",
       "                         &#x27;model__hidden_layer_sizes&#x27;: [(50,), (100,)]},\n",
       "             scoring=&#x27;neg_mean_squared_error&#x27;, verbose=3)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-27\" type=\"checkbox\" ><label for=\"sk-estimator-id-27\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GridSearchCV</label><div class=\"sk-toggleable__content\"><pre>GridSearchCV(cv=[(array([ 0,  1,  2,  3,  4,  5,  6,  8, 10, 11, 12, 13, 14, 15, 16, 17, 18,\n",
       "       19, 21, 22, 23, 24, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37,\n",
       "       38, 40, 42, 43, 47, 48, 49, 50, 51, 52, 53, 54, 57, 58, 60, 61, 63,\n",
       "       64, 65, 67, 68, 69]),\n",
       "                  array([ 7,  9, 20, 25, 39, 41, 44, 45, 46, 55, 56, 59, 62, 66])),\n",
       "                 (array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 12, 13, 14, 15, 16, 17, 20,\n",
       "       21, 22, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 36, 37, 38, 39,\n",
       "       40, 41, 42, 44, 4...\n",
       "       44, 45, 46, 47, 48, 50, 51, 52, 53, 55, 56, 57, 58, 59, 60, 61, 62,\n",
       "       63, 64, 66, 67, 68]),\n",
       "                  array([13, 14, 15, 16, 22, 30, 32, 37, 40, 42, 49, 54, 65, 69]))],\n",
       "             estimator=Pipeline(steps=[(&#x27;scaler&#x27;, StandardScaler()),\n",
       "                                       (&#x27;model&#x27;,\n",
       "                                        MLPRegressor(activation=&#x27;logistic&#x27;,\n",
       "                                                     random_state=42))]),\n",
       "             param_grid={&#x27;model__alpha&#x27;: array([1.e-05]),\n",
       "                         &#x27;model__hidden_layer_sizes&#x27;: [(50,), (100,)]},\n",
       "             scoring=&#x27;neg_mean_squared_error&#x27;, verbose=3)</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-28\" type=\"checkbox\" ><label for=\"sk-estimator-id-28\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;scaler&#x27;, StandardScaler()),\n",
       "                (&#x27;model&#x27;,\n",
       "                 MLPRegressor(activation=&#x27;logistic&#x27;, random_state=42))])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-29\" type=\"checkbox\" ><label for=\"sk-estimator-id-29\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">StandardScaler</label><div class=\"sk-toggleable__content\"><pre>StandardScaler()</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-30\" type=\"checkbox\" ><label for=\"sk-estimator-id-30\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MLPRegressor</label><div class=\"sk-toggleable__content\"><pre>MLPRegressor(activation=&#x27;logistic&#x27;, random_state=42)</pre></div></div></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "GridSearchCV(cv=[(array([ 0,  1,  2,  3,  4,  5,  6,  8, 10, 11, 12, 13, 14, 15, 16, 17, 18,\n",
       "       19, 21, 22, 23, 24, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37,\n",
       "       38, 40, 42, 43, 47, 48, 49, 50, 51, 52, 53, 54, 57, 58, 60, 61, 63,\n",
       "       64, 65, 67, 68, 69]),\n",
       "                  array([ 7,  9, 20, 25, 39, 41, 44, 45, 46, 55, 56, 59, 62, 66])),\n",
       "                 (array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 12, 13, 14, 15, 16, 17, 20,\n",
       "       21, 22, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 36, 37, 38, 39,\n",
       "       40, 41, 42, 44, 4...\n",
       "       44, 45, 46, 47, 48, 50, 51, 52, 53, 55, 56, 57, 58, 59, 60, 61, 62,\n",
       "       63, 64, 66, 67, 68]),\n",
       "                  array([13, 14, 15, 16, 22, 30, 32, 37, 40, 42, 49, 54, 65, 69]))],\n",
       "             estimator=Pipeline(steps=[('scaler', StandardScaler()),\n",
       "                                       ('model',\n",
       "                                        MLPRegressor(activation='logistic',\n",
       "                                                     random_state=42))]),\n",
       "             param_grid={'model__alpha': array([1.e-05]),\n",
       "                         'model__hidden_layer_sizes': [(50,), (100,)]},\n",
       "             scoring='neg_mean_squared_error', verbose=3)"
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search.fit(X_train,Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "base_learners.append(('neuralnetwork', search.best_estimator_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "X does not have valid feature names, but StandardScaler was fitted with feature names\n"
     ]
    }
   ],
   "source": [
    "explainer = shap.KernelExplainer(search.best_estimator_.predict, X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/30 [00:00<?, ?it/s]X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  3%|â–Ž         | 1/30 [00:00<00:19,  1.50it/s]X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  7%|â–‹         | 2/30 [00:01<00:17,  1.63it/s]X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 10%|â–ˆ         | 3/30 [00:01<00:16,  1.65it/s]X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 13%|â–ˆâ–Ž        | 4/30 [00:02<00:15,  1.66it/s]X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 17%|â–ˆâ–‹        | 5/30 [00:03<00:14,  1.70it/s]X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 20%|â–ˆâ–ˆ        | 6/30 [00:03<00:13,  1.72it/s]X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 42 iterations, i.e. alpha=3.927e-05, with an active set of 30 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 43 iterations, i.e. alpha=3.899e-05, with an active set of 31 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 54 iterations, i.e. alpha=2.259e-05, with an active set of 36 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 61 iterations, alpha=1.848e-05, previous alpha=1.835e-05, with an active set of 40 regressors.\n",
      " 23%|â–ˆâ–ˆâ–Ž       | 7/30 [00:04<00:13,  1.71it/s]X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=4.657e-04, with an active set of 7 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 10 iterations, i.e. alpha=2.962e-04, with an active set of 10 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 30 iterations, alpha=8.493e-05, previous alpha=8.169e-05, with an active set of 21 regressors.\n",
      " 27%|â–ˆâ–ˆâ–‹       | 8/30 [00:04<00:12,  1.78it/s]X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 30%|â–ˆâ–ˆâ–ˆ       | 9/30 [00:05<00:11,  1.79it/s]X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 10/30 [00:05<00:11,  1.81it/s]X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 37%|â–ˆâ–ˆâ–ˆâ–‹      | 11/30 [00:06<00:10,  1.89it/s]X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 12/30 [00:06<00:09,  1.92it/s]X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 13/30 [00:07<00:08,  1.97it/s]X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 14/30 [00:07<00:07,  2.04it/s]X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 15/30 [00:08<00:07,  2.01it/s]X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 2 iterations, i.e. alpha=2.069e-03, with an active set of 2 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 74 iterations, alpha=4.965e-04, previous alpha=4.964e-04, with an active set of 33 regressors.\n",
      " 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 16/30 [00:08<00:06,  2.08it/s]X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 17/30 [00:09<00:06,  2.09it/s]X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 49 iterations, i.e. alpha=2.358e-05, with an active set of 31 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 51 iterations, i.e. alpha=2.290e-05, with an active set of 33 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 63 iterations, i.e. alpha=1.216e-05, with an active set of 39 regressors, and the smallest cholesky pivot element being 2.980e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 63 iterations, i.e. alpha=1.185e-05, with an active set of 39 regressors, and the smallest cholesky pivot element being 4.712e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 64 iterations, i.e. alpha=1.173e-05, with an active set of 40 regressors, and the smallest cholesky pivot element being 7.525e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 64 iterations, i.e. alpha=1.152e-05, with an active set of 40 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 64 iterations, i.e. alpha=1.149e-05, with an active set of 40 regressors, and the smallest cholesky pivot element being 7.146e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 64 iterations, i.e. alpha=1.128e-05, with an active set of 40 regressors, and the smallest cholesky pivot element being 9.003e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 65 iterations, i.e. alpha=1.110e-05, with an active set of 41 regressors, and the smallest cholesky pivot element being 6.144e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 65 iterations, i.e. alpha=1.088e-05, with an active set of 41 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 65 iterations, i.e. alpha=1.081e-05, with an active set of 41 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 65 iterations, i.e. alpha=1.062e-05, with an active set of 41 regressors, and the smallest cholesky pivot element being 5.475e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 65 iterations, i.e. alpha=1.052e-05, with an active set of 41 regressors, and the smallest cholesky pivot element being 3.332e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 65 iterations, i.e. alpha=1.049e-05, with an active set of 41 regressors, and the smallest cholesky pivot element being 3.650e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 65 iterations, i.e. alpha=1.043e-05, with an active set of 41 regressors, and the smallest cholesky pivot element being 3.942e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 65 iterations, i.e. alpha=1.033e-05, with an active set of 41 regressors, and the smallest cholesky pivot element being 6.144e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 65 iterations, i.e. alpha=1.032e-05, with an active set of 41 regressors, and the smallest cholesky pivot element being 4.593e-08. Reduce max_iter or increase eps parameters.\n",
      "Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 66 iterations, alpha=1.201e-05, previous alpha=1.025e-05, with an active set of 41 regressors.\n",
      " 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 18/30 [00:09<00:05,  2.07it/s]X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 19/30 [00:10<00:05,  2.14it/s]X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 20/30 [00:10<00:04,  2.13it/s]X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 21/30 [00:10<00:04,  2.12it/s]X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 22/30 [00:11<00:03,  2.15it/s]X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 23/30 [00:11<00:03,  2.11it/s]X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=3.424e-04, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=2.062e-04, with an active set of 9 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 11 iterations, i.e. alpha=1.702e-04, with an active set of 11 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 19 iterations, i.e. alpha=1.092e-04, with an active set of 17 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 22 iterations, i.e. alpha=7.653e-05, with an active set of 20 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 23 iterations, i.e. alpha=7.392e-05, with an active set of 21 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 23 iterations, i.e. alpha=7.392e-05, with an active set of 21 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 26 iterations, alpha=6.923e-05, previous alpha=6.748e-05, with an active set of 23 regressors.\n",
      " 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 24/30 [00:12<00:02,  2.12it/s]X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 11 iterations, i.e. alpha=2.965e-04, with an active set of 11 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 20 iterations, i.e. alpha=1.918e-04, with an active set of 18 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 23 iterations, i.e. alpha=1.483e-04, with an active set of 19 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 27 iterations, i.e. alpha=9.473e-05, with an active set of 21 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 34 iterations, i.e. alpha=4.768e-05, with an active set of 26 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 45 iterations, alpha=3.943e-05, previous alpha=3.868e-05, with an active set of 32 regressors.\n",
      " 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 25/30 [00:12<00:02,  2.16it/s]X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 26/30 [00:13<00:01,  2.11it/s]X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 27/30 [00:13<00:01,  2.12it/s]X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 28/30 [00:14<00:00,  2.15it/s]X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 29/30 [00:14<00:00,  2.09it/s]X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 30/30 [00:15<00:00,  1.97it/s]\n"
     ]
    }
   ],
   "source": [
    "shap_values = explainer.shap_values(X_test, nsamples=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 576x684 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "shap.summary_plot(shap_values,X_test,feature_names=X_test.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = X_train.columns\n",
    "\n",
    "rf_resultX = pd.DataFrame(shap_values, columns = feature_names)\n",
    "\n",
    "vals = np.abs(rf_resultX.values).mean(0)\n",
    "\n",
    "shap_importance = pd.DataFrame(list(zip(feature_names, vals)),\n",
    "                                  columns=['col_name','feature_importance_vals'])\n",
    "shap_importance.sort_values(by=['feature_importance_vals'],\n",
    "                               ascending=False, inplace=True)\n",
    "\n",
    "#shap_importance.to_csv('FeatureInportanceResults/neuralnetwork.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_df = generate_results_table(np.array(shap_importance.feature_importance_vals), np.array(shap_importance.col_name), 'neuralnetwork')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Super Learner Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_models():\n",
    "    base_models = []\n",
    "    #Random forest regressor\n",
    "    base_models.append(base_learners[0][1][1])\n",
    "    #Lasso\n",
    "    base_models.append(base_learners[1][1][1])\n",
    "    #Gradient Boosting\n",
    "    base_models.append(base_learners[2][1])\n",
    "    #NeuralNetwork\n",
    "    base_models.append(base_learners[3][1][1])\n",
    "    return base_models\n",
    "\n",
    "def get_out_of_fold_predictions(X_train, Y_train, base_models):\n",
    "    meta_X = []\n",
    "    meta_Y = []\n",
    "\n",
    "    # enumerate splits\n",
    "    for train_ix, test_ix in muni_cv:\n",
    "        fold_yhats = []\n",
    "        meta_train_X, meta_test_X = X_train.iloc[train_ix], X_train.iloc[test_ix]\n",
    "        meta_train_Y, meta_test_Y = Y_train.iloc[train_ix], Y_train.iloc[test_ix]\n",
    "        meta_Y.extend(meta_test_Y)\n",
    "\n",
    "        # fit and make predictions with each sub-model\n",
    "        for model in base_models:\n",
    "            model.fit(meta_train_X, meta_train_Y)\n",
    "            yhat = model.predict(meta_test_X)\n",
    "            # store columns\n",
    "            fold_yhats.append(yhat.reshape(len(yhat),1))\n",
    "       \n",
    "        meta_X.append(np.hstack(fold_yhats))\n",
    "            \n",
    "    return np.vstack(meta_X), np.asarray(meta_Y)\n",
    "\n",
    "def super_learner_predictions(X, models, meta_model):\n",
    "\tmeta_X = []\n",
    "\tfor model in models:\n",
    "\t\tyhat = model.predict(X) \n",
    "\t\tmeta_X.append(yhat)\n",
    "\t# predict\n",
    "\treturn meta_model.predict(pd.DataFrame(meta_X).T)\n",
    "    \n",
    "def fit_base_models(X, y, models):\n",
    "\tfor model in models:\n",
    "\t\tmodel.fit(X, y)\n",
    " \n",
    "\n",
    "def fit_meta_model(X, y):\n",
    "\tmodel = Ridge()\n",
    "\tmodel.fit(X, y)\n",
    "\treturn model\n",
    "\n",
    "def evaluate_models(X, y, models):\n",
    "\tfor model in models:\n",
    "\t\tyhat = model.predict(X)\n",
    "\t\tmse = mean_squared_error(y, yhat)\n",
    "\t\tprint('%s: %.3f' % (model.__class__.__name__, mse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.444e-02, tolerance: 2.586e-05\n",
      "Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.552e-02, tolerance: 6.735e-05\n",
      "Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.766e-02, tolerance: 7.407e-05\n",
      "Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.481e-02, tolerance: 7.280e-05\n",
      "Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.578e-02, tolerance: 7.697e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta Data Shape:  (70, 4) (70,)\n",
      "RandomForestRegressor: 0.001\n",
      "Lasso: 0.003\n",
      "GradientBoostingRegressor: 0.001\n",
      "MLPRegressor: 0.009\n",
      "MSE: 0.001342312678391937\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.217e-02, tolerance: 7.937e-05\n"
     ]
    }
   ],
   "source": [
    "# get models\n",
    "models = get_models()\n",
    "# get out of fold predictions\n",
    "meta_X, meta_y = get_out_of_fold_predictions(X_train, Y_train, models)\n",
    "print('Meta Data Shape: ', meta_X.shape, meta_y.shape)\n",
    "\n",
    "fit_base_models(X_train, Y_train, models)\n",
    "meta_model = fit_meta_model(meta_X, meta_y)\n",
    "\n",
    "evaluate_models(X_test, Y_test, models)\n",
    "\n",
    "yhat = super_learner_predictions(X_test, models, meta_model)\n",
    "\n",
    "# Evaluate the performance of the model\n",
    "mse = mean_squared_error(Y_test, yhat)\n",
    "print(\"MSE:\", mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "X does not have valid feature names, but GradientBoostingRegressor was fitted with feature names\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]X does not have valid feature names, but GradientBoostingRegressor was fitted with feature names\n",
      "X does not have valid feature names, but GradientBoostingRegressor was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  3%|â–Ž         | 1/30 [00:00<00:15,  1.82it/s]X does not have valid feature names, but GradientBoostingRegressor was fitted with feature names\n",
      "X does not have valid feature names, but GradientBoostingRegressor was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  7%|â–‹         | 2/30 [00:01<00:14,  1.91it/s]X does not have valid feature names, but GradientBoostingRegressor was fitted with feature names\n",
      "X does not have valid feature names, but GradientBoostingRegressor was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 10%|â–ˆ         | 3/30 [00:01<00:13,  2.01it/s]X does not have valid feature names, but GradientBoostingRegressor was fitted with feature names\n",
      "X does not have valid feature names, but GradientBoostingRegressor was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 13%|â–ˆâ–Ž        | 4/30 [00:02<00:13,  2.00it/s]X does not have valid feature names, but GradientBoostingRegressor was fitted with feature names\n",
      "X does not have valid feature names, but GradientBoostingRegressor was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 17%|â–ˆâ–‹        | 5/30 [00:02<00:12,  2.01it/s]X does not have valid feature names, but GradientBoostingRegressor was fitted with feature names\n",
      "X does not have valid feature names, but GradientBoostingRegressor was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 20%|â–ˆâ–ˆ        | 6/30 [00:02<00:11,  2.07it/s]X does not have valid feature names, but GradientBoostingRegressor was fitted with feature names\n",
      "X does not have valid feature names, but GradientBoostingRegressor was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 23%|â–ˆâ–ˆâ–Ž       | 7/30 [00:03<00:11,  1.98it/s]X does not have valid feature names, but GradientBoostingRegressor was fitted with feature names\n",
      "X does not have valid feature names, but GradientBoostingRegressor was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 27%|â–ˆâ–ˆâ–‹       | 8/30 [00:04<00:11,  1.97it/s]X does not have valid feature names, but GradientBoostingRegressor was fitted with feature names\n",
      "X does not have valid feature names, but GradientBoostingRegressor was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 30%|â–ˆâ–ˆâ–ˆ       | 9/30 [00:04<00:10,  2.03it/s]X does not have valid feature names, but GradientBoostingRegressor was fitted with feature names\n",
      "X does not have valid feature names, but GradientBoostingRegressor was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 10/30 [00:05<00:09,  2.00it/s]X does not have valid feature names, but GradientBoostingRegressor was fitted with feature names\n",
      "X does not have valid feature names, but GradientBoostingRegressor was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 37%|â–ˆâ–ˆâ–ˆâ–‹      | 11/30 [00:05<00:09,  2.00it/s]X does not have valid feature names, but GradientBoostingRegressor was fitted with feature names\n",
      "X does not have valid feature names, but GradientBoostingRegressor was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 12/30 [00:05<00:08,  2.05it/s]X does not have valid feature names, but GradientBoostingRegressor was fitted with feature names\n",
      "X does not have valid feature names, but GradientBoostingRegressor was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 13/30 [00:06<00:08,  2.03it/s]X does not have valid feature names, but GradientBoostingRegressor was fitted with feature names\n",
      "X does not have valid feature names, but GradientBoostingRegressor was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 14/30 [00:06<00:07,  2.06it/s]X does not have valid feature names, but GradientBoostingRegressor was fitted with feature names\n",
      "X does not have valid feature names, but GradientBoostingRegressor was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 15/30 [00:07<00:07,  2.07it/s]X does not have valid feature names, but GradientBoostingRegressor was fitted with feature names\n",
      "X does not have valid feature names, but GradientBoostingRegressor was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 16/30 [00:07<00:06,  2.04it/s]X does not have valid feature names, but GradientBoostingRegressor was fitted with feature names\n",
      "X does not have valid feature names, but GradientBoostingRegressor was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 17/30 [00:08<00:06,  2.06it/s]X does not have valid feature names, but GradientBoostingRegressor was fitted with feature names\n",
      "X does not have valid feature names, but GradientBoostingRegressor was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 18/30 [00:08<00:05,  2.04it/s]X does not have valid feature names, but GradientBoostingRegressor was fitted with feature names\n",
      "X does not have valid feature names, but GradientBoostingRegressor was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 19/30 [00:09<00:05,  2.02it/s]X does not have valid feature names, but GradientBoostingRegressor was fitted with feature names\n",
      "X does not have valid feature names, but GradientBoostingRegressor was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 31 iterations, i.e. alpha=1.765e-05, with an active set of 23 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 38 iterations, i.e. alpha=1.038e-05, with an active set of 28 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 46 iterations, i.e. alpha=5.408e-06, with an active set of 30 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 48 iterations, i.e. alpha=5.102e-06, with an active set of 30 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 48 iterations, i.e. alpha=5.025e-06, with an active set of 30 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 51 iterations, alpha=4.065e-06, previous alpha=4.014e-06, with an active set of 32 regressors.\n",
      " 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 20/30 [00:09<00:04,  2.08it/s]X does not have valid feature names, but GradientBoostingRegressor was fitted with feature names\n",
      "X does not have valid feature names, but GradientBoostingRegressor was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 21/30 [00:10<00:04,  2.05it/s]X does not have valid feature names, but GradientBoostingRegressor was fitted with feature names\n",
      "X does not have valid feature names, but GradientBoostingRegressor was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 22/30 [00:10<00:03,  2.04it/s]X does not have valid feature names, but GradientBoostingRegressor was fitted with feature names\n",
      "X does not have valid feature names, but GradientBoostingRegressor was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 23/30 [00:11<00:03,  2.00it/s]X does not have valid feature names, but GradientBoostingRegressor was fitted with feature names\n",
      "X does not have valid feature names, but GradientBoostingRegressor was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 24/30 [00:11<00:03,  2.00it/s]X does not have valid feature names, but GradientBoostingRegressor was fitted with feature names\n",
      "X does not have valid feature names, but GradientBoostingRegressor was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 2 iterations, i.e. alpha=6.248e-04, with an active set of 2 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=5.431e-04, with an active set of 4 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=3.777e-04, with an active set of 7 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 10 iterations, i.e. alpha=3.124e-04, with an active set of 10 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 10 iterations, i.e. alpha=3.124e-04, with an active set of 10 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 10 iterations, i.e. alpha=3.124e-04, with an active set of 10 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 21 iterations, i.e. alpha=2.400e-04, with an active set of 13 regressors, and the smallest cholesky pivot element being 2.788e-08. Reduce max_iter or increase eps parameters.\n",
      "Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 23 iterations, alpha=2.338e-04, previous alpha=2.337e-04, with an active set of 14 regressors.\n",
      " 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 25/30 [00:12<00:02,  2.02it/s]X does not have valid feature names, but GradientBoostingRegressor was fitted with feature names\n",
      "X does not have valid feature names, but GradientBoostingRegressor was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 26/30 [00:12<00:01,  2.05it/s]X does not have valid feature names, but GradientBoostingRegressor was fitted with feature names\n",
      "X does not have valid feature names, but GradientBoostingRegressor was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 27/30 [00:13<00:01,  2.04it/s]X does not have valid feature names, but GradientBoostingRegressor was fitted with feature names\n",
      "X does not have valid feature names, but GradientBoostingRegressor was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 28/30 [00:13<00:00,  2.05it/s]X does not have valid feature names, but GradientBoostingRegressor was fitted with feature names\n",
      "X does not have valid feature names, but GradientBoostingRegressor was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 29/30 [00:14<00:00,  2.04it/s]X does not have valid feature names, but GradientBoostingRegressor was fitted with feature names\n",
      "X does not have valid feature names, but GradientBoostingRegressor was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 30/30 [00:14<00:00,  2.02it/s]\n"
     ]
    }
   ],
   "source": [
    "#Super Learner Feature Importance\n",
    "\n",
    "#Random forest \n",
    "random_forest_weighted_importance = models[0].feature_importances_ * meta_model.coef_[0]\n",
    "\n",
    "#Lasso \n",
    "lasso_weighted_importance = models[1].coef_ * meta_model.coef_[1]\n",
    "\n",
    "#GradientBoostingRegressor\n",
    "gradient_boosting_weighted_importance = models[2].feature_importances_ * meta_model.coef_[2]\n",
    "\n",
    "#NeuralNetwork\n",
    "explainer = shap.KernelExplainer(models[2].predict, X_train)\n",
    "shap_values = explainer.shap_values(X_test, nsamples=100)\n",
    "\n",
    "feature_names = X_train.columns\n",
    "\n",
    "rf_resultX = pd.DataFrame(shap_values, columns = feature_names)\n",
    "\n",
    "vals = np.abs(rf_resultX.values).mean(0)\n",
    "\n",
    "shap_importance = pd.DataFrame(list(zip(feature_names, vals)),\n",
    "                                  columns=['col_name','feature_importance_vals'])\n",
    "\n",
    "\n",
    "nn_weighted_importance = shap_importance.feature_importance_vals * meta_model.coef_[3]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "super_learner_feature_importance = np.mean([random_forest_weighted_importance, lasso_weighted_importance, gradient_boosting_weighted_importance, nn_weighted_importance], axis = 0)\n",
    "\n",
    "\n",
    "features_df = generate_results_table(super_learner_feature_importance, X_train.cols, 'superlearner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>col_name</th>\n",
       "      <th>feature_importance_vals</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>n_companies_RETAIL</td>\n",
       "      <td>0.002540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>n_companies_FORESTRY</td>\n",
       "      <td>0.001623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>near_mines</td>\n",
       "      <td>0.001546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>near_hidrovia</td>\n",
       "      <td>0.001180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>n_companies_AUTOMOBILES AND TRANSPORT</td>\n",
       "      <td>0.000889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>166</th>\n",
       "      <td>overall_winner_public_service_job</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>copper</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169</th>\n",
       "      <td>overall_winner_law_job</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170</th>\n",
       "      <td>overall_winner_technical_job</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151</th>\n",
       "      <td>expen_agr_defense</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>204 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  col_name  feature_importance_vals\n",
       "58                      n_companies_RETAIL                 0.002540\n",
       "63                    n_companies_FORESTRY                 0.001623\n",
       "5                               near_mines                 0.001546\n",
       "7                            near_hidrovia                 0.001180\n",
       "36   n_companies_AUTOMOBILES AND TRANSPORT                 0.000889\n",
       "..                                     ...                      ...\n",
       "166      overall_winner_public_service_job                 0.000000\n",
       "20                                  copper                 0.000000\n",
       "169                 overall_winner_law_job                 0.000000\n",
       "170           overall_winner_technical_job                 0.000000\n",
       "151                      expen_agr_defense                 0.000000\n",
       "\n",
       "[204 rows x 2 columns]"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shap_importance.feature_importance_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "X does not have valid feature names, but GradientBoostingRegressor was fitted with feature names\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]X does not have valid feature names, but GradientBoostingRegressor was fitted with feature names\n",
      "X does not have valid feature names, but GradientBoostingRegressor was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  3%|â–Ž         | 1/30 [00:00<00:16,  1.76it/s]X does not have valid feature names, but GradientBoostingRegressor was fitted with feature names\n",
      "X does not have valid feature names, but GradientBoostingRegressor was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  7%|â–‹         | 2/30 [00:01<00:15,  1.82it/s]X does not have valid feature names, but GradientBoostingRegressor was fitted with feature names\n",
      "X does not have valid feature names, but GradientBoostingRegressor was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 10%|â–ˆ         | 3/30 [00:01<00:14,  1.80it/s]X does not have valid feature names, but GradientBoostingRegressor was fitted with feature names\n",
      "X does not have valid feature names, but GradientBoostingRegressor was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 13%|â–ˆâ–Ž        | 4/30 [00:02<00:14,  1.81it/s]X does not have valid feature names, but GradientBoostingRegressor was fitted with feature names\n",
      "X does not have valid feature names, but GradientBoostingRegressor was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 17%|â–ˆâ–‹        | 5/30 [00:02<00:14,  1.75it/s]X does not have valid feature names, but GradientBoostingRegressor was fitted with feature names\n",
      "X does not have valid feature names, but GradientBoostingRegressor was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 20%|â–ˆâ–ˆ        | 6/30 [00:03<00:14,  1.69it/s]X does not have valid feature names, but GradientBoostingRegressor was fitted with feature names\n",
      "X does not have valid feature names, but GradientBoostingRegressor was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 23%|â–ˆâ–ˆâ–Ž       | 7/30 [00:03<00:12,  1.77it/s]X does not have valid feature names, but GradientBoostingRegressor was fitted with feature names\n",
      "X does not have valid feature names, but GradientBoostingRegressor was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 27%|â–ˆâ–ˆâ–‹       | 8/30 [00:04<00:13,  1.64it/s]X does not have valid feature names, but GradientBoostingRegressor was fitted with feature names\n",
      "X does not have valid feature names, but GradientBoostingRegressor was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 30%|â–ˆâ–ˆâ–ˆ       | 9/30 [00:05<00:12,  1.65it/s]X does not have valid feature names, but GradientBoostingRegressor was fitted with feature names\n",
      "X does not have valid feature names, but GradientBoostingRegressor was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 10/30 [00:05<00:11,  1.73it/s]X does not have valid feature names, but GradientBoostingRegressor was fitted with feature names\n",
      "X does not have valid feature names, but GradientBoostingRegressor was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 37%|â–ˆâ–ˆâ–ˆâ–‹      | 11/30 [00:06<00:11,  1.73it/s]X does not have valid feature names, but GradientBoostingRegressor was fitted with feature names\n",
      "X does not have valid feature names, but GradientBoostingRegressor was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 12/30 [00:06<00:10,  1.78it/s]X does not have valid feature names, but GradientBoostingRegressor was fitted with feature names\n",
      "X does not have valid feature names, but GradientBoostingRegressor was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 13/30 [00:07<00:10,  1.57it/s]X does not have valid feature names, but GradientBoostingRegressor was fitted with feature names\n",
      "X does not have valid feature names, but GradientBoostingRegressor was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 14/30 [00:08<00:10,  1.47it/s]X does not have valid feature names, but GradientBoostingRegressor was fitted with feature names\n",
      "X does not have valid feature names, but GradientBoostingRegressor was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 15/30 [00:09<00:10,  1.41it/s]X does not have valid feature names, but GradientBoostingRegressor was fitted with feature names\n",
      "X does not have valid feature names, but GradientBoostingRegressor was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 16/30 [00:10<00:10,  1.27it/s]X does not have valid feature names, but GradientBoostingRegressor was fitted with feature names\n",
      "X does not have valid feature names, but GradientBoostingRegressor was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 3 iterations, i.e. alpha=6.769e-05, with an active set of 3 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 40 iterations, i.e. alpha=1.329e-05, with an active set of 22 regressors, and the smallest cholesky pivot element being 4.470e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 40 iterations, i.e. alpha=1.327e-05, with an active set of 22 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 40 iterations, i.e. alpha=1.326e-05, with an active set of 22 regressors, and the smallest cholesky pivot element being 7.300e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 40 iterations, i.e. alpha=1.324e-05, with an active set of 22 regressors, and the smallest cholesky pivot element being 2.788e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 40 iterations, i.e. alpha=1.321e-05, with an active set of 22 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 40 iterations, i.e. alpha=1.320e-05, with an active set of 22 regressors, and the smallest cholesky pivot element being 5.373e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 40 iterations, i.e. alpha=1.319e-05, with an active set of 22 regressors, and the smallest cholesky pivot element being 6.829e-08. Reduce max_iter or increase eps parameters.\n",
      "Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 41 iterations, alpha=1.329e-05, previous alpha=1.319e-05, with an active set of 22 regressors.\n",
      " 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 17/30 [00:10<00:09,  1.32it/s]X does not have valid feature names, but GradientBoostingRegressor was fitted with feature names\n",
      "X does not have valid feature names, but GradientBoostingRegressor was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 18/30 [00:11<00:08,  1.37it/s]X does not have valid feature names, but GradientBoostingRegressor was fitted with feature names\n",
      "X does not have valid feature names, but GradientBoostingRegressor was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 19/30 [00:12<00:07,  1.45it/s]X does not have valid feature names, but GradientBoostingRegressor was fitted with feature names\n",
      "X does not have valid feature names, but GradientBoostingRegressor was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 20/30 [00:12<00:06,  1.54it/s]X does not have valid feature names, but GradientBoostingRegressor was fitted with feature names\n",
      "X does not have valid feature names, but GradientBoostingRegressor was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 21/30 [00:13<00:05,  1.59it/s]X does not have valid feature names, but GradientBoostingRegressor was fitted with feature names\n",
      "X does not have valid feature names, but GradientBoostingRegressor was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 22/30 [00:13<00:04,  1.68it/s]X does not have valid feature names, but GradientBoostingRegressor was fitted with feature names\n",
      "X does not have valid feature names, but GradientBoostingRegressor was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 23/30 [00:14<00:03,  1.76it/s]X does not have valid feature names, but GradientBoostingRegressor was fitted with feature names\n",
      "X does not have valid feature names, but GradientBoostingRegressor was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 24/30 [00:14<00:03,  1.81it/s]X does not have valid feature names, but GradientBoostingRegressor was fitted with feature names\n",
      "X does not have valid feature names, but GradientBoostingRegressor was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 25/30 [00:15<00:02,  1.89it/s]X does not have valid feature names, but GradientBoostingRegressor was fitted with feature names\n",
      "X does not have valid feature names, but GradientBoostingRegressor was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 26/30 [00:15<00:02,  1.92it/s]X does not have valid feature names, but GradientBoostingRegressor was fitted with feature names\n",
      "X does not have valid feature names, but GradientBoostingRegressor was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "Regressors in active set degenerate. Dropping a regressor, after 60 iterations, i.e. alpha=1.630e-06, with an active set of 32 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 61 iterations, i.e. alpha=1.626e-06, with an active set of 33 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 61 iterations, i.e. alpha=1.612e-06, with an active set of 33 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 82 iterations, alpha=8.138e-07, previous alpha=8.138e-07, with an active set of 39 regressors.\n",
      " 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 27/30 [00:16<00:01,  1.93it/s]X does not have valid feature names, but GradientBoostingRegressor was fitted with feature names\n",
      "X does not have valid feature names, but GradientBoostingRegressor was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 28/30 [00:16<00:01,  1.99it/s]X does not have valid feature names, but GradientBoostingRegressor was fitted with feature names\n",
      "X does not have valid feature names, but GradientBoostingRegressor was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      " 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 29/30 [00:17<00:00,  1.97it/s]X does not have valid feature names, but GradientBoostingRegressor was fitted with feature names\n",
      "X does not have valid feature names, but GradientBoostingRegressor was fitted with feature names\n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 30/30 [00:17<00:00,  1.68it/s]\n"
     ]
    }
   ],
   "source": [
    "explainer = shap.KernelExplainer(models[2].predict, X_train)\n",
    "shap_values = explainer.shap_values(X_test, nsamples=100)\n",
    "\n",
    "feature_names = X_train.columns\n",
    "\n",
    "rf_resultX = pd.DataFrame(shap_values, columns = feature_names)\n",
    "\n",
    "vals = np.abs(rf_resultX.values).mean(0)\n",
    "\n",
    "shap_importance = pd.DataFrame(list(zip(feature_names, vals)),\n",
    "                                  columns=['col_name','feature_importance_vals'])\n",
    "shap_importance.sort_values(by=['feature_importance_vals'],\n",
    "                               ascending=False, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
       "        0.00000000e+00, 0.00000000e+00, 4.05675901e-03],\n",
       "       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "       ...,\n",
       "       [0.00000000e+00, 0.00000000e+00, 2.27030615e-05, ...,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "       [0.00000000e+00, 2.62342827e-03, 1.15576693e-03, ...,\n",
       "        0.00000000e+00, 0.00000000e+00, 3.23303850e-03],\n",
       "       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00]])"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(shap_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.00000000e+00, 8.27232956e-03, 0.00000000e+00, 2.45284578e-06,\n",
       "       1.00258157e-02, 2.70454800e-02, 1.87085659e-02, 2.36352376e-02,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       6.16839315e-06, 5.71077321e-06, 0.00000000e+00, 6.53311822e-05,\n",
       "       4.54313060e-04, 0.00000000e+00, 4.59988680e-05, 7.60774651e-03,\n",
       "       2.10151117e-03, 3.67111152e-04, 0.00000000e+00, 0.00000000e+00,\n",
       "       1.59830459e-03, 7.51131916e-04, 5.27079327e-05, 9.46272764e-06,\n",
       "       1.08405126e-02, 3.82764842e-04, 5.35716997e-05, 0.00000000e+00,\n",
       "       2.38443171e-02, 3.32685614e-06, 0.00000000e+00, 0.00000000e+00,\n",
       "       2.03749826e-03, 3.06422011e-03, 3.56033268e-03, 1.47949525e-04,\n",
       "       3.12960734e-04, 6.44586730e-03, 6.05142310e-05, 1.59564483e-05,\n",
       "       0.00000000e+00, 6.35006603e-03, 1.57532747e-04, 2.00061838e-01,\n",
       "       4.20919543e-03, 5.02515472e-03, 2.44965303e-02, 1.25099521e-03,\n",
       "       0.00000000e+00, 2.36857179e-02, 1.59335430e-01, 4.40363893e-02,\n",
       "       0.00000000e+00, 7.64830171e-03, 0.00000000e+00, 4.17897528e-02,\n",
       "       1.59916103e-03, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       7.32348126e-06, 0.00000000e+00, 1.63810513e-04, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 5.32058927e-03, 2.39675018e-06,\n",
       "       0.00000000e+00, 1.62106243e-05, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 1.56018716e-03, 4.89131641e-03, 6.86179735e-04,\n",
       "       8.62164667e-07, 7.42157714e-06, 9.16242896e-03, 7.12378293e-04,\n",
       "       4.83542690e-03, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       3.63256178e-03, 6.99532398e-05, 7.48924880e-06, 4.09936341e-03,\n",
       "       2.81443620e-03, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 8.85473702e-05, 4.38372680e-04, 1.39804822e-03,\n",
       "       5.28896136e-03, 3.05879985e-05, 8.66172104e-05, 0.00000000e+00,\n",
       "       6.31374749e-03, 1.49845995e-06, 6.31564711e-03, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 1.61264012e-02,\n",
       "       1.89913130e-06, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 1.94080349e-06, 1.30865587e-04, 1.79850776e-06,\n",
       "       1.55291331e-02, 6.52578779e-05, 3.01312799e-05, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 4.43520199e-04,\n",
       "       3.90964829e-03, 3.45536951e-06, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 4.26873127e-03, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       1.08806186e-04, 8.15210602e-04, 1.35446869e-04, 5.90447399e-05,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 1.41836010e-03,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 2.08308238e-05, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 2.51829599e-04, 0.00000000e+00,\n",
       "       2.61180959e-05, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 1.58287070e-01, 1.74764732e-02, 2.56305241e-04,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 3.40996142e-02,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 1.74085054e-02])"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models[2].feature_importances_ * meta_model.coef_[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "arrays used as indices must be of integer (or boolean) type",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-121-04b7728aaa5a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mX_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_importances_\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   4305\u001b[0m             \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4307\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4308\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_scalar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4309\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: arrays used as indices must be of integer (or boolean) type"
     ]
    }
   ],
   "source": [
    "X_train.columns[models[0].feature_importances_]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.00696935, -0.00554754, -0.0074047 ,  0.00154693])"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meta_model.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def super_learner_predictions(X, models, meta_model):\n",
    "\tmeta_X = []\n",
    "\tfor model in models:\n",
    "\t\tyhat = model.predict(X) \n",
    "\t\tmeta_X.append(yhat)\n",
    "\t# predict\n",
    "\treturn meta_model.predict(pd.DataFrame(meta_X).T)\n",
    "    \n",
    "def fit_base_models(X, y, models):\n",
    "\tfor model in models:\n",
    "\t\tmodel.fit(X, y)\n",
    " \n",
    "\n",
    "def fit_meta_model(X, y):\n",
    "\tmodel = Ridge()\n",
    "\tmodel.fit(X, y)\n",
    "\treturn model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta  (70, 4) (70,)\n",
      "RandomForestRegressor: 0.001\n",
      "Lasso: 0.003\n",
      "GradientBoostingRegressor: 0.002\n",
      "MLPRegressor: 0.008\n",
      "MSE: 0.0023895988264233748\n"
     ]
    }
   ],
   "source": [
    "# get models\n",
    "models = get_models()\n",
    "# get out of fold predictions\n",
    "meta_X, meta_y = get_out_of_fold_predictions(X_train, Y_train, models)\n",
    "print('Meta ', meta_X.shape, meta_y.shape)\n",
    "\n",
    "fit_base_models(X_train, Y_train, models)\n",
    "meta_model = fit_meta_model(meta_X, meta_y)\n",
    "\n",
    "evaluate_models(X_test, Y_test, models)\n",
    "\n",
    "yhat = super_learner_predictions(X_test, models, meta_model)\n",
    "\n",
    "# Evaluate the performance of the model\n",
    "mse = mean_squared_error(Y_test, yhat)\n",
    "print(\"MSE:\", mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# List of base models\n",
    "#base_models = [LinearRegression(), DecisionTreeRegressor(), RandomForestRegressor()]\n",
    "\n",
    "# Define number of folds for cross-validation\n",
    "k = 5\n",
    "\n",
    "# Define empty array for storing base model predictions\n",
    "base_model_predictions = np.zeros((X_train.shape[0], len(base_models)))\n",
    "\n",
    "\n",
    "# Define K-Fold cross-validation iterator\n",
    "kf = KFold(n_splits=k, shuffle=True, random_state=42)\n",
    "\n",
    "# Loop over each fold in the data\n",
    "for train_index, test_index in kf.split(X_train):\n",
    "    \n",
    "    # Split data into training and testing sets for this fold\n",
    "    X_train_fold, X_test_fold = X_train.iloc[train_index], X_train.iloc[test_index]\n",
    "    Y_train_fold, Y_test_fold = Y_train.iloc[train_index], Y_train.iloc[test_index]\n",
    "    \n",
    "    # Loop over each base model and train it on the training data for this fold\n",
    "    for i, model in enumerate(base_models):\n",
    "        model.fit(X_train_fold, Y_train_fold)\n",
    "        y_pred = model.predict(X_test_fold)\n",
    "        base_model_predictions[test_index, i] = y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-29 {color: black;background-color: white;}#sk-container-id-29 pre{padding: 0;}#sk-container-id-29 div.sk-toggleable {background-color: white;}#sk-container-id-29 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-29 label.sk-toggleable__label-arrow:before {content: \"â–¸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-29 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-29 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-29 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-29 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-29 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-29 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"â–¾\";}#sk-container-id-29 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-29 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-29 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-29 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-29 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-29 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-29 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-29 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-29 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-29 div.sk-item {position: relative;z-index: 1;}#sk-container-id-29 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-29 div.sk-item::before, #sk-container-id-29 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-29 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-29 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-29 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-29 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-29 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-29 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-29 div.sk-label-container {text-align: center;}#sk-container-id-29 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-29 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-29\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Ridge()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-76\" type=\"checkbox\" checked><label for=\"sk-estimator-id-76\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Ridge</label><div class=\"sk-toggleable__content\"><pre>Ridge()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "Ridge()"
      ]
     },
     "execution_count": 313,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the meta estimator (logistic regression) on the base model predictions\n",
    "meta_estimator = Ridge()\n",
    "meta_estimator.fit(base_model_predictions, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(70, 4)"
      ]
     },
     "execution_count": 317,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_model_predictions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "X has feature names, but Ridge was fitted without feature names\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "X has 204 features, but Ridge is expecting 4 features as input.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-315-60c259416129>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Make predictions on the test set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmeta_estimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Evaluate the performance of the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mmse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmean_squared_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_base.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    384\u001b[0m             \u001b[0mReturns\u001b[0m \u001b[0mpredicted\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    385\u001b[0m         \"\"\"\n\u001b[0;32m--> 386\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_decision_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    387\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    388\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_set_intercept\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_offset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_offset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_scale\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_base.py\u001b[0m in \u001b[0;36m_decision_function\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    367\u001b[0m         \u001b[0mcheck_is_fitted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    368\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 369\u001b[0;31m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"csr\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"csc\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"coo\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    370\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0msafe_sparse_dot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcoef_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdense_output\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintercept_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    371\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/sklearn/base.py\u001b[0m in \u001b[0;36m_validate_data\u001b[0;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[1;32m    598\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    599\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mno_val_X\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mcheck_params\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"ensure_2d\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 600\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_n_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    601\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    602\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/sklearn/base.py\u001b[0m in \u001b[0;36m_check_n_features\u001b[0;34m(self, X, reset)\u001b[0m\n\u001b[1;32m    398\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    399\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mn_features\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_features_in_\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 400\u001b[0;31m             raise ValueError(\n\u001b[0m\u001b[1;32m    401\u001b[0m                 \u001b[0;34mf\"X has {n_features} features, but {self.__class__.__name__} \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    402\u001b[0m                 \u001b[0;34mf\"is expecting {self.n_features_in_} features as input.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: X has 204 features, but Ridge is expecting 4 features as input."
     ]
    }
   ],
   "source": [
    "# Make predictions on the test set\n",
    "y_pred = meta_estimator.predict(X_test)\n",
    "\n",
    "# Evaluate the performance of the model\n",
    "mse = mean_squared_error(Y_test, y_pred)\n",
    "print(\"MSE:\", mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Ridge' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-311-6ab10fbf622f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Train the meta estimator (logistic regression) on the base model predictions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmeta_estimator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRidge\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mmeta_estimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_model_predictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Ridge' is not defined"
     ]
    }
   ],
   "source": [
    "# Train the meta estimator (logistic regression) on the base model predictions\n",
    "meta_estimator = Ridge()\n",
    "meta_estimator.fit(base_model_predictions, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Train the meta estimator (logistic regression) on the base model predictions\n",
    "meta_estimator = LogisticRegression()\n",
    "meta_estimator.fit(base_model_predictions, Y_train)\n",
    "\n",
    "# Make predictions on test data using the Super Learner ensemble\n",
    "super_learner_predictions = np.zeros((X_test.shape[0], len(base_models)))\n",
    "for i, model in enumerate(base_models):\n",
    "    y_pred = model.predict(X_test)\n",
    "    super_learner_predictions[:, i] = y_pred\n",
    "\n",
    "y_pred_final = meta_estimator.predict(super_learner_predictions)\n",
    "\n",
    "# Evaluate performance of Super Learner ensemble\n",
    "mse = mean_squared_error(Y_test, y_pred_final)\n",
    "print(\"MSE of Super Learner ensemble:\", mse)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fitting 2 layers\n",
      "Processing layer-1             done | 00:00:04\n",
      "Processing layer-2             done | 00:00:00\n",
      "Fit complete                        | 00:00:04\n",
      "\n",
      "Predicting 2 layers\n",
      "Processing layer-1             done | 00:00:00\n",
      "Processing layer-2             done | 00:00:00\n",
      "Predict complete                    | 00:00:00\n",
      "MSE: 0.0024206304822519597\n"
     ]
    }
   ],
   "source": [
    "from mlens.ensemble import SuperLearner\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import KFold\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "base_models = []\n",
    "\n",
    "#Random forest regressor\n",
    "base_models.append(base_learners[0][1][1])\n",
    "#Lasso\n",
    "base_models.append(base_learners[1][1][1])\n",
    "#Gradient Boosting\n",
    "base_models.append(base_learners[2][1])\n",
    "#NeuralNetwork\n",
    "base_models.append(base_learners[3][1][1])\n",
    "\n",
    "# Define the meta estimator for the ensemble\n",
    "meta_estimator = Ridge()\n",
    "\n",
    "# Define the number of folds for cross-validation\n",
    "n_folds = 5\n",
    "\n",
    "# Define the cross-validation strategy\n",
    "cv = KFold(n_splits=n_folds, shuffle=True, random_state=42)\n",
    "\n",
    "# Create a SuperLearner ensemble\n",
    "sl = SuperLearner(\n",
    "    folds=5,\n",
    "    random_state=42,\n",
    "    verbose=2,\n",
    "    scorer=mean_squared_error,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Add the base models to the ensemble\n",
    "sl.add(base_models)\n",
    "\n",
    "# Add the meta estimator to the ensemble\n",
    "sl.add_meta(meta_estimator)\n",
    "\n",
    "# Fit the SuperLearner on the training data\n",
    "sl.fit(X_train, Y_train)\n",
    "\n",
    "# Make predictions on new data\n",
    "y_pred = sl.predict(X_test)\n",
    "\n",
    "# Evaluate the performance of the model\n",
    "mse = mean_squared_error(Y_test, y_pred)\n",
    "print(\"MSE:\", mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_models = []\n",
    "\n",
    "#Random forest regressor\n",
    "base_models.append(base_learners[0][1][1])\n",
    "\n",
    "#Lasso\n",
    "base_models.append(base_learners[1][1][1])\n",
    "\n",
    "#Gradient Boosting\n",
    "base_models.append(base_learners[2][1])\n",
    "\n",
    "#NeuralNetwork\n",
    "base_models.append(base_learners[3][1][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fitting 2 layers\n",
      "Processing layer-1             "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.588e-03, tolerance: 3.565e-06\n",
      "Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.311e-02, tolerance: 1.194e-05\n",
      "[lasso.0.2] Could not score lasso. Details:\n",
      "ValueError('continuous is not supported')\n",
      "[lasso.0.1] Could not score lasso. Details:\n",
      "ValueError('continuous is not supported')\n",
      "[gradientboostingregressor.0.1] Could not score gradientboostingregressor. Details:\n",
      "ValueError('continuous is not supported')\n",
      "[gradientboostingregressor.0.2] Could not score gradientboostingregressor. Details:\n",
      "ValueError('continuous is not supported')\n",
      "[mlpregressor.0.1] Could not score mlpregressor. Details:\n",
      "ValueError('continuous is not supported')\n",
      "[mlpregressor.0.2] Could not score mlpregressor. Details:\n",
      "ValueError('continuous is not supported')\n",
      "[randomforestregressor.0.1] Could not score randomforestregressor. Details:\n",
      "ValueError('continuous is not supported')\n",
      "[randomforestregressor.0.2] Could not score randomforestregressor. Details:\n",
      "ValueError('continuous is not supported')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done | 00:00:02\n",
      "Processing layer-2             "
     ]
    },
    {
     "ename": "JoblibValueError",
     "evalue": "JoblibValueError\n___________________________________________________________________________\nMultiprocessing exception:\n...........................................................................\n/Users/annieulichney/opt/anaconda3/lib/python3.8/runpy.py in _run_module_as_main(mod_name='ipykernel_launcher', alter_argv=1)\n    189         msg = \"%s: %s\" % (sys.executable, exc)\n    190         sys.exit(msg)\n    191     main_globals = sys.modules[\"__main__\"].__dict__\n    192     if alter_argv:\n    193         sys.argv[0] = mod_spec.origin\n--> 194     return _run_code(code, main_globals, None,\n        code = <code object <module> at 0x7feb99e44d40, file \"/...3.8/site-packages/ipykernel_launcher.py\", line 1>\n        main_globals = {'__annotations__': {}, '__builtins__': <module 'builtins' (built-in)>, '__cached__': '/Users/annieulichney/opt/anaconda3/lib/python3.8...ges/__pycache__/ipykernel_launcher.cpython-38.pyc', '__doc__': 'Entry point for launching an IPython kernel.\\n\\nTh...orts until\\nafter removing the cwd from sys.path.\\n', '__file__': '/Users/annieulichney/opt/anaconda3/lib/python3.8/site-packages/ipykernel_launcher.py', '__loader__': <_frozen_importlib_external.SourceFileLoader object>, '__name__': '__main__', '__package__': '', '__spec__': ModuleSpec(name='ipykernel_launcher', loader=<_f...b/python3.8/site-packages/ipykernel_launcher.py'), 'app': <module 'ipykernel.kernelapp' from '/Users/annie.../python3.8/site-packages/ipykernel/kernelapp.py'>, ...}\n        mod_spec = ModuleSpec(name='ipykernel_launcher', loader=<_f...b/python3.8/site-packages/ipykernel_launcher.py')\n    195                      \"__main__\", mod_spec)\n    196 \n    197 def run_module(mod_name, init_globals=None,\n    198                run_name=None, alter_sys=False):\n\n...........................................................................\n/Users/annieulichney/opt/anaconda3/lib/python3.8/runpy.py in _run_code(code=<code object <module> at 0x7feb99e44d40, file \"/...3.8/site-packages/ipykernel_launcher.py\", line 1>, run_globals={'__annotations__': {}, '__builtins__': <module 'builtins' (built-in)>, '__cached__': '/Users/annieulichney/opt/anaconda3/lib/python3.8...ges/__pycache__/ipykernel_launcher.cpython-38.pyc', '__doc__': 'Entry point for launching an IPython kernel.\\n\\nTh...orts until\\nafter removing the cwd from sys.path.\\n', '__file__': '/Users/annieulichney/opt/anaconda3/lib/python3.8/site-packages/ipykernel_launcher.py', '__loader__': <_frozen_importlib_external.SourceFileLoader object>, '__name__': '__main__', '__package__': '', '__spec__': ModuleSpec(name='ipykernel_launcher', loader=<_f...b/python3.8/site-packages/ipykernel_launcher.py'), 'app': <module 'ipykernel.kernelapp' from '/Users/annie.../python3.8/site-packages/ipykernel/kernelapp.py'>, ...}, init_globals=None, mod_name='__main__', mod_spec=ModuleSpec(name='ipykernel_launcher', loader=<_f...b/python3.8/site-packages/ipykernel_launcher.py'), pkg_name='', script_name=None)\n     82                        __cached__ = cached,\n     83                        __doc__ = None,\n     84                        __loader__ = loader,\n     85                        __package__ = pkg_name,\n     86                        __spec__ = mod_spec)\n---> 87     exec(code, run_globals)\n        code = <code object <module> at 0x7feb99e44d40, file \"/...3.8/site-packages/ipykernel_launcher.py\", line 1>\n        run_globals = {'__annotations__': {}, '__builtins__': <module 'builtins' (built-in)>, '__cached__': '/Users/annieulichney/opt/anaconda3/lib/python3.8...ges/__pycache__/ipykernel_launcher.cpython-38.pyc', '__doc__': 'Entry point for launching an IPython kernel.\\n\\nTh...orts until\\nafter removing the cwd from sys.path.\\n', '__file__': '/Users/annieulichney/opt/anaconda3/lib/python3.8/site-packages/ipykernel_launcher.py', '__loader__': <_frozen_importlib_external.SourceFileLoader object>, '__name__': '__main__', '__package__': '', '__spec__': ModuleSpec(name='ipykernel_launcher', loader=<_f...b/python3.8/site-packages/ipykernel_launcher.py'), 'app': <module 'ipykernel.kernelapp' from '/Users/annie.../python3.8/site-packages/ipykernel/kernelapp.py'>, ...}\n     88     return run_globals\n     89 \n     90 def _run_module_code(code, init_globals=None,\n     91                     mod_name=None, mod_spec=None,\n\n...........................................................................\n/Users/annieulichney/opt/anaconda3/lib/python3.8/site-packages/ipykernel_launcher.py in <module>()\n     11     # This is added back by InteractiveShellApp.init_path()\n     12     if sys.path[0] == '':\n     13         del sys.path[0]\n     14 \n     15     from ipykernel import kernelapp as app\n---> 16     app.launch_new_instance()\n\n...........................................................................\n/Users/annieulichney/opt/anaconda3/lib/python3.8/site-packages/traitlets/config/application.py in launch_instance(cls=<class 'ipykernel.kernelapp.IPKernelApp'>, argv=None, **kwargs={})\n    840 \n    841         If a global instance already exists, this reinitializes and starts it\n    842         \"\"\"\n    843         app = cls.instance(**kwargs)\n    844         app.initialize(argv)\n--> 845         app.start()\n        app.start = <bound method IPKernelApp.start of <ipykernel.kernelapp.IPKernelApp object>>\n    846 \n    847 #-----------------------------------------------------------------------------\n    848 # utility functions, for convenience\n    849 #-----------------------------------------------------------------------------\n\n...........................................................................\n/Users/annieulichney/opt/anaconda3/lib/python3.8/site-packages/ipykernel/kernelapp.py in start(self=<ipykernel.kernelapp.IPKernelApp object>)\n    607                 tr.run()\n    608             except KeyboardInterrupt:\n    609                 pass\n    610         else:\n    611             try:\n--> 612                 self.io_loop.start()\n        self.io_loop.start = <bound method BaseAsyncIOLoop.start of <tornado.platform.asyncio.AsyncIOMainLoop object>>\n    613             except KeyboardInterrupt:\n    614                 pass\n    615 \n    616 \n\n...........................................................................\n/Users/annieulichney/opt/anaconda3/lib/python3.8/site-packages/tornado/platform/asyncio.py in start(self=<tornado.platform.asyncio.AsyncIOMainLoop object>)\n    194         except (RuntimeError, AssertionError):\n    195             old_loop = None  # type: ignore\n    196         try:\n    197             self._setup_logging()\n    198             asyncio.set_event_loop(self.asyncio_loop)\n--> 199             self.asyncio_loop.run_forever()\n        self.asyncio_loop.run_forever = <bound method BaseEventLoop.run_forever of <_Uni...EventLoop running=True closed=False debug=False>>\n    200         finally:\n    201             asyncio.set_event_loop(old_loop)\n    202 \n    203     def stop(self) -> None:\n\n...........................................................................\n/Users/annieulichney/opt/anaconda3/lib/python3.8/asyncio/base_events.py in run_forever(self=<_UnixSelectorEventLoop running=True closed=False debug=False>)\n    565         sys.set_asyncgen_hooks(firstiter=self._asyncgen_firstiter_hook,\n    566                                finalizer=self._asyncgen_finalizer_hook)\n    567         try:\n    568             events._set_running_loop(self)\n    569             while True:\n--> 570                 self._run_once()\n        self._run_once = <bound method BaseEventLoop._run_once of <_UnixS...EventLoop running=True closed=False debug=False>>\n    571                 if self._stopping:\n    572                     break\n    573         finally:\n    574             self._stopping = False\n\n...........................................................................\n/Users/annieulichney/opt/anaconda3/lib/python3.8/asyncio/base_events.py in _run_once(self=<_UnixSelectorEventLoop running=True closed=False debug=False>)\n   1854                         logger.warning('Executing %s took %.3f seconds',\n   1855                                        _format_handle(handle), dt)\n   1856                 finally:\n   1857                     self._current_handle = None\n   1858             else:\n-> 1859                 handle._run()\n        handle._run = <bound method Handle._run of <Handle IOLoop.add_...b/python3.8/site-packages/tornado/ioloop.py:688>>\n   1860         handle = None  # Needed to break cycles when an exception occurs.\n   1861 \n   1862     def _set_coroutine_origin_tracking(self, enabled):\n   1863         if bool(enabled) == bool(self._coroutine_origin_tracking_enabled):\n\n...........................................................................\n/Users/annieulichney/opt/anaconda3/lib/python3.8/asyncio/events.py in _run(self=<Handle IOLoop.add_future.<locals>.<lambda>(<Fut...ib/python3.8/site-packages/tornado/ioloop.py:688>)\n     76     def cancelled(self):\n     77         return self._cancelled\n     78 \n     79     def _run(self):\n     80         try:\n---> 81             self._context.run(self._callback, *self._args)\n        self._context.run = <built-in method run of Context object>\n        self._callback = <function IOLoop.add_future.<locals>.<lambda>>\n        self._args = (<Future finished result=(10, 632, <bound method....9453720>, <zmq.sugar.fr...x7fe65d5bed50>, ...]))>,)\n     82         except (SystemExit, KeyboardInterrupt):\n     83             raise\n     84         except BaseException as exc:\n     85             cb = format_helpers._format_callback_source(\n\n...........................................................................\n/Users/annieulichney/opt/anaconda3/lib/python3.8/site-packages/tornado/ioloop.py in <lambda>(f=<Future finished result=(10, 632, <bound method....9453720>, <zmq.sugar.fr...x7fe65d5bed50>, ...]))>)\n    683             #\n    684             # Wrap the callback in self._run_callback so we control\n    685             # the error logging (i.e. it goes to tornado.log.app_log\n    686             # instead of asyncio's log).\n    687             future.add_done_callback(\n--> 688                 lambda f: self._run_callback(functools.partial(callback, future))\n        f = <Future finished result=(10, 632, <bound method....9453720>, <zmq.sugar.fr...x7fe65d5bed50>, ...]))>\n    689             )\n    690         else:\n    691             assert is_future(future)\n    692             # For concurrent futures, we use self.add_callback, so\n\n...........................................................................\n/Users/annieulichney/opt/anaconda3/lib/python3.8/site-packages/tornado/ioloop.py in _run_callback(self=<tornado.platform.asyncio.AsyncIOMainLoop object>, callback=functools.partial(<function Runner.handle_yield....453720>, <zmq.sugar.fr...x7fe65d5bed50>, ...]))>))\n    736         .. versionchanged:: 6.0\n    737 \n    738            CancelledErrors are no longer logged.\n    739         \"\"\"\n    740         try:\n--> 741             ret = callback()\n        ret = undefined\n        callback = functools.partial(<function Runner.handle_yield....453720>, <zmq.sugar.fr...x7fe65d5bed50>, ...]))>)\n    742             if ret is not None:\n    743                 from tornado import gen\n    744 \n    745                 # Functions that return Futures typically swallow all\n\n...........................................................................\n/Users/annieulichney/opt/anaconda3/lib/python3.8/site-packages/tornado/gen.py in inner(f=None)\n    809         elif not self.future.done():\n    810 \n    811             def inner(f: Any) -> None:\n    812                 # Break a reference cycle to speed GC.\n    813                 f = None  # noqa: F841\n--> 814                 self.ctx_run(self.run)\n    815 \n    816             self.io_loop.add_future(self.future, inner)\n    817             return False\n    818         return True\n\n...........................................................................\n/Users/annieulichney/opt/anaconda3/lib/python3.8/site-packages/tornado/gen.py in run(self=<tornado.gen.Runner object>)\n    770                         finally:\n    771                             # Break up a reference to itself\n    772                             # for faster GC on CPython.\n    773                             exc_info = None\n    774                     else:\n--> 775                         yielded = self.gen.send(value)\n        yielded = undefined\n        self.gen.send = <built-in method send of generator object>\n        value = (10, 632, <bound method Kernel.dispatch_shell of <ipykernel.ipkernel.IPythonKernel object>>, (<zmq.eventloop.zmqstream.ZMQStream object>, [<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>]))\n    776 \n    777                 except (StopIteration, Return) as e:\n    778                     self.finished = True\n    779                     self.future = _null_future\n\n...........................................................................\n/Users/annieulichney/opt/anaconda3/lib/python3.8/site-packages/ipykernel/kernelbase.py in process_one(self=<ipykernel.ipkernel.IPythonKernel object>, wait=True)\n    360         else:\n    361             try:\n    362                 priority, t, dispatch, args = self.msg_queue.get_nowait()\n    363             except QueueEmpty:\n    364                 return None\n--> 365         yield gen.maybe_future(dispatch(*args))\n        dispatch = <bound method Kernel.dispatch_shell of <ipykernel.ipkernel.IPythonKernel object>>\n        args = (<zmq.eventloop.zmqstream.ZMQStream object>, [<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>])\n    366 \n    367     @gen.coroutine\n    368     def dispatch_queue(self):\n    369         \"\"\"Coroutine to preserve order of message handling\n\n...........................................................................\n/Users/annieulichney/opt/anaconda3/lib/python3.8/site-packages/tornado/gen.py in wrapper(*args=(<ipykernel.ipkernel.IPythonKernel object>, <zmq.eventloop.zmqstream.ZMQStream object>, [<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>]), **kwargs={})\n    229                 # avoid the cost of creating a Runner when the coroutine\n    230                 # never actually yields, which in turn allows us to\n    231                 # use \"optional\" coroutines in critical path code without\n    232                 # performance penalty for the synchronous case.\n    233                 try:\n--> 234                     yielded = ctx_run(next, result)\n        yielded = undefined\n        ctx_run = <built-in method run of Context object>\n        result = <generator object Kernel.dispatch_shell>\n    235                 except (StopIteration, Return) as e:\n    236                     future_set_result_unless_cancelled(\n    237                         future, _value_from_stopiteration(e)\n    238                     )\n\n...........................................................................\n/Users/annieulichney/opt/anaconda3/lib/python3.8/site-packages/ipykernel/kernelbase.py in dispatch_shell(self=<ipykernel.ipkernel.IPythonKernel object>, stream=<zmq.eventloop.zmqstream.ZMQStream object>, msg={'buffers': [], 'content': {'allow_stdin': True, 'code': 'from mlens.ensemble import SuperLearner\\nfrom skl...rain)\\n\\n# Predict\\npreds = ensemble.predict(X_test)', 'silent': False, 'stop_on_error': False, 'store_history': True, 'user_expressions': {}}, 'header': {'date': datetime.datetime(2023, 5, 9, 0, 23, 7, 439000, tzinfo=tzutc()), 'msg_id': '192988bd-1986-4d0d-a104-e363d1fccffd', 'msg_type': 'execute_request', 'session': 'f8677049-449b-4371-ac7d-878e11fe03ff', 'username': '751b3616-df4d-43b3-b034-72e0f89e0c4a', 'version': '5.2'}, 'metadata': {'cellId': 'vscode-notebook-cell:/Users/annieulichney/Docume...eforestation/ResultsWithPolitical.ipynb#ch0000127'}, 'msg_id': '192988bd-1986-4d0d-a104-e363d1fccffd', 'msg_type': 'execute_request', 'parent_header': {}})\n    263             try:\n    264                 self.pre_handler_hook()\n    265             except Exception:\n    266                 self.log.debug(\"Unable to signal in pre_handler_hook:\", exc_info=True)\n    267             try:\n--> 268                 yield gen.maybe_future(handler(stream, idents, msg))\n        handler = <bound method Kernel.execute_request of <ipykernel.ipkernel.IPythonKernel object>>\n        stream = <zmq.eventloop.zmqstream.ZMQStream object>\n        idents = [b'cf5dc902-7071-485f-a33a-9ea2e7aada4b']\n        msg = {'buffers': [], 'content': {'allow_stdin': True, 'code': 'from mlens.ensemble import SuperLearner\\nfrom skl...rain)\\n\\n# Predict\\npreds = ensemble.predict(X_test)', 'silent': False, 'stop_on_error': False, 'store_history': True, 'user_expressions': {}}, 'header': {'date': datetime.datetime(2023, 5, 9, 0, 23, 7, 439000, tzinfo=tzutc()), 'msg_id': '192988bd-1986-4d0d-a104-e363d1fccffd', 'msg_type': 'execute_request', 'session': 'f8677049-449b-4371-ac7d-878e11fe03ff', 'username': '751b3616-df4d-43b3-b034-72e0f89e0c4a', 'version': '5.2'}, 'metadata': {'cellId': 'vscode-notebook-cell:/Users/annieulichney/Docume...eforestation/ResultsWithPolitical.ipynb#ch0000127'}, 'msg_id': '192988bd-1986-4d0d-a104-e363d1fccffd', 'msg_type': 'execute_request', 'parent_header': {}}\n    269             except Exception:\n    270                 self.log.error(\"Exception in message handler:\", exc_info=True)\n    271             finally:\n    272                 try:\n\n...........................................................................\n/Users/annieulichney/opt/anaconda3/lib/python3.8/site-packages/tornado/gen.py in wrapper(*args=(<ipykernel.ipkernel.IPythonKernel object>, <zmq.eventloop.zmqstream.ZMQStream object>, [b'cf5dc902-7071-485f-a33a-9ea2e7aada4b'], {'buffers': [], 'content': {'allow_stdin': True, 'code': 'from mlens.ensemble import SuperLearner\\nfrom skl...rain)\\n\\n# Predict\\npreds = ensemble.predict(X_test)', 'silent': False, 'stop_on_error': False, 'store_history': True, 'user_expressions': {}}, 'header': {'date': datetime.datetime(2023, 5, 9, 0, 23, 7, 439000, tzinfo=tzutc()), 'msg_id': '192988bd-1986-4d0d-a104-e363d1fccffd', 'msg_type': 'execute_request', 'session': 'f8677049-449b-4371-ac7d-878e11fe03ff', 'username': '751b3616-df4d-43b3-b034-72e0f89e0c4a', 'version': '5.2'}, 'metadata': {'cellId': 'vscode-notebook-cell:/Users/annieulichney/Docume...eforestation/ResultsWithPolitical.ipynb#ch0000127'}, 'msg_id': '192988bd-1986-4d0d-a104-e363d1fccffd', 'msg_type': 'execute_request', 'parent_header': {}}), **kwargs={})\n    229                 # avoid the cost of creating a Runner when the coroutine\n    230                 # never actually yields, which in turn allows us to\n    231                 # use \"optional\" coroutines in critical path code without\n    232                 # performance penalty for the synchronous case.\n    233                 try:\n--> 234                     yielded = ctx_run(next, result)\n        yielded = undefined\n        ctx_run = <built-in method run of Context object>\n        result = <generator object Kernel.execute_request>\n    235                 except (StopIteration, Return) as e:\n    236                     future_set_result_unless_cancelled(\n    237                         future, _value_from_stopiteration(e)\n    238                     )\n\n...........................................................................\n/Users/annieulichney/opt/anaconda3/lib/python3.8/site-packages/ipykernel/kernelbase.py in execute_request(self=<ipykernel.ipkernel.IPythonKernel object>, stream=<zmq.eventloop.zmqstream.ZMQStream object>, ident=[b'cf5dc902-7071-485f-a33a-9ea2e7aada4b'], parent={'buffers': [], 'content': {'allow_stdin': True, 'code': 'from mlens.ensemble import SuperLearner\\nfrom skl...rain)\\n\\n# Predict\\npreds = ensemble.predict(X_test)', 'silent': False, 'stop_on_error': False, 'store_history': True, 'user_expressions': {}}, 'header': {'date': datetime.datetime(2023, 5, 9, 0, 23, 7, 439000, tzinfo=tzutc()), 'msg_id': '192988bd-1986-4d0d-a104-e363d1fccffd', 'msg_type': 'execute_request', 'session': 'f8677049-449b-4371-ac7d-878e11fe03ff', 'username': '751b3616-df4d-43b3-b034-72e0f89e0c4a', 'version': '5.2'}, 'metadata': {'cellId': 'vscode-notebook-cell:/Users/annieulichney/Docume...eforestation/ResultsWithPolitical.ipynb#ch0000127'}, 'msg_id': '192988bd-1986-4d0d-a104-e363d1fccffd', 'msg_type': 'execute_request', 'parent_header': {}})\n    538         if not silent:\n    539             self.execution_count += 1\n    540             self._publish_execute_input(code, parent, self.execution_count)\n    541 \n    542         reply_content = yield gen.maybe_future(\n--> 543             self.do_execute(\n        self.do_execute = <bound method IPythonKernel.do_execute of <ipykernel.ipkernel.IPythonKernel object>>\n        code = 'from mlens.ensemble import SuperLearner\\nfrom skl...rain)\\n\\n# Predict\\npreds = ensemble.predict(X_test)'\n        silent = False\n        store_history = True\n        user_expressions = {}\n        allow_stdin = True\n    544                 code, silent, store_history,\n    545                 user_expressions, allow_stdin,\n    546             )\n    547         )\n\n...........................................................................\n/Users/annieulichney/opt/anaconda3/lib/python3.8/site-packages/tornado/gen.py in wrapper(*args=(<ipykernel.ipkernel.IPythonKernel object>, 'from mlens.ensemble import SuperLearner\\nfrom skl...rain)\\n\\n# Predict\\npreds = ensemble.predict(X_test)', False, True, {}, True), **kwargs={})\n    229                 # avoid the cost of creating a Runner when the coroutine\n    230                 # never actually yields, which in turn allows us to\n    231                 # use \"optional\" coroutines in critical path code without\n    232                 # performance penalty for the synchronous case.\n    233                 try:\n--> 234                     yielded = ctx_run(next, result)\n        yielded = undefined\n        ctx_run = <built-in method run of Context object>\n        result = <generator object IPythonKernel.do_execute>\n    235                 except (StopIteration, Return) as e:\n    236                     future_set_result_unless_cancelled(\n    237                         future, _value_from_stopiteration(e)\n    238                     )\n\n...........................................................................\n/Users/annieulichney/opt/anaconda3/lib/python3.8/site-packages/ipykernel/ipkernel.py in do_execute(self=<ipykernel.ipkernel.IPythonKernel object>, code='from mlens.ensemble import SuperLearner\\nfrom skl...rain)\\n\\n# Predict\\npreds = ensemble.predict(X_test)', silent=False, store_history=True, user_expressions={}, allow_stdin=True)\n    301                             shell.events.trigger('post_run_cell', res)\n    302             else:\n    303                 # runner isn't already running,\n    304                 # make synchronous call,\n    305                 # letting shell dispatch to loop runners\n--> 306                 res = shell.run_cell(code, store_history=store_history, silent=silent)\n        res = undefined\n        code = 'from mlens.ensemble import SuperLearner\\nfrom skl...rain)\\n\\n# Predict\\npreds = ensemble.predict(X_test)'\n        store_history = True\n        silent = False\n    307         finally:\n    308             self._restore_input()\n    309 \n    310         if res.error_before_exec is not None:\n\n...........................................................................\n/Users/annieulichney/opt/anaconda3/lib/python3.8/site-packages/ipykernel/zmqshell.py in run_cell(self=<ipykernel.zmqshell.ZMQInteractiveShell object>, *args=('from mlens.ensemble import SuperLearner\\nfrom skl...rain)\\n\\n# Predict\\npreds = ensemble.predict(X_test)',), **kwargs={'silent': False, 'store_history': True})\n    531             )\n    532         self.payload_manager.write_payload(payload)\n    533 \n    534     def run_cell(self, *args, **kwargs):\n    535         self._last_traceback = None\n--> 536         return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n        self.run_cell = <bound method ZMQInteractiveShell.run_cell of <ipykernel.zmqshell.ZMQInteractiveShell object>>\n        args = ('from mlens.ensemble import SuperLearner\\nfrom skl...rain)\\n\\n# Predict\\npreds = ensemble.predict(X_test)',)\n        kwargs = {'silent': False, 'store_history': True}\n    537 \n    538     def _showtraceback(self, etype, evalue, stb):\n    539         # try to preserve ordering of tracebacks and print statements\n    540         sys.stdout.flush()\n\n...........................................................................\n/Users/annieulichney/opt/anaconda3/lib/python3.8/site-packages/IPython/core/interactiveshell.py in run_cell(self=<ipykernel.zmqshell.ZMQInteractiveShell object>, raw_cell='from mlens.ensemble import SuperLearner\\nfrom skl...rain)\\n\\n# Predict\\npreds = ensemble.predict(X_test)', store_history=True, silent=False, shell_futures=True)\n   2842         -------\n   2843         result : :class:`ExecutionResult`\n   2844         \"\"\"\n   2845         result = None\n   2846         try:\n-> 2847             result = self._run_cell(\n        result = None\n        self._run_cell = <bound method InteractiveShell._run_cell of <ipykernel.zmqshell.ZMQInteractiveShell object>>\n        raw_cell = 'from mlens.ensemble import SuperLearner\\nfrom skl...rain)\\n\\n# Predict\\npreds = ensemble.predict(X_test)'\n        store_history = True\n        silent = False\n        shell_futures = True\n   2848                 raw_cell, store_history, silent, shell_futures)\n   2849         finally:\n   2850             self.events.trigger('post_execute')\n   2851             if not silent:\n\n...........................................................................\n/Users/annieulichney/opt/anaconda3/lib/python3.8/site-packages/IPython/core/interactiveshell.py in _run_cell(self=<ipykernel.zmqshell.ZMQInteractiveShell object>, raw_cell='from mlens.ensemble import SuperLearner\\nfrom skl...rain)\\n\\n# Predict\\npreds = ensemble.predict(X_test)', store_history=True, silent=False, shell_futures=True)\n   2869             runner = self.loop_runner\n   2870         else:\n   2871             runner = _pseudo_sync_runner\n   2872 \n   2873         try:\n-> 2874             return runner(coro)\n        runner = <function _pseudo_sync_runner>\n        coro = <coroutine object InteractiveShell.run_cell_async>\n   2875         except BaseException as e:\n   2876             info = ExecutionInfo(raw_cell, store_history, silent, shell_futures)\n   2877             result = ExecutionResult(info)\n   2878             result.error_in_exec = e\n\n...........................................................................\n/Users/annieulichney/opt/anaconda3/lib/python3.8/site-packages/IPython/core/async_helpers.py in _pseudo_sync_runner(coro=<coroutine object InteractiveShell.run_cell_async>)\n     63 \n     64     Credit to Nathaniel Smith\n     65 \n     66     \"\"\"\n     67     try:\n---> 68         coro.send(None)\n        coro.send = <built-in method send of coroutine object>\n     69     except StopIteration as exc:\n     70         return exc.value\n     71     else:\n     72         # TODO: do not raise but return an execution result with the right info.\n\n...........................................................................\n/Users/annieulichney/opt/anaconda3/lib/python3.8/site-packages/IPython/core/interactiveshell.py in run_cell_async(self=<ipykernel.zmqshell.ZMQInteractiveShell object>, raw_cell='from mlens.ensemble import SuperLearner\\nfrom skl...rain)\\n\\n# Predict\\npreds = ensemble.predict(X_test)', store_history=True, silent=False, shell_futures=True)\n   3045                 # Execute the user code\n   3046                 interactivity = \"none\" if silent else self.ast_node_interactivity\n   3047                 if _run_async:\n   3048                     interactivity = 'async'\n   3049 \n-> 3050                 has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n        has_raised = undefined\n        self.run_ast_nodes = <bound method InteractiveShell.run_ast_nodes of <ipykernel.zmqshell.ZMQInteractiveShell object>>\n        code_ast.body = [<_ast.ImportFrom object>, <_ast.ImportFrom object>, <_ast.ImportFrom object>, <_ast.ImportFrom object>, <_ast.Assign object>, <_ast.Expr object>, <_ast.Expr object>, <_ast.Expr object>, <_ast.Assign object>]\n        cell_name = '<ipython-input-300-2a626d4720dc>'\n        interactivity = 'last_expr'\n        compiler = <IPython.core.compilerop.CachingCompiler object>\n   3051                        interactivity=interactivity, compiler=compiler, result=result)\n   3052 \n   3053                 self.last_execution_succeeded = not has_raised\n   3054                 self.last_execution_result = result\n\n...........................................................................\n/Users/annieulichney/opt/anaconda3/lib/python3.8/site-packages/IPython/core/interactiveshell.py in run_ast_nodes(self=<ipykernel.zmqshell.ZMQInteractiveShell object>, nodelist=[<_ast.ImportFrom object>, <_ast.ImportFrom object>, <_ast.ImportFrom object>, <_ast.ImportFrom object>, <_ast.Assign object>, <_ast.Expr object>, <_ast.Expr object>, <_ast.Expr object>, <_ast.Assign object>], cell_name='<ipython-input-300-2a626d4720dc>', interactivity='none', compiler=<IPython.core.compilerop.CachingCompiler object>, result=<ExecutionResult object at 7fe6603a1430, executi...rue silent=False shell_futures=True> result=None>)\n   3237                     elif mode == 'single':\n   3238                         mod = ast.Interactive([node])\n   3239                     with compiler.extra_flags(getattr(ast, 'PyCF_ALLOW_TOP_LEVEL_AWAIT', 0x0) if self.autoawait else 0x0):\n   3240                         code = compiler(mod, cell_name, mode)\n   3241                         asy = compare(code)\n-> 3242                     if (await self.run_code(code, result,  async_=asy)):\n        self.run_code = <bound method InteractiveShell.run_code of <ipykernel.zmqshell.ZMQInteractiveShell object>>\n        code = <code object <module> at 0x7fe6581d5030, file \"<ipython-input-300-2a626d4720dc>\", line 20>\n        result = <ExecutionResult object at 7fe6603a1430, executi...rue silent=False shell_futures=True> result=None>\n        asy = False\n   3243                         return True\n   3244 \n   3245             # Flush softspace\n   3246             if softspace(sys.stdout, 0):\n\n...........................................................................\n/Users/annieulichney/opt/anaconda3/lib/python3.8/site-packages/IPython/core/interactiveshell.py in run_code(self=<ipykernel.zmqshell.ZMQInteractiveShell object>, code_obj=<code object <module> at 0x7fe6581d5030, file \"<ipython-input-300-2a626d4720dc>\", line 20>, result=<ExecutionResult object at 7fe6603a1430, executi...rue silent=False shell_futures=True> result=None>, async_=False)\n   3314                     code = compile('last_expr', 'fake', \"single\")\n   3315                     exec(code, {'last_expr': last_expr})\n   3316                 elif async_ :\n   3317                     await eval(code_obj, self.user_global_ns, self.user_ns)\n   3318                 else:\n-> 3319                     exec(code_obj, self.user_global_ns, self.user_ns)\n        code_obj = <code object <module> at 0x7fe6581d5030, file \"<ipython-input-300-2a626d4720dc>\", line 20>\n        self.user_global_ns = {'AdaBoostClassifier': <class 'sklearn.ensemble._weight_boosting.AdaBoostClassifier'>, 'BaggingClassifier': <class 'sklearn.ensemble._bagging.BaggingClassifier'>, 'DecisionTreeClassifier': <class 'sklearn.tree._classes.DecisionTreeClassifier'>, 'ExtraTreesClassifier': <class 'sklearn.ensemble._forest.ExtraTreesClassifier'>, 'GaussianNB': <class 'sklearn.naive_bayes.GaussianNB'>, 'GradientBoostingRegressor': <class 'sklearn.ensemble._gb.GradientBoostingRegressor'>, 'GridSearchCV': <class 'sklearn.model_selection._search.GridSearchCV'>, 'GroupKFold': <class 'sklearn.model_selection._split.GroupKFold'>, 'In': ['', 'import pandas as pd\\nimport numpy as np\\nimport py...the pyshp package\\nimport matplotlib.pyplot as plt', \"df_full = pd.read_csv('/Users/annieulichney/Docu...itHub/Deforestation/FinalData/FinalData2004.csv')\", 'for year in [ 2005, 2006, 2007, 2008, 2009, 2010...ull = pd.concat([df_full, pd.read_csv(filename)])', 'def drop_conditional(cols_list, df):\\n    for col...        df = df.drop(col, axis = 1)\\n    return df', \"#df_full = drop_conditional(['Unnamed: 0', 'ID', ], df_full)\", 'df_full.shape', 'df_full.ID', '#should year be in it? ', \"X_cols  = ['year', 'rain1', 'elevation', 'slope'...erup_votes_proportion', \\n#'overall_winner_idade',\", 'from sklearn.model_selection import GroupKFold, cross_val_predict\\n\\nimport geopandas as gpd', 'df_full.shape[0]', \"df_full = df_full.sample(10000).reset_index(drop...xy(df_full.x, df_full.y))\\n# XYs = gdf['geometry']\", '#Select Test/Train Indices\\nn_folds = 10 \\nmunis =...n(test_inds)/(len(train_inds) + len(test_inds)) )', \"#partition data into test/train sets\\n\\ndf_full_te...est['geometry']\\nXYs_train = gdf_train['geometry']\", 'PLOT = True\\n\\nif PLOT:\\n    fig, axs = plt.subplot...rscale=100)\\n    plt.tight_layout()\\n    plt.show()', '#Select Cross Validation Fold Indices: \\n\\nn_folds...ld)]\\nmuni_cv = [*zip(train_indices,test_indices)]', 'PLOT_FOLDS = False\\nif PLOT_FOLDS: \\n    fig, axs ...rscale=100)\\n    plt.tight_layout()\\n    plt.show()', 'X_train.head()', '# Count null values in each column\\nnull_counts =...tem[1], reverse=True))\\n\\nprint(sorted_null_counts)', ...], 'KFold': <class 'sklearn.model_selection._split.KFold'>, ...}\n        self.user_ns = {'AdaBoostClassifier': <class 'sklearn.ensemble._weight_boosting.AdaBoostClassifier'>, 'BaggingClassifier': <class 'sklearn.ensemble._bagging.BaggingClassifier'>, 'DecisionTreeClassifier': <class 'sklearn.tree._classes.DecisionTreeClassifier'>, 'ExtraTreesClassifier': <class 'sklearn.ensemble._forest.ExtraTreesClassifier'>, 'GaussianNB': <class 'sklearn.naive_bayes.GaussianNB'>, 'GradientBoostingRegressor': <class 'sklearn.ensemble._gb.GradientBoostingRegressor'>, 'GridSearchCV': <class 'sklearn.model_selection._search.GridSearchCV'>, 'GroupKFold': <class 'sklearn.model_selection._split.GroupKFold'>, 'In': ['', 'import pandas as pd\\nimport numpy as np\\nimport py...the pyshp package\\nimport matplotlib.pyplot as plt', \"df_full = pd.read_csv('/Users/annieulichney/Docu...itHub/Deforestation/FinalData/FinalData2004.csv')\", 'for year in [ 2005, 2006, 2007, 2008, 2009, 2010...ull = pd.concat([df_full, pd.read_csv(filename)])', 'def drop_conditional(cols_list, df):\\n    for col...        df = df.drop(col, axis = 1)\\n    return df', \"#df_full = drop_conditional(['Unnamed: 0', 'ID', ], df_full)\", 'df_full.shape', 'df_full.ID', '#should year be in it? ', \"X_cols  = ['year', 'rain1', 'elevation', 'slope'...erup_votes_proportion', \\n#'overall_winner_idade',\", 'from sklearn.model_selection import GroupKFold, cross_val_predict\\n\\nimport geopandas as gpd', 'df_full.shape[0]', \"df_full = df_full.sample(10000).reset_index(drop...xy(df_full.x, df_full.y))\\n# XYs = gdf['geometry']\", '#Select Test/Train Indices\\nn_folds = 10 \\nmunis =...n(test_inds)/(len(train_inds) + len(test_inds)) )', \"#partition data into test/train sets\\n\\ndf_full_te...est['geometry']\\nXYs_train = gdf_train['geometry']\", 'PLOT = True\\n\\nif PLOT:\\n    fig, axs = plt.subplot...rscale=100)\\n    plt.tight_layout()\\n    plt.show()', '#Select Cross Validation Fold Indices: \\n\\nn_folds...ld)]\\nmuni_cv = [*zip(train_indices,test_indices)]', 'PLOT_FOLDS = False\\nif PLOT_FOLDS: \\n    fig, axs ...rscale=100)\\n    plt.tight_layout()\\n    plt.show()', 'X_train.head()', '# Count null values in each column\\nnull_counts =...tem[1], reverse=True))\\n\\nprint(sorted_null_counts)', ...], 'KFold': <class 'sklearn.model_selection._split.KFold'>, ...}\n   3320             finally:\n   3321                 # Reset our crash handler in place\n   3322                 sys.excepthook = old_excepthook\n   3323         except SystemExit as e:\n\n...........................................................................\n/Users/annieulichney/Documents/GitHub/Deforestation/<ipython-input-300-2a626d4720dc> in <module>()\n     15 ensemble.add_meta(LogisticRegression())\n     16 \n     17 # --- Use ---\n     18 \n     19 # Fit ensemble\n---> 20 ensemble.fit(X_train, Y_train)\n     21 \n     22 # Predict\n     23 preds = ensemble.predict(X_test)\n\n...........................................................................\n/Users/annieulichney/opt/anaconda3/lib/python3.8/site-packages/mlens/ensemble/base.py in fit(self=SuperLearner(array_check=None, backend=None, fol...0x7fe65f987040>, shuffle=False,\n       verbose=2), X=    year     rain1  elevation  slope  aspect  ne...03398  \n69    0.000000  \n\n[70 rows x 204 columns], y=0     0.006777\n1     0.000000\n2     0.003566\n3  ...192\nName: forest_diff, Length: 70, dtype: float64, **kwargs={})\n    509             return self\n    510 \n    511         if self.model_selection:\n    512             self._id_train.fit(X)\n    513 \n--> 514         out = self._backend.fit(X, y, **kwargs)\n        out = undefined\n        self._backend.fit = <bound method Sequential.fit of Sequential(backe...nsformers=[])],\n   verbose=1)],\n      verbose=2)>\n        X =     year     rain1  elevation  slope  aspect  ne...03398  \n69    0.000000  \n\n[70 rows x 204 columns]\n        y = 0     0.006777\n1     0.000000\n2     0.003566\n3  ...192\nName: forest_diff, Length: 70, dtype: float64\n        kwargs = {}\n    515         if out is not self._backend:\n    516             # fit_transform\n    517             return out\n    518         else:\n\n...........................................................................\n/Users/annieulichney/opt/anaconda3/lib/python3.8/site-packages/mlens/ensemble/base.py in fit(self=Sequential(backend='threading', dtype=<class 'nu...ansformers=[])],\n   verbose=1)],\n      verbose=2), X=    year     rain1  elevation  slope  aspect  ne...03398  \n69    0.000000  \n\n[70 rows x 204 columns], y=0     0.006777\n1     0.000000\n2     0.003566\n3  ...192\nName: forest_diff, Length: 70, dtype: float64, **kwargs={})\n    153 \n    154         f, t0 = print_job(self, \"Fitting\")\n    155 \n    156         with ParallelProcessing(self.backend, self.n_jobs,\n    157                                 max(self.verbose - 4, 0)) as manager:\n--> 158             out = manager.stack(self, 'fit', X, y, **kwargs)\n        out = undefined\n        manager.stack = <bound method ParallelProcessing.stack of <mlens.parallel.backend.ParallelProcessing object>>\n        self = Sequential(backend='threading', dtype=<class 'nu...ansformers=[])],\n   verbose=1)],\n      verbose=2)\n        X =     year     rain1  elevation  slope  aspect  ne...03398  \n69    0.000000  \n\n[70 rows x 204 columns]\n        y = 0     0.006777\n1     0.000000\n2     0.003566\n3  ...192\nName: forest_diff, Length: 70, dtype: float64\n        kwargs = {}\n    159 \n    160         if self.verbose:\n    161             print_time(t0, \"{:<35}\".format(\"Fit complete\"), file=f)\n    162 \n\n...........................................................................\n/Users/annieulichney/opt/anaconda3/lib/python3.8/site-packages/mlens/parallel/backend.py in stack(self=<mlens.parallel.backend.ParallelProcessing object>, caller=Sequential(backend='threading', dtype=<class 'nu...ansformers=[])],\n   verbose=1)],\n      verbose=2), job='fit', X=    year     rain1  elevation  slope  aspect  ne...03398  \n69    0.000000  \n\n[70 rows x 204 columns], y=0     0.006777\n1     0.000000\n2     0.003566\n3  ...192\nName: forest_diff, Length: 70, dtype: float64, path=None, return_preds=False, warm_start=False, split=True, **kwargs={})\n    668             Prediction array(s).\n    669         \"\"\"\n    670         out = self.initialize(\n    671             job=job, X=X, y=y, path=path, warm_start=warm_start,\n    672             return_preds=return_preds, split=split, stack=True)\n--> 673         return self.process(caller=caller, out=out, **kwargs)\n        self.process = <bound method ParallelProcessing.process of <mlens.parallel.backend.ParallelProcessing object>>\n        caller = Sequential(backend='threading', dtype=<class 'nu...ansformers=[])],\n   verbose=1)],\n      verbose=2)\n        out = {}\n        kwargs = {}\n    674 \n    675     def process(self, caller, out, **kwargs):\n    676         \"\"\"Process job.\n    677 \n\n...........................................................................\n/Users/annieulichney/opt/anaconda3/lib/python3.8/site-packages/mlens/parallel/backend.py in process(self=<mlens.parallel.backend.ParallelProcessing object>, caller=Sequential(backend='threading', dtype=<class 'nu...ansformers=[])],\n   verbose=1)],\n      verbose=2), out=None, **kwargs={})\n    713                       backend=self.backend) as parallel:\n    714 \n    715             for task in caller:\n    716                 self.job.clear()\n    717 \n--> 718                 self._partial_process(task, parallel, **kwargs)\n        self._partial_process = <bound method ParallelProcessing._partial_proces...lens.parallel.backend.ParallelProcessing object>>\n        task = Layer(backend='threading', dtype=<class 'numpy.f..._exception=True, transformers=[])],\n   verbose=1)\n        parallel = Parallel(n_jobs=-1)\n        kwargs = {}\n    719 \n    720                 if task.name in return_names:\n    721                     out.append(self.get_preds(dtype=_dtype(task)))\n    722 \n\n...........................................................................\n/Users/annieulichney/opt/anaconda3/lib/python3.8/site-packages/mlens/parallel/backend.py in _partial_process(self=<mlens.parallel.backend.ParallelProcessing object>, task=Layer(backend='threading', dtype=<class 'numpy.f..._exception=True, transformers=[])],\n   verbose=1), parallel=Parallel(n_jobs=-1), **kwargs={})\n    734         task.setup(self.job.predict_in, self.job.targets, self.job.job)\n    735 \n    736         if not task.__no_output__:\n    737             self._gen_prediction_array(task, self.job.job, self.__threading__)\n    738 \n--> 739         task(self.job.args(**kwargs), parallel=parallel)\n        task = Layer(backend='threading', dtype=<class 'numpy.f..._exception=True, transformers=[])],\n   verbose=1)\n        self.job.args = <bound method Job.args of <mlens.parallel.backend.Job object>>\n        kwargs = {}\n        parallel = Parallel(n_jobs=-1)\n    740 \n    741         if not task.__no_output__ and getattr(task, 'n_feature_prop', 0):\n    742             self._propagate_features(task)\n    743 \n\n...........................................................................\n/Users/annieulichney/opt/anaconda3/lib/python3.8/site-packages/mlens/parallel/layer.py in __call__(self=Layer(backend='threading', dtype=<class 'numpy.f..._exception=True, transformers=[])],\n   verbose=1), args={'auxiliary': {'P': None, 'X': array([[-3.36665614e-03, -6.37534400e-03, -5.410...51e-02,\n        -1.52171021e-02]], dtype=float32), 'y': 0     0.006777\n1     0.000000\n2     0.003566\n3  ...192\nName: forest_diff, Length: 70, dtype: float64}, 'dir': [], 'job': 'fit', 'main': {'P': array([[0.],\n       [0.],\n       [0.],\n       [0...  [0.],\n       [0.],\n       [0.]], dtype=float32), 'X': array([[-3.36665614e-03, -6.37534400e-03, -5.410...51e-02,\n        -1.52171021e-02]], dtype=float32), 'y': 0     0.006777\n1     0.000000\n2     0.003566\n3  ...192\nName: forest_diff, Length: 70, dtype: float64}}, parallel=Parallel(n_jobs=-1))\n    146 \n    147         if self.verbose >= 2:\n    148             safe_print(msg.format('Learners ...'), file=f, end=e2)\n    149             t1 = time()\n    150 \n--> 151         parallel(delayed(sublearner, not _threading)()\n        parallel = Parallel(n_jobs=-1)\n        self.learners = [Learner(attr='predict', backend='threading', dty...orer=<function accuracy_score at 0x7fe65f987040>)]\n        args = {'auxiliary': {'P': None, 'X': array([[-3.36665614e-03, -6.37534400e-03, -5.410...51e-02,\n        -1.52171021e-02]], dtype=float32), 'y': 0     0.006777\n1     0.000000\n2     0.003566\n3  ...192\nName: forest_diff, Length: 70, dtype: float64}, 'dir': [], 'job': 'fit', 'main': {'P': array([[0.],\n       [0.],\n       [0.],\n       [0...  [0.],\n       [0.],\n       [0.]], dtype=float32), 'X': array([[-3.36665614e-03, -6.37534400e-03, -5.410...51e-02,\n        -1.52171021e-02]], dtype=float32), 'y': 0     0.006777\n1     0.000000\n2     0.003566\n3  ...192\nName: forest_diff, Length: 70, dtype: float64}}\n    152                  for learner in self.learners\n    153                  for sublearner in learner(args, 'main'))\n    154 \n    155         if self.verbose >= 2:\n\n...........................................................................\n/Users/annieulichney/opt/anaconda3/lib/python3.8/site-packages/mlens/externals/joblib/parallel.py in __call__(self=Parallel(n_jobs=-1), iterable=<generator object Layer.__call__.<locals>.<genexpr>>)\n    788             if pre_dispatch == \"all\" or n_jobs == 1:\n    789                 # The iterable was consumed all at once by the above for loop.\n    790                 # No need to wait for async callbacks to trigger to\n    791                 # consumption.\n    792                 self._iterating = False\n--> 793             self.retrieve()\n        self.retrieve = <bound method Parallel.retrieve of Parallel(n_jobs=-1)>\n    794             # Make sure that we get a last message telling us we are done\n    795             elapsed_time = time.time() - self._start_time\n    796             self._print('Done %3i out of %3i | elapsed: %s finished',\n    797                         (len(self._output), len(self._output),\n\n---------------------------------------------------------------------------\nSub-process traceback:\n---------------------------------------------------------------------------\nValueError                                         Mon May  8 20:23:14 2023\nPID: 25543      Python 3.8.8: /Users/annieulichney/opt/anaconda3/bin/python\n...........................................................................\n/Users/annieulichney/opt/anaconda3/lib/python3.8/site-packages/mlens/externals/joblib/parallel.py in __call__(self=<mlens.externals.joblib.parallel.BatchedCalls object>)\n    130     def __init__(self, iterator_slice):\n    131         self.items = list(iterator_slice)\n    132         self._size = len(self.items)\n    133 \n    134     def __call__(self):\n--> 135         return [func(*args, **kwargs) for func, args, kwargs in self.items]\n        self.items = [(<mlens.parallel.learner.SubLearner object>, (), {})]\n    136 \n    137     def __len__(self):\n    138         return self._size\n    139 \n\n...........................................................................\n/Users/annieulichney/opt/anaconda3/lib/python3.8/site-packages/mlens/externals/joblib/parallel.py in <listcomp>(.0=<list_iterator object>)\n    130     def __init__(self, iterator_slice):\n    131         self.items = list(iterator_slice)\n    132         self._size = len(self.items)\n    133 \n    134     def __call__(self):\n--> 135         return [func(*args, **kwargs) for func, args, kwargs in self.items]\n        func = <mlens.parallel.learner.SubLearner object>\n        args = ()\n        kwargs = {}\n    136 \n    137     def __len__(self):\n    138         return self._size\n    139 \n\n...........................................................................\n/Users/annieulichney/opt/anaconda3/lib/python3.8/site-packages/mlens/parallel/learner.py in __call__(self=<mlens.parallel.learner.SubLearner object>)\n    119         else:\n    120             self.processing_index = ''\n    121 \n    122     def __call__(self):\n    123         \"\"\"Launch job\"\"\"\n--> 124         return getattr(self, self.job)()\n        self = <mlens.parallel.learner.SubLearner object>\n        self.job = 'fit'\n    125 \n    126     def fit(self, path=None):\n    127         \"\"\"Fit sub-learner\"\"\"\n    128         if path is None:\n\n...........................................................................\n/Users/annieulichney/opt/anaconda3/lib/python3.8/site-packages/mlens/parallel/learner.py in fit(self=<mlens.parallel.learner.SubLearner object>, path=[])\n    128         if path is None:\n    129             path = self.path\n    130         t0 = time()\n    131         transformers = self._load_preprocess(path)\n    132 \n--> 133         self._fit(transformers)\n        self._fit = <bound method SubLearner._fit of <mlens.parallel.learner.SubLearner object>>\n        transformers = None\n    134 \n    135         if self.out_array is not None:\n    136             self._predict(transformers, self.scorer is not None)\n    137 \n\n...........................................................................\n/Users/annieulichney/opt/anaconda3/lib/python3.8/site-packages/mlens/parallel/learner.py in _fit(self=<mlens.parallel.learner.SubLearner object>, transformers=None)\n    174         t0 = time()\n    175         if transformers:\n    176             xtemp, ytemp = transformers.transform(xtemp, ytemp)\n    177 \n    178         # Fit estimator\n--> 179         self.estimator.fit(xtemp, ytemp)\n        self.estimator.fit = <bound method LogisticRegression.fit of LogisticRegression()>\n        xtemp = array([[-3.36665614e-03, -6.37534400e-03, -5.410...51e-02,\n        -1.52171021e-02]], dtype=float32)\n        ytemp = 0     0.006777\n1     0.000000\n2     0.003566\n3  ...192\nName: forest_diff, Length: 70, dtype: float64\n    180         self.fit_time_ = time() - t0\n    181 \n    182     def _load_preprocess(self, path):\n    183         \"\"\"Load preprocessing pipeline\"\"\"\n\n...........................................................................\n/Users/annieulichney/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py in fit(self=LogisticRegression(), X=array([[-3.36665614e-03, -6.37534400e-03, -5.410...e-02,  3.90149951e-02,\n        -1.52171021e-02]]), y=array([ 0.00677705,  0.        ,  0.00356555, -0...00170457, -0.01167369,  0.0016818 ,  0.00519219]), sample_weight=None)\n   1141             accept_sparse=\"csr\",\n   1142             dtype=_dtype,\n   1143             order=\"C\",\n   1144             accept_large_sparse=solver not in [\"liblinear\", \"sag\", \"saga\"],\n   1145         )\n-> 1146         check_classification_targets(y)\n        y = array([ 0.00677705,  0.        ,  0.00356555, -0...00170457, -0.01167369,  0.0016818 ,  0.00519219])\n   1147         self.classes_ = np.unique(y)\n   1148 \n   1149         multi_class = _check_multi_class(self.multi_class, solver, len(self.classes_))\n   1150 \n\n...........................................................................\n/Users/annieulichney/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/multiclass.py in check_classification_targets(y=array([ 0.00677705,  0.        ,  0.00356555, -0...00170457, -0.01167369,  0.0016818 ,  0.00519219]))\n    195         \"multiclass\",\n    196         \"multiclass-multioutput\",\n    197         \"multilabel-indicator\",\n    198         \"multilabel-sequences\",\n    199     ]:\n--> 200         raise ValueError(\"Unknown label type: %r\" % y_type)\n        y_type = 'continuous'\n    201 \n    202 \n    203 def type_of_target(y, input_name=\"\"):\n    204     \"\"\"Determine the type of data indicated by the target.\n\nValueError: Unknown label type: 'continuous'\n___________________________________________________________________________",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/mlens/externals/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    349\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 350\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    351\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/mlens/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    134\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 135\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    136\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/mlens/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    134\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 135\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    136\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/mlens/parallel/learner.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    123\u001b[0m         \u001b[0;34m\"\"\"Launch job\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 124\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    125\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/mlens/parallel/learner.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, path)\u001b[0m\n\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 133\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtransformers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/mlens/parallel/learner.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, transformers)\u001b[0m\n\u001b[1;32m    178\u001b[0m         \u001b[0;31m# Fit estimator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 179\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxtemp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mytemp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    180\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_time_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mt0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m   1145\u001b[0m         )\n\u001b[0;32m-> 1146\u001b[0;31m         \u001b[0mcheck_classification_targets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1147\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/multiclass.py\u001b[0m in \u001b[0;36mcheck_classification_targets\u001b[0;34m(y)\u001b[0m\n\u001b[1;32m    199\u001b[0m     ]:\n\u001b[0;32m--> 200\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Unknown label type: %r\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0my_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    201\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Unknown label type: 'continuous'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mTransportableException\u001b[0m                    Traceback (most recent call last)",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/mlens/externals/joblib/parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    702\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'supports_timeout'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 703\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    704\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/multiprocessing/pool.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    770\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 771\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    772\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/multiprocessing/pool.py\u001b[0m in \u001b[0;36mworker\u001b[0;34m(inqueue, outqueue, initializer, initargs, maxtasks, wrap_exception)\u001b[0m\n\u001b[1;32m    124\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/mlens/externals/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    358\u001b[0m             \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mformat_exc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me_tb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtb_offset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 359\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mTransportableException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    360\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTransportableException\u001b[0m: TransportableException\n___________________________________________________________________________\nValueError                                         Mon May  8 20:23:14 2023\nPID: 25543      Python 3.8.8: /Users/annieulichney/opt/anaconda3/bin/python\n...........................................................................\n/Users/annieulichney/opt/anaconda3/lib/python3.8/site-packages/mlens/externals/joblib/parallel.py in __call__(self=<mlens.externals.joblib.parallel.BatchedCalls object>)\n    130     def __init__(self, iterator_slice):\n    131         self.items = list(iterator_slice)\n    132         self._size = len(self.items)\n    133 \n    134     def __call__(self):\n--> 135         return [func(*args, **kwargs) for func, args, kwargs in self.items]\n        self.items = [(<mlens.parallel.learner.SubLearner object>, (), {})]\n    136 \n    137     def __len__(self):\n    138         return self._size\n    139 \n\n...........................................................................\n/Users/annieulichney/opt/anaconda3/lib/python3.8/site-packages/mlens/externals/joblib/parallel.py in <listcomp>(.0=<list_iterator object>)\n    130     def __init__(self, iterator_slice):\n    131         self.items = list(iterator_slice)\n    132         self._size = len(self.items)\n    133 \n    134     def __call__(self):\n--> 135         return [func(*args, **kwargs) for func, args, kwargs in self.items]\n        func = <mlens.parallel.learner.SubLearner object>\n        args = ()\n        kwargs = {}\n    136 \n    137     def __len__(self):\n    138         return self._size\n    139 \n\n...........................................................................\n/Users/annieulichney/opt/anaconda3/lib/python3.8/site-packages/mlens/parallel/learner.py in __call__(self=<mlens.parallel.learner.SubLearner object>)\n    119         else:\n    120             self.processing_index = ''\n    121 \n    122     def __call__(self):\n    123         \"\"\"Launch job\"\"\"\n--> 124         return getattr(self, self.job)()\n        self = <mlens.parallel.learner.SubLearner object>\n        self.job = 'fit'\n    125 \n    126     def fit(self, path=None):\n    127         \"\"\"Fit sub-learner\"\"\"\n    128         if path is None:\n\n...........................................................................\n/Users/annieulichney/opt/anaconda3/lib/python3.8/site-packages/mlens/parallel/learner.py in fit(self=<mlens.parallel.learner.SubLearner object>, path=[])\n    128         if path is None:\n    129             path = self.path\n    130         t0 = time()\n    131         transformers = self._load_preprocess(path)\n    132 \n--> 133         self._fit(transformers)\n        self._fit = <bound method SubLearner._fit of <mlens.parallel.learner.SubLearner object>>\n        transformers = None\n    134 \n    135         if self.out_array is not None:\n    136             self._predict(transformers, self.scorer is not None)\n    137 \n\n...........................................................................\n/Users/annieulichney/opt/anaconda3/lib/python3.8/site-packages/mlens/parallel/learner.py in _fit(self=<mlens.parallel.learner.SubLearner object>, transformers=None)\n    174         t0 = time()\n    175         if transformers:\n    176             xtemp, ytemp = transformers.transform(xtemp, ytemp)\n    177 \n    178         # Fit estimator\n--> 179         self.estimator.fit(xtemp, ytemp)\n        self.estimator.fit = <bound method LogisticRegression.fit of LogisticRegression()>\n        xtemp = array([[-3.36665614e-03, -6.37534400e-03, -5.410...51e-02,\n        -1.52171021e-02]], dtype=float32)\n        ytemp = 0     0.006777\n1     0.000000\n2     0.003566\n3  ...192\nName: forest_diff, Length: 70, dtype: float64\n    180         self.fit_time_ = time() - t0\n    181 \n    182     def _load_preprocess(self, path):\n    183         \"\"\"Load preprocessing pipeline\"\"\"\n\n...........................................................................\n/Users/annieulichney/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py in fit(self=LogisticRegression(), X=array([[-3.36665614e-03, -6.37534400e-03, -5.410...e-02,  3.90149951e-02,\n        -1.52171021e-02]]), y=array([ 0.00677705,  0.        ,  0.00356555, -0...00170457, -0.01167369,  0.0016818 ,  0.00519219]), sample_weight=None)\n   1141             accept_sparse=\"csr\",\n   1142             dtype=_dtype,\n   1143             order=\"C\",\n   1144             accept_large_sparse=solver not in [\"liblinear\", \"sag\", \"saga\"],\n   1145         )\n-> 1146         check_classification_targets(y)\n        y = array([ 0.00677705,  0.        ,  0.00356555, -0...00170457, -0.01167369,  0.0016818 ,  0.00519219])\n   1147         self.classes_ = np.unique(y)\n   1148 \n   1149         multi_class = _check_multi_class(self.multi_class, solver, len(self.classes_))\n   1150 \n\n...........................................................................\n/Users/annieulichney/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/multiclass.py in check_classification_targets(y=array([ 0.00677705,  0.        ,  0.00356555, -0...00170457, -0.01167369,  0.0016818 ,  0.00519219]))\n    195         \"multiclass\",\n    196         \"multiclass-multioutput\",\n    197         \"multilabel-indicator\",\n    198         \"multilabel-sequences\",\n    199     ]:\n--> 200         raise ValueError(\"Unknown label type: %r\" % y_type)\n        y_type = 'continuous'\n    201 \n    202 \n    203 def type_of_target(y, input_name=\"\"):\n    204     \"\"\"Determine the type of data indicated by the target.\n\nValueError: Unknown label type: 'continuous'\n___________________________________________________________________________",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mJoblibValueError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-300-2a626d4720dc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;31m# Fit ensemble\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0mensemble\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;31m# Predict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/mlens/ensemble/base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, **kwargs)\u001b[0m\n\u001b[1;32m    512\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_id_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    513\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 514\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    515\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    516\u001b[0m             \u001b[0;31m# fit_transform\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/mlens/ensemble/base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, **kwargs)\u001b[0m\n\u001b[1;32m    156\u001b[0m         with ParallelProcessing(self.backend, self.n_jobs,\n\u001b[1;32m    157\u001b[0m                                 max(self.verbose - 4, 0)) as manager:\n\u001b[0;32m--> 158\u001b[0;31m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmanager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'fit'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    159\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/mlens/parallel/backend.py\u001b[0m in \u001b[0;36mstack\u001b[0;34m(self, caller, job, X, y, path, return_preds, warm_start, split, **kwargs)\u001b[0m\n\u001b[1;32m    671\u001b[0m             \u001b[0mjob\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwarm_start\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwarm_start\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    672\u001b[0m             return_preds=return_preds, split=split, stack=True)\n\u001b[0;32m--> 673\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcaller\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcaller\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    674\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    675\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaller\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/mlens/parallel/backend.py\u001b[0m in \u001b[0;36mprocess\u001b[0;34m(self, caller, out, **kwargs)\u001b[0m\n\u001b[1;32m    716\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    717\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 718\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_partial_process\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparallel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    719\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    720\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mtask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreturn_names\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/mlens/parallel/backend.py\u001b[0m in \u001b[0;36m_partial_process\u001b[0;34m(self, task, parallel, **kwargs)\u001b[0m\n\u001b[1;32m    737\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gen_prediction_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__threading__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    738\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 739\u001b[0;31m         \u001b[0mtask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparallel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparallel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    741\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__no_output__\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'n_feature_prop'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/mlens/parallel/layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, args, parallel)\u001b[0m\n\u001b[1;32m    149\u001b[0m             \u001b[0mt1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 151\u001b[0;31m         parallel(delayed(sublearner, not _threading)()\n\u001b[0m\u001b[1;32m    152\u001b[0m                  \u001b[0;32mfor\u001b[0m \u001b[0mlearner\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearners\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m                  for sublearner in learner(args, 'main'))\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/mlens/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m    791\u001b[0m                 \u001b[0;31m# consumption.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    792\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 793\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    794\u001b[0m             \u001b[0;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    795\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/mlens/externals/joblib/parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    742\u001b[0m                     \u001b[0mexception\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexception_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreport\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    743\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 744\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0mexception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    745\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    746\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mJoblibValueError\u001b[0m: JoblibValueError\n___________________________________________________________________________\nMultiprocessing exception:\n...........................................................................\n/Users/annieulichney/opt/anaconda3/lib/python3.8/runpy.py in _run_module_as_main(mod_name='ipykernel_launcher', alter_argv=1)\n    189         msg = \"%s: %s\" % (sys.executable, exc)\n    190         sys.exit(msg)\n    191     main_globals = sys.modules[\"__main__\"].__dict__\n    192     if alter_argv:\n    193         sys.argv[0] = mod_spec.origin\n--> 194     return _run_code(code, main_globals, None,\n        code = <code object <module> at 0x7feb99e44d40, file \"/...3.8/site-packages/ipykernel_launcher.py\", line 1>\n        main_globals = {'__annotations__': {}, '__builtins__': <module 'builtins' (built-in)>, '__cached__': '/Users/annieulichney/opt/anaconda3/lib/python3.8...ges/__pycache__/ipykernel_launcher.cpython-38.pyc', '__doc__': 'Entry point for launching an IPython kernel.\\n\\nTh...orts until\\nafter removing the cwd from sys.path.\\n', '__file__': '/Users/annieulichney/opt/anaconda3/lib/python3.8/site-packages/ipykernel_launcher.py', '__loader__': <_frozen_importlib_external.SourceFileLoader object>, '__name__': '__main__', '__package__': '', '__spec__': ModuleSpec(name='ipykernel_launcher', loader=<_f...b/python3.8/site-packages/ipykernel_launcher.py'), 'app': <module 'ipykernel.kernelapp' from '/Users/annie.../python3.8/site-packages/ipykernel/kernelapp.py'>, ...}\n        mod_spec = ModuleSpec(name='ipykernel_launcher', loader=<_f...b/python3.8/site-packages/ipykernel_launcher.py')\n    195                      \"__main__\", mod_spec)\n    196 \n    197 def run_module(mod_name, init_globals=None,\n    198                run_name=None, alter_sys=False):\n\n...........................................................................\n/Users/annieulichney/opt/anaconda3/lib/python3.8/runpy.py in _run_code(code=<code object <module> at 0x7feb99e44d40, file \"/...3.8/site-packages/ipykernel_launcher.py\", line 1>, run_globals={'__annotations__': {}, '__builtins__': <module 'builtins' (built-in)>, '__cached__': '/Users/annieulichney/opt/anaconda3/lib/python3.8...ges/__pycache__/ipykernel_launcher.cpython-38.pyc', '__doc__': 'Entry point for launching an IPython kernel.\\n\\nTh...orts until\\nafter removing the cwd from sys.path.\\n', '__file__': '/Users/annieulichney/opt/anaconda3/lib/python3.8/site-packages/ipykernel_launcher.py', '__loader__': <_frozen_importlib_external.SourceFileLoader object>, '__name__': '__main__', '__package__': '', '__spec__': ModuleSpec(name='ipykernel_launcher', loader=<_f...b/python3.8/site-packages/ipykernel_launcher.py'), 'app': <module 'ipykernel.kernelapp' from '/Users/annie.../python3.8/site-packages/ipykernel/kernelapp.py'>, ...}, init_globals=None, mod_name='__main__', mod_spec=ModuleSpec(name='ipykernel_launcher', loader=<_f...b/python3.8/site-packages/ipykernel_launcher.py'), pkg_name='', script_name=None)\n     82                        __cached__ = cached,\n     83                        __doc__ = None,\n     84                        __loader__ = loader,\n     85                        __package__ = pkg_name,\n     86                        __spec__ = mod_spec)\n---> 87     exec(code, run_globals)\n        code = <code object <module> at 0x7feb99e44d40, file \"/...3.8/site-packages/ipykernel_launcher.py\", line 1>\n        run_globals = {'__annotations__': {}, '__builtins__': <module 'builtins' (built-in)>, '__cached__': '/Users/annieulichney/opt/anaconda3/lib/python3.8...ges/__pycache__/ipykernel_launcher.cpython-38.pyc', '__doc__': 'Entry point for launching an IPython kernel.\\n\\nTh...orts until\\nafter removing the cwd from sys.path.\\n', '__file__': '/Users/annieulichney/opt/anaconda3/lib/python3.8/site-packages/ipykernel_launcher.py', '__loader__': <_frozen_importlib_external.SourceFileLoader object>, '__name__': '__main__', '__package__': '', '__spec__': ModuleSpec(name='ipykernel_launcher', loader=<_f...b/python3.8/site-packages/ipykernel_launcher.py'), 'app': <module 'ipykernel.kernelapp' from '/Users/annie.../python3.8/site-packages/ipykernel/kernelapp.py'>, ...}\n     88     return run_globals\n     89 \n     90 def _run_module_code(code, init_globals=None,\n     91                     mod_name=None, mod_spec=None,\n\n...........................................................................\n/Users/annieulichney/opt/anaconda3/lib/python3.8/site-packages/ipykernel_launcher.py in <module>()\n     11     # This is added back by InteractiveShellApp.init_path()\n     12     if sys.path[0] == '':\n     13         del sys.path[0]\n     14 \n     15     from ipykernel import kernelapp as app\n---> 16     app.launch_new_instance()\n\n...........................................................................\n/Users/annieulichney/opt/anaconda3/lib/python3.8/site-packages/traitlets/config/application.py in launch_instance(cls=<class 'ipykernel.kernelapp.IPKernelApp'>, argv=None, **kwargs={})\n    840 \n    841         If a global instance already exists, this reinitializes and starts it\n    842         \"\"\"\n    843         app = cls.instance(**kwargs)\n    844         app.initialize(argv)\n--> 845         app.start()\n        app.start = <bound method IPKernelApp.start of <ipykernel.kernelapp.IPKernelApp object>>\n    846 \n    847 #-----------------------------------------------------------------------------\n    848 # utility functions, for convenience\n    849 #-----------------------------------------------------------------------------\n\n...........................................................................\n/Users/annieulichney/opt/anaconda3/lib/python3.8/site-packages/ipykernel/kernelapp.py in start(self=<ipykernel.kernelapp.IPKernelApp object>)\n    607                 tr.run()\n    608             except KeyboardInterrupt:\n    609                 pass\n    610         else:\n    611             try:\n--> 612                 self.io_loop.start()\n        self.io_loop.start = <bound method BaseAsyncIOLoop.start of <tornado.platform.asyncio.AsyncIOMainLoop object>>\n    613             except KeyboardInterrupt:\n    614                 pass\n    615 \n    616 \n\n...........................................................................\n/Users/annieulichney/opt/anaconda3/lib/python3.8/site-packages/tornado/platform/asyncio.py in start(self=<tornado.platform.asyncio.AsyncIOMainLoop object>)\n    194         except (RuntimeError, AssertionError):\n    195             old_loop = None  # type: ignore\n    196         try:\n    197             self._setup_logging()\n    198             asyncio.set_event_loop(self.asyncio_loop)\n--> 199             self.asyncio_loop.run_forever()\n        self.asyncio_loop.run_forever = <bound method BaseEventLoop.run_forever of <_Uni...EventLoop running=True closed=False debug=False>>\n    200         finally:\n    201             asyncio.set_event_loop(old_loop)\n    202 \n    203     def stop(self) -> None:\n\n...........................................................................\n/Users/annieulichney/opt/anaconda3/lib/python3.8/asyncio/base_events.py in run_forever(self=<_UnixSelectorEventLoop running=True closed=False debug=False>)\n    565         sys.set_asyncgen_hooks(firstiter=self._asyncgen_firstiter_hook,\n    566                                finalizer=self._asyncgen_finalizer_hook)\n    567         try:\n    568             events._set_running_loop(self)\n    569             while True:\n--> 570                 self._run_once()\n        self._run_once = <bound method BaseEventLoop._run_once of <_UnixS...EventLoop running=True closed=False debug=False>>\n    571                 if self._stopping:\n    572                     break\n    573         finally:\n    574             self._stopping = False\n\n...........................................................................\n/Users/annieulichney/opt/anaconda3/lib/python3.8/asyncio/base_events.py in _run_once(self=<_UnixSelectorEventLoop running=True closed=False debug=False>)\n   1854                         logger.warning('Executing %s took %.3f seconds',\n   1855                                        _format_handle(handle), dt)\n   1856                 finally:\n   1857                     self._current_handle = None\n   1858             else:\n-> 1859                 handle._run()\n        handle._run = <bound method Handle._run of <Handle IOLoop.add_...b/python3.8/site-packages/tornado/ioloop.py:688>>\n   1860         handle = None  # Needed to break cycles when an exception occurs.\n   1861 \n   1862     def _set_coroutine_origin_tracking(self, enabled):\n   1863         if bool(enabled) == bool(self._coroutine_origin_tracking_enabled):\n\n...........................................................................\n/Users/annieulichney/opt/anaconda3/lib/python3.8/asyncio/events.py in _run(self=<Handle IOLoop.add_future.<locals>.<lambda>(<Fut...ib/python3.8/site-packages/tornado/ioloop.py:688>)\n     76     def cancelled(self):\n     77         return self._cancelled\n     78 \n     79     def _run(self):\n     80         try:\n---> 81             self._context.run(self._callback, *self._args)\n        self._context.run = <built-in method run of Context object>\n        self._callback = <function IOLoop.add_future.<locals>.<lambda>>\n        self._args = (<Future finished result=(10, 632, <bound method....9453720>, <zmq.sugar.fr...x7fe65d5bed50>, ...]))>,)\n     82         except (SystemExit, KeyboardInterrupt):\n     83             raise\n     84         except BaseException as exc:\n     85             cb = format_helpers._format_callback_source(\n\n...........................................................................\n/Users/annieulichney/opt/anaconda3/lib/python3.8/site-packages/tornado/ioloop.py in <lambda>(f=<Future finished result=(10, 632, <bound method....9453720>, <zmq.sugar.fr...x7fe65d5bed50>, ...]))>)\n    683             #\n    684             # Wrap the callback in self._run_callback so we control\n    685             # the error logging (i.e. it goes to tornado.log.app_log\n    686             # instead of asyncio's log).\n    687             future.add_done_callback(\n--> 688                 lambda f: self._run_callback(functools.partial(callback, future))\n        f = <Future finished result=(10, 632, <bound method....9453720>, <zmq.sugar.fr...x7fe65d5bed50>, ...]))>\n    689             )\n    690         else:\n    691             assert is_future(future)\n    692             # For concurrent futures, we use self.add_callback, so\n\n...........................................................................\n/Users/annieulichney/opt/anaconda3/lib/python3.8/site-packages/tornado/ioloop.py in _run_callback(self=<tornado.platform.asyncio.AsyncIOMainLoop object>, callback=functools.partial(<function Runner.handle_yield....453720>, <zmq.sugar.fr...x7fe65d5bed50>, ...]))>))\n    736         .. versionchanged:: 6.0\n    737 \n    738            CancelledErrors are no longer logged.\n    739         \"\"\"\n    740         try:\n--> 741             ret = callback()\n        ret = undefined\n        callback = functools.partial(<function Runner.handle_yield....453720>, <zmq.sugar.fr...x7fe65d5bed50>, ...]))>)\n    742             if ret is not None:\n    743                 from tornado import gen\n    744 \n    745                 # Functions that return Futures typically swallow all\n\n...........................................................................\n/Users/annieulichney/opt/anaconda3/lib/python3.8/site-packages/tornado/gen.py in inner(f=None)\n    809         elif not self.future.done():\n    810 \n    811             def inner(f: Any) -> None:\n    812                 # Break a reference cycle to speed GC.\n    813                 f = None  # noqa: F841\n--> 814                 self.ctx_run(self.run)\n    815 \n    816             self.io_loop.add_future(self.future, inner)\n    817             return False\n    818         return True\n\n...........................................................................\n/Users/annieulichney/opt/anaconda3/lib/python3.8/site-packages/tornado/gen.py in run(self=<tornado.gen.Runner object>)\n    770                         finally:\n    771                             # Break up a reference to itself\n    772                             # for faster GC on CPython.\n    773                             exc_info = None\n    774                     else:\n--> 775                         yielded = self.gen.send(value)\n        yielded = undefined\n        self.gen.send = <built-in method send of generator object>\n        value = (10, 632, <bound method Kernel.dispatch_shell of <ipykernel.ipkernel.IPythonKernel object>>, (<zmq.eventloop.zmqstream.ZMQStream object>, [<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>]))\n    776 \n    777                 except (StopIteration, Return) as e:\n    778                     self.finished = True\n    779                     self.future = _null_future\n\n...........................................................................\n/Users/annieulichney/opt/anaconda3/lib/python3.8/site-packages/ipykernel/kernelbase.py in process_one(self=<ipykernel.ipkernel.IPythonKernel object>, wait=True)\n    360         else:\n    361             try:\n    362                 priority, t, dispatch, args = self.msg_queue.get_nowait()\n    363             except QueueEmpty:\n    364                 return None\n--> 365         yield gen.maybe_future(dispatch(*args))\n        dispatch = <bound method Kernel.dispatch_shell of <ipykernel.ipkernel.IPythonKernel object>>\n        args = (<zmq.eventloop.zmqstream.ZMQStream object>, [<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>])\n    366 \n    367     @gen.coroutine\n    368     def dispatch_queue(self):\n    369         \"\"\"Coroutine to preserve order of message handling\n\n...........................................................................\n/Users/annieulichney/opt/anaconda3/lib/python3.8/site-packages/tornado/gen.py in wrapper(*args=(<ipykernel.ipkernel.IPythonKernel object>, <zmq.eventloop.zmqstream.ZMQStream object>, [<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>]), **kwargs={})\n    229                 # avoid the cost of creating a Runner when the coroutine\n    230                 # never actually yields, which in turn allows us to\n    231                 # use \"optional\" coroutines in critical path code without\n    232                 # performance penalty for the synchronous case.\n    233                 try:\n--> 234                     yielded = ctx_run(next, result)\n        yielded = undefined\n        ctx_run = <built-in method run of Context object>\n        result = <generator object Kernel.dispatch_shell>\n    235                 except (StopIteration, Return) as e:\n    236                     future_set_result_unless_cancelled(\n    237                         future, _value_from_stopiteration(e)\n    238                     )\n\n...........................................................................\n/Users/annieulichney/opt/anaconda3/lib/python3.8/site-packages/ipykernel/kernelbase.py in dispatch_shell(self=<ipykernel.ipkernel.IPythonKernel object>, stream=<zmq.eventloop.zmqstream.ZMQStream object>, msg={'buffers': [], 'content': {'allow_stdin': True, 'code': 'from mlens.ensemble import SuperLearner\\nfrom skl...rain)\\n\\n# Predict\\npreds = ensemble.predict(X_test)', 'silent': False, 'stop_on_error': False, 'store_history': True, 'user_expressions': {}}, 'header': {'date': datetime.datetime(2023, 5, 9, 0, 23, 7, 439000, tzinfo=tzutc()), 'msg_id': '192988bd-1986-4d0d-a104-e363d1fccffd', 'msg_type': 'execute_request', 'session': 'f8677049-449b-4371-ac7d-878e11fe03ff', 'username': '751b3616-df4d-43b3-b034-72e0f89e0c4a', 'version': '5.2'}, 'metadata': {'cellId': 'vscode-notebook-cell:/Users/annieulichney/Docume...eforestation/ResultsWithPolitical.ipynb#ch0000127'}, 'msg_id': '192988bd-1986-4d0d-a104-e363d1fccffd', 'msg_type': 'execute_request', 'parent_header': {}})\n    263             try:\n    264                 self.pre_handler_hook()\n    265             except Exception:\n    266                 self.log.debug(\"Unable to signal in pre_handler_hook:\", exc_info=True)\n    267             try:\n--> 268                 yield gen.maybe_future(handler(stream, idents, msg))\n        handler = <bound method Kernel.execute_request of <ipykernel.ipkernel.IPythonKernel object>>\n        stream = <zmq.eventloop.zmqstream.ZMQStream object>\n        idents = [b'cf5dc902-7071-485f-a33a-9ea2e7aada4b']\n        msg = {'buffers': [], 'content': {'allow_stdin': True, 'code': 'from mlens.ensemble import SuperLearner\\nfrom skl...rain)\\n\\n# Predict\\npreds = ensemble.predict(X_test)', 'silent': False, 'stop_on_error': False, 'store_history': True, 'user_expressions': {}}, 'header': {'date': datetime.datetime(2023, 5, 9, 0, 23, 7, 439000, tzinfo=tzutc()), 'msg_id': '192988bd-1986-4d0d-a104-e363d1fccffd', 'msg_type': 'execute_request', 'session': 'f8677049-449b-4371-ac7d-878e11fe03ff', 'username': '751b3616-df4d-43b3-b034-72e0f89e0c4a', 'version': '5.2'}, 'metadata': {'cellId': 'vscode-notebook-cell:/Users/annieulichney/Docume...eforestation/ResultsWithPolitical.ipynb#ch0000127'}, 'msg_id': '192988bd-1986-4d0d-a104-e363d1fccffd', 'msg_type': 'execute_request', 'parent_header': {}}\n    269             except Exception:\n    270                 self.log.error(\"Exception in message handler:\", exc_info=True)\n    271             finally:\n    272                 try:\n\n...........................................................................\n/Users/annieulichney/opt/anaconda3/lib/python3.8/site-packages/tornado/gen.py in wrapper(*args=(<ipykernel.ipkernel.IPythonKernel object>, <zmq.eventloop.zmqstream.ZMQStream object>, [b'cf5dc902-7071-485f-a33a-9ea2e7aada4b'], {'buffers': [], 'content': {'allow_stdin': True, 'code': 'from mlens.ensemble import SuperLearner\\nfrom skl...rain)\\n\\n# Predict\\npreds = ensemble.predict(X_test)', 'silent': False, 'stop_on_error': False, 'store_history': True, 'user_expressions': {}}, 'header': {'date': datetime.datetime(2023, 5, 9, 0, 23, 7, 439000, tzinfo=tzutc()), 'msg_id': '192988bd-1986-4d0d-a104-e363d1fccffd', 'msg_type': 'execute_request', 'session': 'f8677049-449b-4371-ac7d-878e11fe03ff', 'username': '751b3616-df4d-43b3-b034-72e0f89e0c4a', 'version': '5.2'}, 'metadata': {'cellId': 'vscode-notebook-cell:/Users/annieulichney/Docume...eforestation/ResultsWithPolitical.ipynb#ch0000127'}, 'msg_id': '192988bd-1986-4d0d-a104-e363d1fccffd', 'msg_type': 'execute_request', 'parent_header': {}}), **kwargs={})\n    229                 # avoid the cost of creating a Runner when the coroutine\n    230                 # never actually yields, which in turn allows us to\n    231                 # use \"optional\" coroutines in critical path code without\n    232                 # performance penalty for the synchronous case.\n    233                 try:\n--> 234                     yielded = ctx_run(next, result)\n        yielded = undefined\n        ctx_run = <built-in method run of Context object>\n        result = <generator object Kernel.execute_request>\n    235                 except (StopIteration, Return) as e:\n    236                     future_set_result_unless_cancelled(\n    237                         future, _value_from_stopiteration(e)\n    238                     )\n\n...........................................................................\n/Users/annieulichney/opt/anaconda3/lib/python3.8/site-packages/ipykernel/kernelbase.py in execute_request(self=<ipykernel.ipkernel.IPythonKernel object>, stream=<zmq.eventloop.zmqstream.ZMQStream object>, ident=[b'cf5dc902-7071-485f-a33a-9ea2e7aada4b'], parent={'buffers': [], 'content': {'allow_stdin': True, 'code': 'from mlens.ensemble import SuperLearner\\nfrom skl...rain)\\n\\n# Predict\\npreds = ensemble.predict(X_test)', 'silent': False, 'stop_on_error': False, 'store_history': True, 'user_expressions': {}}, 'header': {'date': datetime.datetime(2023, 5, 9, 0, 23, 7, 439000, tzinfo=tzutc()), 'msg_id': '192988bd-1986-4d0d-a104-e363d1fccffd', 'msg_type': 'execute_request', 'session': 'f8677049-449b-4371-ac7d-878e11fe03ff', 'username': '751b3616-df4d-43b3-b034-72e0f89e0c4a', 'version': '5.2'}, 'metadata': {'cellId': 'vscode-notebook-cell:/Users/annieulichney/Docume...eforestation/ResultsWithPolitical.ipynb#ch0000127'}, 'msg_id': '192988bd-1986-4d0d-a104-e363d1fccffd', 'msg_type': 'execute_request', 'parent_header': {}})\n    538         if not silent:\n    539             self.execution_count += 1\n    540             self._publish_execute_input(code, parent, self.execution_count)\n    541 \n    542         reply_content = yield gen.maybe_future(\n--> 543             self.do_execute(\n        self.do_execute = <bound method IPythonKernel.do_execute of <ipykernel.ipkernel.IPythonKernel object>>\n        code = 'from mlens.ensemble import SuperLearner\\nfrom skl...rain)\\n\\n# Predict\\npreds = ensemble.predict(X_test)'\n        silent = False\n        store_history = True\n        user_expressions = {}\n        allow_stdin = True\n    544                 code, silent, store_history,\n    545                 user_expressions, allow_stdin,\n    546             )\n    547         )\n\n...........................................................................\n/Users/annieulichney/opt/anaconda3/lib/python3.8/site-packages/tornado/gen.py in wrapper(*args=(<ipykernel.ipkernel.IPythonKernel object>, 'from mlens.ensemble import SuperLearner\\nfrom skl...rain)\\n\\n# Predict\\npreds = ensemble.predict(X_test)', False, True, {}, True), **kwargs={})\n    229                 # avoid the cost of creating a Runner when the coroutine\n    230                 # never actually yields, which in turn allows us to\n    231                 # use \"optional\" coroutines in critical path code without\n    232                 # performance penalty for the synchronous case.\n    233                 try:\n--> 234                     yielded = ctx_run(next, result)\n        yielded = undefined\n        ctx_run = <built-in method run of Context object>\n        result = <generator object IPythonKernel.do_execute>\n    235                 except (StopIteration, Return) as e:\n    236                     future_set_result_unless_cancelled(\n    237                         future, _value_from_stopiteration(e)\n    238                     )\n\n...........................................................................\n/Users/annieulichney/opt/anaconda3/lib/python3.8/site-packages/ipykernel/ipkernel.py in do_execute(self=<ipykernel.ipkernel.IPythonKernel object>, code='from mlens.ensemble import SuperLearner\\nfrom skl...rain)\\n\\n# Predict\\npreds = ensemble.predict(X_test)', silent=False, store_history=True, user_expressions={}, allow_stdin=True)\n    301                             shell.events.trigger('post_run_cell', res)\n    302             else:\n    303                 # runner isn't already running,\n    304                 # make synchronous call,\n    305                 # letting shell dispatch to loop runners\n--> 306                 res = shell.run_cell(code, store_history=store_history, silent=silent)\n        res = undefined\n        code = 'from mlens.ensemble import SuperLearner\\nfrom skl...rain)\\n\\n# Predict\\npreds = ensemble.predict(X_test)'\n        store_history = True\n        silent = False\n    307         finally:\n    308             self._restore_input()\n    309 \n    310         if res.error_before_exec is not None:\n\n...........................................................................\n/Users/annieulichney/opt/anaconda3/lib/python3.8/site-packages/ipykernel/zmqshell.py in run_cell(self=<ipykernel.zmqshell.ZMQInteractiveShell object>, *args=('from mlens.ensemble import SuperLearner\\nfrom skl...rain)\\n\\n# Predict\\npreds = ensemble.predict(X_test)',), **kwargs={'silent': False, 'store_history': True})\n    531             )\n    532         self.payload_manager.write_payload(payload)\n    533 \n    534     def run_cell(self, *args, **kwargs):\n    535         self._last_traceback = None\n--> 536         return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n        self.run_cell = <bound method ZMQInteractiveShell.run_cell of <ipykernel.zmqshell.ZMQInteractiveShell object>>\n        args = ('from mlens.ensemble import SuperLearner\\nfrom skl...rain)\\n\\n# Predict\\npreds = ensemble.predict(X_test)',)\n        kwargs = {'silent': False, 'store_history': True}\n    537 \n    538     def _showtraceback(self, etype, evalue, stb):\n    539         # try to preserve ordering of tracebacks and print statements\n    540         sys.stdout.flush()\n\n...........................................................................\n/Users/annieulichney/opt/anaconda3/lib/python3.8/site-packages/IPython/core/interactiveshell.py in run_cell(self=<ipykernel.zmqshell.ZMQInteractiveShell object>, raw_cell='from mlens.ensemble import SuperLearner\\nfrom skl...rain)\\n\\n# Predict\\npreds = ensemble.predict(X_test)', store_history=True, silent=False, shell_futures=True)\n   2842         -------\n   2843         result : :class:`ExecutionResult`\n   2844         \"\"\"\n   2845         result = None\n   2846         try:\n-> 2847             result = self._run_cell(\n        result = None\n        self._run_cell = <bound method InteractiveShell._run_cell of <ipykernel.zmqshell.ZMQInteractiveShell object>>\n        raw_cell = 'from mlens.ensemble import SuperLearner\\nfrom skl...rain)\\n\\n# Predict\\npreds = ensemble.predict(X_test)'\n        store_history = True\n        silent = False\n        shell_futures = True\n   2848                 raw_cell, store_history, silent, shell_futures)\n   2849         finally:\n   2850             self.events.trigger('post_execute')\n   2851             if not silent:\n\n...........................................................................\n/Users/annieulichney/opt/anaconda3/lib/python3.8/site-packages/IPython/core/interactiveshell.py in _run_cell(self=<ipykernel.zmqshell.ZMQInteractiveShell object>, raw_cell='from mlens.ensemble import SuperLearner\\nfrom skl...rain)\\n\\n# Predict\\npreds = ensemble.predict(X_test)', store_history=True, silent=False, shell_futures=True)\n   2869             runner = self.loop_runner\n   2870         else:\n   2871             runner = _pseudo_sync_runner\n   2872 \n   2873         try:\n-> 2874             return runner(coro)\n        runner = <function _pseudo_sync_runner>\n        coro = <coroutine object InteractiveShell.run_cell_async>\n   2875         except BaseException as e:\n   2876             info = ExecutionInfo(raw_cell, store_history, silent, shell_futures)\n   2877             result = ExecutionResult(info)\n   2878             result.error_in_exec = e\n\n...........................................................................\n/Users/annieulichney/opt/anaconda3/lib/python3.8/site-packages/IPython/core/async_helpers.py in _pseudo_sync_runner(coro=<coroutine object InteractiveShell.run_cell_async>)\n     63 \n     64     Credit to Nathaniel Smith\n     65 \n     66     \"\"\"\n     67     try:\n---> 68         coro.send(None)\n        coro.send = <built-in method send of coroutine object>\n     69     except StopIteration as exc:\n     70         return exc.value\n     71     else:\n     72         # TODO: do not raise but return an execution result with the right info.\n\n...........................................................................\n/Users/annieulichney/opt/anaconda3/lib/python3.8/site-packages/IPython/core/interactiveshell.py in run_cell_async(self=<ipykernel.zmqshell.ZMQInteractiveShell object>, raw_cell='from mlens.ensemble import SuperLearner\\nfrom skl...rain)\\n\\n# Predict\\npreds = ensemble.predict(X_test)', store_history=True, silent=False, shell_futures=True)\n   3045                 # Execute the user code\n   3046                 interactivity = \"none\" if silent else self.ast_node_interactivity\n   3047                 if _run_async:\n   3048                     interactivity = 'async'\n   3049 \n-> 3050                 has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n        has_raised = undefined\n        self.run_ast_nodes = <bound method InteractiveShell.run_ast_nodes of <ipykernel.zmqshell.ZMQInteractiveShell object>>\n        code_ast.body = [<_ast.ImportFrom object>, <_ast.ImportFrom object>, <_ast.ImportFrom object>, <_ast.ImportFrom object>, <_ast.Assign object>, <_ast.Expr object>, <_ast.Expr object>, <_ast.Expr object>, <_ast.Assign object>]\n        cell_name = '<ipython-input-300-2a626d4720dc>'\n        interactivity = 'last_expr'\n        compiler = <IPython.core.compilerop.CachingCompiler object>\n   3051                        interactivity=interactivity, compiler=compiler, result=result)\n   3052 \n   3053                 self.last_execution_succeeded = not has_raised\n   3054                 self.last_execution_result = result\n\n...........................................................................\n/Users/annieulichney/opt/anaconda3/lib/python3.8/site-packages/IPython/core/interactiveshell.py in run_ast_nodes(self=<ipykernel.zmqshell.ZMQInteractiveShell object>, nodelist=[<_ast.ImportFrom object>, <_ast.ImportFrom object>, <_ast.ImportFrom object>, <_ast.ImportFrom object>, <_ast.Assign object>, <_ast.Expr object>, <_ast.Expr object>, <_ast.Expr object>, <_ast.Assign object>], cell_name='<ipython-input-300-2a626d4720dc>', interactivity='none', compiler=<IPython.core.compilerop.CachingCompiler object>, result=<ExecutionResult object at 7fe6603a1430, executi...rue silent=False shell_futures=True> result=None>)\n   3237                     elif mode == 'single':\n   3238                         mod = ast.Interactive([node])\n   3239                     with compiler.extra_flags(getattr(ast, 'PyCF_ALLOW_TOP_LEVEL_AWAIT', 0x0) if self.autoawait else 0x0):\n   3240                         code = compiler(mod, cell_name, mode)\n   3241                         asy = compare(code)\n-> 3242                     if (await self.run_code(code, result,  async_=asy)):\n        self.run_code = <bound method InteractiveShell.run_code of <ipykernel.zmqshell.ZMQInteractiveShell object>>\n        code = <code object <module> at 0x7fe6581d5030, file \"<ipython-input-300-2a626d4720dc>\", line 20>\n        result = <ExecutionResult object at 7fe6603a1430, executi...rue silent=False shell_futures=True> result=None>\n        asy = False\n   3243                         return True\n   3244 \n   3245             # Flush softspace\n   3246             if softspace(sys.stdout, 0):\n\n...........................................................................\n/Users/annieulichney/opt/anaconda3/lib/python3.8/site-packages/IPython/core/interactiveshell.py in run_code(self=<ipykernel.zmqshell.ZMQInteractiveShell object>, code_obj=<code object <module> at 0x7fe6581d5030, file \"<ipython-input-300-2a626d4720dc>\", line 20>, result=<ExecutionResult object at 7fe6603a1430, executi...rue silent=False shell_futures=True> result=None>, async_=False)\n   3314                     code = compile('last_expr', 'fake', \"single\")\n   3315                     exec(code, {'last_expr': last_expr})\n   3316                 elif async_ :\n   3317                     await eval(code_obj, self.user_global_ns, self.user_ns)\n   3318                 else:\n-> 3319                     exec(code_obj, self.user_global_ns, self.user_ns)\n        code_obj = <code object <module> at 0x7fe6581d5030, file \"<ipython-input-300-2a626d4720dc>\", line 20>\n        self.user_global_ns = {'AdaBoostClassifier': <class 'sklearn.ensemble._weight_boosting.AdaBoostClassifier'>, 'BaggingClassifier': <class 'sklearn.ensemble._bagging.BaggingClassifier'>, 'DecisionTreeClassifier': <class 'sklearn.tree._classes.DecisionTreeClassifier'>, 'ExtraTreesClassifier': <class 'sklearn.ensemble._forest.ExtraTreesClassifier'>, 'GaussianNB': <class 'sklearn.naive_bayes.GaussianNB'>, 'GradientBoostingRegressor': <class 'sklearn.ensemble._gb.GradientBoostingRegressor'>, 'GridSearchCV': <class 'sklearn.model_selection._search.GridSearchCV'>, 'GroupKFold': <class 'sklearn.model_selection._split.GroupKFold'>, 'In': ['', 'import pandas as pd\\nimport numpy as np\\nimport py...the pyshp package\\nimport matplotlib.pyplot as plt', \"df_full = pd.read_csv('/Users/annieulichney/Docu...itHub/Deforestation/FinalData/FinalData2004.csv')\", 'for year in [ 2005, 2006, 2007, 2008, 2009, 2010...ull = pd.concat([df_full, pd.read_csv(filename)])', 'def drop_conditional(cols_list, df):\\n    for col...        df = df.drop(col, axis = 1)\\n    return df', \"#df_full = drop_conditional(['Unnamed: 0', 'ID', ], df_full)\", 'df_full.shape', 'df_full.ID', '#should year be in it? ', \"X_cols  = ['year', 'rain1', 'elevation', 'slope'...erup_votes_proportion', \\n#'overall_winner_idade',\", 'from sklearn.model_selection import GroupKFold, cross_val_predict\\n\\nimport geopandas as gpd', 'df_full.shape[0]', \"df_full = df_full.sample(10000).reset_index(drop...xy(df_full.x, df_full.y))\\n# XYs = gdf['geometry']\", '#Select Test/Train Indices\\nn_folds = 10 \\nmunis =...n(test_inds)/(len(train_inds) + len(test_inds)) )', \"#partition data into test/train sets\\n\\ndf_full_te...est['geometry']\\nXYs_train = gdf_train['geometry']\", 'PLOT = True\\n\\nif PLOT:\\n    fig, axs = plt.subplot...rscale=100)\\n    plt.tight_layout()\\n    plt.show()', '#Select Cross Validation Fold Indices: \\n\\nn_folds...ld)]\\nmuni_cv = [*zip(train_indices,test_indices)]', 'PLOT_FOLDS = False\\nif PLOT_FOLDS: \\n    fig, axs ...rscale=100)\\n    plt.tight_layout()\\n    plt.show()', 'X_train.head()', '# Count null values in each column\\nnull_counts =...tem[1], reverse=True))\\n\\nprint(sorted_null_counts)', ...], 'KFold': <class 'sklearn.model_selection._split.KFold'>, ...}\n        self.user_ns = {'AdaBoostClassifier': <class 'sklearn.ensemble._weight_boosting.AdaBoostClassifier'>, 'BaggingClassifier': <class 'sklearn.ensemble._bagging.BaggingClassifier'>, 'DecisionTreeClassifier': <class 'sklearn.tree._classes.DecisionTreeClassifier'>, 'ExtraTreesClassifier': <class 'sklearn.ensemble._forest.ExtraTreesClassifier'>, 'GaussianNB': <class 'sklearn.naive_bayes.GaussianNB'>, 'GradientBoostingRegressor': <class 'sklearn.ensemble._gb.GradientBoostingRegressor'>, 'GridSearchCV': <class 'sklearn.model_selection._search.GridSearchCV'>, 'GroupKFold': <class 'sklearn.model_selection._split.GroupKFold'>, 'In': ['', 'import pandas as pd\\nimport numpy as np\\nimport py...the pyshp package\\nimport matplotlib.pyplot as plt', \"df_full = pd.read_csv('/Users/annieulichney/Docu...itHub/Deforestation/FinalData/FinalData2004.csv')\", 'for year in [ 2005, 2006, 2007, 2008, 2009, 2010...ull = pd.concat([df_full, pd.read_csv(filename)])', 'def drop_conditional(cols_list, df):\\n    for col...        df = df.drop(col, axis = 1)\\n    return df', \"#df_full = drop_conditional(['Unnamed: 0', 'ID', ], df_full)\", 'df_full.shape', 'df_full.ID', '#should year be in it? ', \"X_cols  = ['year', 'rain1', 'elevation', 'slope'...erup_votes_proportion', \\n#'overall_winner_idade',\", 'from sklearn.model_selection import GroupKFold, cross_val_predict\\n\\nimport geopandas as gpd', 'df_full.shape[0]', \"df_full = df_full.sample(10000).reset_index(drop...xy(df_full.x, df_full.y))\\n# XYs = gdf['geometry']\", '#Select Test/Train Indices\\nn_folds = 10 \\nmunis =...n(test_inds)/(len(train_inds) + len(test_inds)) )', \"#partition data into test/train sets\\n\\ndf_full_te...est['geometry']\\nXYs_train = gdf_train['geometry']\", 'PLOT = True\\n\\nif PLOT:\\n    fig, axs = plt.subplot...rscale=100)\\n    plt.tight_layout()\\n    plt.show()', '#Select Cross Validation Fold Indices: \\n\\nn_folds...ld)]\\nmuni_cv = [*zip(train_indices,test_indices)]', 'PLOT_FOLDS = False\\nif PLOT_FOLDS: \\n    fig, axs ...rscale=100)\\n    plt.tight_layout()\\n    plt.show()', 'X_train.head()', '# Count null values in each column\\nnull_counts =...tem[1], reverse=True))\\n\\nprint(sorted_null_counts)', ...], 'KFold': <class 'sklearn.model_selection._split.KFold'>, ...}\n   3320             finally:\n   3321                 # Reset our crash handler in place\n   3322                 sys.excepthook = old_excepthook\n   3323         except SystemExit as e:\n\n...........................................................................\n/Users/annieulichney/Documents/GitHub/Deforestation/<ipython-input-300-2a626d4720dc> in <module>()\n     15 ensemble.add_meta(LogisticRegression())\n     16 \n     17 # --- Use ---\n     18 \n     19 # Fit ensemble\n---> 20 ensemble.fit(X_train, Y_train)\n     21 \n     22 # Predict\n     23 preds = ensemble.predict(X_test)\n\n...........................................................................\n/Users/annieulichney/opt/anaconda3/lib/python3.8/site-packages/mlens/ensemble/base.py in fit(self=SuperLearner(array_check=None, backend=None, fol...0x7fe65f987040>, shuffle=False,\n       verbose=2), X=    year     rain1  elevation  slope  aspect  ne...03398  \n69    0.000000  \n\n[70 rows x 204 columns], y=0     0.006777\n1     0.000000\n2     0.003566\n3  ...192\nName: forest_diff, Length: 70, dtype: float64, **kwargs={})\n    509             return self\n    510 \n    511         if self.model_selection:\n    512             self._id_train.fit(X)\n    513 \n--> 514         out = self._backend.fit(X, y, **kwargs)\n        out = undefined\n        self._backend.fit = <bound method Sequential.fit of Sequential(backe...nsformers=[])],\n   verbose=1)],\n      verbose=2)>\n        X =     year     rain1  elevation  slope  aspect  ne...03398  \n69    0.000000  \n\n[70 rows x 204 columns]\n        y = 0     0.006777\n1     0.000000\n2     0.003566\n3  ...192\nName: forest_diff, Length: 70, dtype: float64\n        kwargs = {}\n    515         if out is not self._backend:\n    516             # fit_transform\n    517             return out\n    518         else:\n\n...........................................................................\n/Users/annieulichney/opt/anaconda3/lib/python3.8/site-packages/mlens/ensemble/base.py in fit(self=Sequential(backend='threading', dtype=<class 'nu...ansformers=[])],\n   verbose=1)],\n      verbose=2), X=    year     rain1  elevation  slope  aspect  ne...03398  \n69    0.000000  \n\n[70 rows x 204 columns], y=0     0.006777\n1     0.000000\n2     0.003566\n3  ...192\nName: forest_diff, Length: 70, dtype: float64, **kwargs={})\n    153 \n    154         f, t0 = print_job(self, \"Fitting\")\n    155 \n    156         with ParallelProcessing(self.backend, self.n_jobs,\n    157                                 max(self.verbose - 4, 0)) as manager:\n--> 158             out = manager.stack(self, 'fit', X, y, **kwargs)\n        out = undefined\n        manager.stack = <bound method ParallelProcessing.stack of <mlens.parallel.backend.ParallelProcessing object>>\n        self = Sequential(backend='threading', dtype=<class 'nu...ansformers=[])],\n   verbose=1)],\n      verbose=2)\n        X =     year     rain1  elevation  slope  aspect  ne...03398  \n69    0.000000  \n\n[70 rows x 204 columns]\n        y = 0     0.006777\n1     0.000000\n2     0.003566\n3  ...192\nName: forest_diff, Length: 70, dtype: float64\n        kwargs = {}\n    159 \n    160         if self.verbose:\n    161             print_time(t0, \"{:<35}\".format(\"Fit complete\"), file=f)\n    162 \n\n...........................................................................\n/Users/annieulichney/opt/anaconda3/lib/python3.8/site-packages/mlens/parallel/backend.py in stack(self=<mlens.parallel.backend.ParallelProcessing object>, caller=Sequential(backend='threading', dtype=<class 'nu...ansformers=[])],\n   verbose=1)],\n      verbose=2), job='fit', X=    year     rain1  elevation  slope  aspect  ne...03398  \n69    0.000000  \n\n[70 rows x 204 columns], y=0     0.006777\n1     0.000000\n2     0.003566\n3  ...192\nName: forest_diff, Length: 70, dtype: float64, path=None, return_preds=False, warm_start=False, split=True, **kwargs={})\n    668             Prediction array(s).\n    669         \"\"\"\n    670         out = self.initialize(\n    671             job=job, X=X, y=y, path=path, warm_start=warm_start,\n    672             return_preds=return_preds, split=split, stack=True)\n--> 673         return self.process(caller=caller, out=out, **kwargs)\n        self.process = <bound method ParallelProcessing.process of <mlens.parallel.backend.ParallelProcessing object>>\n        caller = Sequential(backend='threading', dtype=<class 'nu...ansformers=[])],\n   verbose=1)],\n      verbose=2)\n        out = {}\n        kwargs = {}\n    674 \n    675     def process(self, caller, out, **kwargs):\n    676         \"\"\"Process job.\n    677 \n\n...........................................................................\n/Users/annieulichney/opt/anaconda3/lib/python3.8/site-packages/mlens/parallel/backend.py in process(self=<mlens.parallel.backend.ParallelProcessing object>, caller=Sequential(backend='threading', dtype=<class 'nu...ansformers=[])],\n   verbose=1)],\n      verbose=2), out=None, **kwargs={})\n    713                       backend=self.backend) as parallel:\n    714 \n    715             for task in caller:\n    716                 self.job.clear()\n    717 \n--> 718                 self._partial_process(task, parallel, **kwargs)\n        self._partial_process = <bound method ParallelProcessing._partial_proces...lens.parallel.backend.ParallelProcessing object>>\n        task = Layer(backend='threading', dtype=<class 'numpy.f..._exception=True, transformers=[])],\n   verbose=1)\n        parallel = Parallel(n_jobs=-1)\n        kwargs = {}\n    719 \n    720                 if task.name in return_names:\n    721                     out.append(self.get_preds(dtype=_dtype(task)))\n    722 \n\n...........................................................................\n/Users/annieulichney/opt/anaconda3/lib/python3.8/site-packages/mlens/parallel/backend.py in _partial_process(self=<mlens.parallel.backend.ParallelProcessing object>, task=Layer(backend='threading', dtype=<class 'numpy.f..._exception=True, transformers=[])],\n   verbose=1), parallel=Parallel(n_jobs=-1), **kwargs={})\n    734         task.setup(self.job.predict_in, self.job.targets, self.job.job)\n    735 \n    736         if not task.__no_output__:\n    737             self._gen_prediction_array(task, self.job.job, self.__threading__)\n    738 \n--> 739         task(self.job.args(**kwargs), parallel=parallel)\n        task = Layer(backend='threading', dtype=<class 'numpy.f..._exception=True, transformers=[])],\n   verbose=1)\n        self.job.args = <bound method Job.args of <mlens.parallel.backend.Job object>>\n        kwargs = {}\n        parallel = Parallel(n_jobs=-1)\n    740 \n    741         if not task.__no_output__ and getattr(task, 'n_feature_prop', 0):\n    742             self._propagate_features(task)\n    743 \n\n...........................................................................\n/Users/annieulichney/opt/anaconda3/lib/python3.8/site-packages/mlens/parallel/layer.py in __call__(self=Layer(backend='threading', dtype=<class 'numpy.f..._exception=True, transformers=[])],\n   verbose=1), args={'auxiliary': {'P': None, 'X': array([[-3.36665614e-03, -6.37534400e-03, -5.410...51e-02,\n        -1.52171021e-02]], dtype=float32), 'y': 0     0.006777\n1     0.000000\n2     0.003566\n3  ...192\nName: forest_diff, Length: 70, dtype: float64}, 'dir': [], 'job': 'fit', 'main': {'P': array([[0.],\n       [0.],\n       [0.],\n       [0...  [0.],\n       [0.],\n       [0.]], dtype=float32), 'X': array([[-3.36665614e-03, -6.37534400e-03, -5.410...51e-02,\n        -1.52171021e-02]], dtype=float32), 'y': 0     0.006777\n1     0.000000\n2     0.003566\n3  ...192\nName: forest_diff, Length: 70, dtype: float64}}, parallel=Parallel(n_jobs=-1))\n    146 \n    147         if self.verbose >= 2:\n    148             safe_print(msg.format('Learners ...'), file=f, end=e2)\n    149             t1 = time()\n    150 \n--> 151         parallel(delayed(sublearner, not _threading)()\n        parallel = Parallel(n_jobs=-1)\n        self.learners = [Learner(attr='predict', backend='threading', dty...orer=<function accuracy_score at 0x7fe65f987040>)]\n        args = {'auxiliary': {'P': None, 'X': array([[-3.36665614e-03, -6.37534400e-03, -5.410...51e-02,\n        -1.52171021e-02]], dtype=float32), 'y': 0     0.006777\n1     0.000000\n2     0.003566\n3  ...192\nName: forest_diff, Length: 70, dtype: float64}, 'dir': [], 'job': 'fit', 'main': {'P': array([[0.],\n       [0.],\n       [0.],\n       [0...  [0.],\n       [0.],\n       [0.]], dtype=float32), 'X': array([[-3.36665614e-03, -6.37534400e-03, -5.410...51e-02,\n        -1.52171021e-02]], dtype=float32), 'y': 0     0.006777\n1     0.000000\n2     0.003566\n3  ...192\nName: forest_diff, Length: 70, dtype: float64}}\n    152                  for learner in self.learners\n    153                  for sublearner in learner(args, 'main'))\n    154 \n    155         if self.verbose >= 2:\n\n...........................................................................\n/Users/annieulichney/opt/anaconda3/lib/python3.8/site-packages/mlens/externals/joblib/parallel.py in __call__(self=Parallel(n_jobs=-1), iterable=<generator object Layer.__call__.<locals>.<genexpr>>)\n    788             if pre_dispatch == \"all\" or n_jobs == 1:\n    789                 # The iterable was consumed all at once by the above for loop.\n    790                 # No need to wait for async callbacks to trigger to\n    791                 # consumption.\n    792                 self._iterating = False\n--> 793             self.retrieve()\n        self.retrieve = <bound method Parallel.retrieve of Parallel(n_jobs=-1)>\n    794             # Make sure that we get a last message telling us we are done\n    795             elapsed_time = time.time() - self._start_time\n    796             self._print('Done %3i out of %3i | elapsed: %s finished',\n    797                         (len(self._output), len(self._output),\n\n---------------------------------------------------------------------------\nSub-process traceback:\n---------------------------------------------------------------------------\nValueError                                         Mon May  8 20:23:14 2023\nPID: 25543      Python 3.8.8: /Users/annieulichney/opt/anaconda3/bin/python\n...........................................................................\n/Users/annieulichney/opt/anaconda3/lib/python3.8/site-packages/mlens/externals/joblib/parallel.py in __call__(self=<mlens.externals.joblib.parallel.BatchedCalls object>)\n    130     def __init__(self, iterator_slice):\n    131         self.items = list(iterator_slice)\n    132         self._size = len(self.items)\n    133 \n    134     def __call__(self):\n--> 135         return [func(*args, **kwargs) for func, args, kwargs in self.items]\n        self.items = [(<mlens.parallel.learner.SubLearner object>, (), {})]\n    136 \n    137     def __len__(self):\n    138         return self._size\n    139 \n\n...........................................................................\n/Users/annieulichney/opt/anaconda3/lib/python3.8/site-packages/mlens/externals/joblib/parallel.py in <listcomp>(.0=<list_iterator object>)\n    130     def __init__(self, iterator_slice):\n    131         self.items = list(iterator_slice)\n    132         self._size = len(self.items)\n    133 \n    134     def __call__(self):\n--> 135         return [func(*args, **kwargs) for func, args, kwargs in self.items]\n        func = <mlens.parallel.learner.SubLearner object>\n        args = ()\n        kwargs = {}\n    136 \n    137     def __len__(self):\n    138         return self._size\n    139 \n\n...........................................................................\n/Users/annieulichney/opt/anaconda3/lib/python3.8/site-packages/mlens/parallel/learner.py in __call__(self=<mlens.parallel.learner.SubLearner object>)\n    119         else:\n    120             self.processing_index = ''\n    121 \n    122     def __call__(self):\n    123         \"\"\"Launch job\"\"\"\n--> 124         return getattr(self, self.job)()\n        self = <mlens.parallel.learner.SubLearner object>\n        self.job = 'fit'\n    125 \n    126     def fit(self, path=None):\n    127         \"\"\"Fit sub-learner\"\"\"\n    128         if path is None:\n\n...........................................................................\n/Users/annieulichney/opt/anaconda3/lib/python3.8/site-packages/mlens/parallel/learner.py in fit(self=<mlens.parallel.learner.SubLearner object>, path=[])\n    128         if path is None:\n    129             path = self.path\n    130         t0 = time()\n    131         transformers = self._load_preprocess(path)\n    132 \n--> 133         self._fit(transformers)\n        self._fit = <bound method SubLearner._fit of <mlens.parallel.learner.SubLearner object>>\n        transformers = None\n    134 \n    135         if self.out_array is not None:\n    136             self._predict(transformers, self.scorer is not None)\n    137 \n\n...........................................................................\n/Users/annieulichney/opt/anaconda3/lib/python3.8/site-packages/mlens/parallel/learner.py in _fit(self=<mlens.parallel.learner.SubLearner object>, transformers=None)\n    174         t0 = time()\n    175         if transformers:\n    176             xtemp, ytemp = transformers.transform(xtemp, ytemp)\n    177 \n    178         # Fit estimator\n--> 179         self.estimator.fit(xtemp, ytemp)\n        self.estimator.fit = <bound method LogisticRegression.fit of LogisticRegression()>\n        xtemp = array([[-3.36665614e-03, -6.37534400e-03, -5.410...51e-02,\n        -1.52171021e-02]], dtype=float32)\n        ytemp = 0     0.006777\n1     0.000000\n2     0.003566\n3  ...192\nName: forest_diff, Length: 70, dtype: float64\n    180         self.fit_time_ = time() - t0\n    181 \n    182     def _load_preprocess(self, path):\n    183         \"\"\"Load preprocessing pipeline\"\"\"\n\n...........................................................................\n/Users/annieulichney/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py in fit(self=LogisticRegression(), X=array([[-3.36665614e-03, -6.37534400e-03, -5.410...e-02,  3.90149951e-02,\n        -1.52171021e-02]]), y=array([ 0.00677705,  0.        ,  0.00356555, -0...00170457, -0.01167369,  0.0016818 ,  0.00519219]), sample_weight=None)\n   1141             accept_sparse=\"csr\",\n   1142             dtype=_dtype,\n   1143             order=\"C\",\n   1144             accept_large_sparse=solver not in [\"liblinear\", \"sag\", \"saga\"],\n   1145         )\n-> 1146         check_classification_targets(y)\n        y = array([ 0.00677705,  0.        ,  0.00356555, -0...00170457, -0.01167369,  0.0016818 ,  0.00519219])\n   1147         self.classes_ = np.unique(y)\n   1148 \n   1149         multi_class = _check_multi_class(self.multi_class, solver, len(self.classes_))\n   1150 \n\n...........................................................................\n/Users/annieulichney/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/multiclass.py in check_classification_targets(y=array([ 0.00677705,  0.        ,  0.00356555, -0...00170457, -0.01167369,  0.0016818 ,  0.00519219]))\n    195         \"multiclass\",\n    196         \"multiclass-multioutput\",\n    197         \"multilabel-indicator\",\n    198         \"multilabel-sequences\",\n    199     ]:\n--> 200         raise ValueError(\"Unknown label type: %r\" % y_type)\n        y_type = 'continuous'\n    201 \n    202 \n    203 def type_of_target(y, input_name=\"\"):\n    204     \"\"\"Determine the type of data indicated by the target.\n\nValueError: Unknown label type: 'continuous'\n___________________________________________________________________________"
     ]
    }
   ],
   "source": [
    "from mlens.ensemble import SuperLearner\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# --- Build ---\n",
    "# Passing a scoring function will create cv scores during fitting\n",
    "# the scorer should be a simple function accepting to vectors and returning a scalar\n",
    "ensemble = SuperLearner(scorer=accuracy_score, random_state=42, verbose=2)\n",
    "\n",
    "# Build the first layer\n",
    "ensemble.add(base_models)\n",
    "\n",
    "# Attach the final meta estimator\n",
    "ensemble.add_meta(LogisticRegression())\n",
    "\n",
    "# --- Use ---\n",
    "\n",
    "# Fit ensemble\n",
    "ensemble.fit(X_train, Y_train)\n",
    "\n",
    "# Predict\n",
    "preds = ensemble.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_models():\n",
    "    \"\"\"Generate a library of base learners.\"\"\"\n",
    "    nn = base_learners[3][1][1]\n",
    "    gb = base_learners[2][1]\n",
    "    la = base_learners[1][1][1]\n",
    "    rf = base_learners[0][1][1]\n",
    "\n",
    "    models = {\n",
    "              'nn': nn,\n",
    "              'random forest': rf,\n",
    "              'gbm': gb,\n",
    "              'lasso': la,\n",
    "              }\n",
    "\n",
    "    return models\n",
    "\n",
    "def train_predict(model_list):\n",
    "    \"\"\"Fit models in list on training set and return preds\"\"\"\n",
    "    P = np.zeros((Y_test.shape[0], len(model_list)))\n",
    "    P = pd.DataFrame(P)\n",
    "\n",
    "    print(\"Fitting models.\")\n",
    "    cols = list()\n",
    "    for i, (name, m) in enumerate(models.items()):\n",
    "        print(\"%s...\" % name, end=\" \", flush=False)\n",
    "        m.fit(X_train, Y_train)\n",
    "        P[i] = m.predict(X_test)\n",
    "        cols.append(name)\n",
    "        print(\"done\")\n",
    "\n",
    "    P.columns = cols\n",
    "    print(\"Done.\\n\")\n",
    "    return P\n",
    "\n",
    "\n",
    "def score_models(P, y):\n",
    "    \"\"\"Score model in prediction DF\"\"\"\n",
    "    print(\"Scoring models.\")\n",
    "    for m in P.columns:\n",
    "        #score = roc_auc_score(y, P[m])\n",
    "        score = accuracy_score(y, P[m])\n",
    "        print(\"%-26s: %.3f\" % (m, score))\n",
    "    print(\"Done.\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting models.\n",
      "nn... done\n",
      "random forest... done\n",
      "gbm... done\n",
      "lasso... done\n",
      "Done.\n",
      "\n",
      "Scoring models.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "continuous is not supported",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-298-b8e463e716c3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mP\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_predict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mscore_models\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mP\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-297-4ef6e991c318>\u001b[0m in \u001b[0;36mscore_models\u001b[0;34m(P, y)\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mm\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mP\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0;31m#score = roc_auc_score(y, P[m])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m         \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mP\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"%-26s: %.3f\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscore\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Done.\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py\u001b[0m in \u001b[0;36maccuracy_score\u001b[0;34m(y_true, y_pred, normalize, sample_weight)\u001b[0m\n\u001b[1;32m    209\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m     \u001b[0;31m# Compute accuracy for each possible representation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m     \u001b[0my_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_targets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0my_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"multilabel\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py\u001b[0m in \u001b[0;36m_check_targets\u001b[0;34m(y_true, y_pred)\u001b[0m\n\u001b[1;32m    102\u001b[0m     \u001b[0;31m# No metrics support \"multiclass-multioutput\" format\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0my_type\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"binary\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"multiclass\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"multilabel-indicator\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 104\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"{0} is not supported\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0my_type\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"binary\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"multiclass\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: continuous is not supported"
     ]
    }
   ],
   "source": [
    "models = get_models()\n",
    "\n",
    "P = train_predict(models)\n",
    "score_models(P, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlens.ensemble import SuperLearner\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_models = []\n",
    "\n",
    "#Random forest regressor\n",
    "base_models.append(base_learners[0][1][1])\n",
    "\n",
    "#Lasso\n",
    "base_models.append(base_learners[1][1][1])\n",
    "\n",
    "#Gradient Boosting\n",
    "base_models.append(base_learners[2][1])\n",
    "\n",
    "#NeuralNetwork\n",
    "base_models.append(base_learners[3][1][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_models():\n",
    "\tmodels = list()\n",
    "\tmodels.append(base_learners[0][1][1])\n",
    "\tmodels.append(base_learners[1][1][1])\n",
    "\tmodels.append(base_learners[2][1])\n",
    "\tmodels.append(base_learners[3][1][1])\n",
    "\treturn models\n",
    "\n",
    "def super_learner_predictions(X, models, meta_model):\n",
    "\tmeta_X = list()\n",
    "\tfor model in models:\n",
    "\t\tyhat = model.predict_proba(X) \n",
    "\t\tmeta_X.append(yhat)\n",
    "\tmeta_X = np.hstack(meta_X)\n",
    "\t# predict\n",
    "\treturn meta_model.predict(meta_X)\n",
    "\n",
    "def fit_base_models(X, y, models):\n",
    "\tfor model in models:\n",
    "\t\tmodel.fit(X, y)\n",
    "\n",
    "def fit_meta_model(X, y):\n",
    "\tmodel = LogisticRegression()\n",
    "\tmodel.fit(X, y)\n",
    "\treturn model\n",
    "\n",
    "def get_out_of_fold_predictions(X, y, models, muni_cv):\n",
    "\tmeta_X, meta_y = list(), list()\n",
    "\t\n",
    "\tfor train_ix, test_ix in muni_cv:\n",
    "\t\tfold_yhats = list()\n",
    "\t\t# get data\n",
    "\t\ttrain_X, test_X = X.iloc[train_ix], X.iloc[test_ix]\n",
    "\t\ttrain_y, test_y = y[train_ix], y[test_ix]\n",
    "\t\tmeta_y.extend(test_y)\n",
    "\t\t\n",
    "\t\t# fit and make predictions with each sub-model\n",
    "\t\tfor model in models:\n",
    "\t\t\tmodel.fit(train_X, train_y)\n",
    "\t\t\tyhat = model.predict(test_X)\n",
    "\t\t\t\n",
    "\t\t\tfold_yhats.append(yhat)\n",
    "\t\t\n",
    "\t\tmeta_X.append(np.hstack(fold_yhats))\n",
    "\treturn np.vstack(meta_X), np.asarray(meta_y)\n",
    "\n",
    "def evaluate_models(X, y, models):\n",
    "\tfor model in models:\n",
    "\t\tyhat = model.predict(X)\n",
    "\t\tacc = accuracy_score(y, yhat)\n",
    "\t\tprint('%s: %.3f' % (model.__class__.__name__, acc*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import hstack\n",
    "from numpy import vstack\n",
    "from numpy import asarray\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_models():\n",
    "\tmodels = list()\n",
    "\tmodels.append(LogisticRegression(solver='liblinear'))\n",
    "\tmodels.append(DecisionTreeClassifier())\n",
    "\tmodels.append(SVC(gamma='scale', probability=True))\n",
    "\tmodels.append(GaussianNB())\n",
    "\tmodels.append(KNeighborsClassifier())\n",
    "\tmodels.append(AdaBoostClassifier())\n",
    "\tmodels.append(BaggingClassifier(n_estimators=10))\n",
    "\tmodels.append(RandomForestClassifier(n_estimators=10))\n",
    "\tmodels.append(ExtraTreesClassifier(n_estimators=10))\n",
    "\treturn models\n",
    " \n",
    "\n",
    "def get_out_of_fold_predictions(X, y, models):\n",
    "\tmeta_X, meta_y = list(), list()\n",
    "\t\n",
    "\tkfold = KFold(n_splits=10, shuffle=True)\n",
    "\t\n",
    "\tfor train_ix, test_ix in kfold.split(X):\n",
    "\t\tfold_yhats = list()\n",
    "\t\t# get data\n",
    "\t\ttrain_X, test_X = X.iloc[train_ix], X.iloc[test_ix]\n",
    "\t\ttrain_y, test_y = y.iloc[train_ix], y.iloc[test_ix]\n",
    "\t\tmeta_y.extend(test_y)\n",
    "\t\t# fit and make predictions with each sub-model\n",
    "\t\tfor model in models:\n",
    "\t\t\tmodel.fit(train_X, train_y)\n",
    "\t\t\tyhat = model.predict(test_X)\n",
    "\t\t\t\n",
    "\t\t\tfold_yhats.append(yhat)\n",
    "\t\t\n",
    "\t\tmeta_X.append(hstack(fold_yhats))\n",
    "\treturn vstack(meta_X), asarray(meta_y)\n",
    " \n",
    "\n",
    "def fit_base_models(X, y, models):\n",
    "\tfor model in models:\n",
    "\t\tmodel.fit(X, y)\n",
    " \n",
    "# fit a meta model\n",
    "def fit_meta_model(X, y):\n",
    "\tmodel = LogisticRegression(solver='liblinear')\n",
    "\tmodel.fit(X, y)\n",
    "\treturn model\n",
    " \n",
    "\n",
    "def evaluate_models(X, y, models):\n",
    "\tfor model in models:\n",
    "\t\tyhat = model.predict(X)\n",
    "\t\tacc = accuracy_score(y, yhat)\n",
    "\t\tprint('%s: %.3f' % (model.__class__.__name__, acc*100))\n",
    " \n",
    "#make predictions with stacked model\n",
    "def super_learner_predictions(X, models, meta_model):\n",
    "\tmeta_X = list()\n",
    "\tfor model in models:\n",
    "\t\tyhat = model.predict(X) \n",
    "\t\tmeta_X.append(yhat)\n",
    "\tmeta_X = hstack(meta_X)\n",
    "\t# predict\n",
    "\treturn meta_model.predict(meta_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train (70, 204) (70,) Test (30, 204) (30,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.692e-02, tolerance: 1.539e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta  (10, 28) (70,)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [10, 70]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-241-210782b0dca3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;31m# fit the meta model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0mmeta_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfit_meta_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmeta_X\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeta_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;31m# evaluate base models\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0mevaluate_models\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-240-57912aa2da93>\u001b[0m in \u001b[0;36mfit_meta_model\u001b[0;34m(X, y)\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mfit_meta_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLogisticRegression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msolver\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'liblinear'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m   1136\u001b[0m             \u001b[0m_dtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1137\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1138\u001b[0;31m         X, y = self._validate_data(\n\u001b[0m\u001b[1;32m   1139\u001b[0m             \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1140\u001b[0m             \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/sklearn/base.py\u001b[0m in \u001b[0;36m_validate_data\u001b[0;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[1;32m    594\u001b[0m                 \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"y\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcheck_y_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    595\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 596\u001b[0;31m                 \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_X_y\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcheck_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    597\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    598\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_X_y\u001b[0;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[1;32m   1090\u001b[0m     \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_y\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmulti_output\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmulti_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_numeric\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my_numeric\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mestimator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1091\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1092\u001b[0;31m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1093\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1094\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_consistent_length\u001b[0;34m(*arrays)\u001b[0m\n\u001b[1;32m    385\u001b[0m     \u001b[0muniques\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlengths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    386\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 387\u001b[0;31m         raise ValueError(\n\u001b[0m\u001b[1;32m    388\u001b[0m             \u001b[0;34m\"Found input variables with inconsistent numbers of samples: %r\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    389\u001b[0m             \u001b[0;34m%\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlengths\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [10, 70]"
     ]
    }
   ],
   "source": [
    "#X, y = make_blobs(n_samples=2000, centers=2, n_features=200, cluster_std=20)\n",
    "# split\n",
    "#X, X_val, y, y_val = train_test_split(X, y, test_size=0.50)\n",
    "\n",
    "\n",
    "\n",
    "print('Train', X_train.shape, Y_train.shape, 'Test', X_test.shape, Y_test.shape)\n",
    "\n",
    "# get models\n",
    "models = base_models\n",
    "\n",
    "# get out of fold predictions\n",
    "meta_X, meta_y = get_out_of_fold_predictions(X_train, Y_train, models)\n",
    "print('Meta ', meta_X.shape, meta_y.shape)\n",
    "# fit base models\n",
    "\n",
    "fit_base_models(X_train, Y_train, models)\n",
    "\n",
    "# fit the meta model\n",
    "meta_model = fit_meta_model(meta_X, meta_y)\n",
    "# evaluate base models\n",
    "evaluate_models(X_test, Y_test, models)\n",
    "\n",
    "\n",
    "# evaluate meta model\n",
    "yhat = super_learner_predictions(X_test, models, meta_model)\n",
    "\n",
    "print('Super Learner: %.3f' % (accuracy_score(Y_test, yhat) * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta  (5, 56) (70,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.895e-02, tolerance: 1.433e-05\n"
     ]
    }
   ],
   "source": [
    "meta_X, meta_y = list(), list()\n",
    "\n",
    "for train_ix, test_ix in muni_cv:\n",
    "\t\tfold_yhats = list()\n",
    "\t\t# get data\n",
    "\t\ttrain_X, test_X = X_train.iloc[train_ix], X_train.iloc[test_ix]\n",
    "\t\ttrain_y, test_y = Y_train[train_ix], Y_train[test_ix]\n",
    "\t\tmeta_y.extend(test_y)\n",
    "\t\t\n",
    "\t\t# fit and make predictions with each sub-model\n",
    "\t\tfor model in base_models:\n",
    "\t\t\tmodel.fit(train_X, train_y)\n",
    "\t\t\tyhat = model.predict(test_X)\n",
    "\t\t\t\n",
    "\t\t\tfold_yhats.append(yhat)\n",
    "\t\t\n",
    "\t\tmeta_X.append(np.hstack(fold_yhats))\n",
    "\n",
    "\n",
    "meta_X, meta_y = np.vstack(meta_X), np.asarray(meta_y)\n",
    "\n",
    "print('Meta ', meta_X.shape, meta_y.shape)\n",
    "\n",
    "# for model in base_models:\n",
    "# \tmodel.fit(X_train, Y_train)\n",
    "\n",
    "# meta_model = LogisticRegression()\n",
    "# meta_model.fit(meta_X, meta_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(70,)"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meta_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.        ,  0.        ,  0.        , -0.03951597,  0.        ,\n",
       "        0.        ,  0.        ,  0.00184131,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.01592779, -0.0053252 ,  0.00677705,\n",
       "        0.00356555, -0.13710403,  0.        ,  0.00263631,  0.        ,\n",
       "        0.        , -0.01008558, -0.291695  , -0.17331958,  0.        ,\n",
       "        0.        , -0.00178474,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.01031447,  0.06197214, -0.02561009,\n",
       "       -0.0069738 ,  0.        ,  0.        ,  0.        ,  0.00259484,\n",
       "        0.        ,  0.00519219,  0.00890779,  0.0062561 , -0.00442839,\n",
       "        0.        ,  0.07266927,  0.        ,  0.        ,  0.        ,\n",
       "        0.        , -0.00590225, -0.05252337,  0.        , -0.00170457,\n",
       "        0.0016818 ,  0.04620814, -0.04285574, -0.06331849,  0.        ,\n",
       "       -0.02076918, -0.01339138, -0.00258282,  0.        ,  0.        ,\n",
       "        0.        , -0.0770421 ,  0.        ,  0.01133901, -0.01167369])"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.asarray(meta_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.895e-02, tolerance: 1.433e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta  (5, 56) (70,)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [5, 70]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-218-d57886a4b195>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mmeta_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfit_meta_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmeta_X\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeta_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-217-4a2e015ad02d>\u001b[0m in \u001b[0;36mfit_meta_model\u001b[0;34m(X, y)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mfit_meta_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLogisticRegression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m   1136\u001b[0m             \u001b[0m_dtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1137\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1138\u001b[0;31m         X, y = self._validate_data(\n\u001b[0m\u001b[1;32m   1139\u001b[0m             \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1140\u001b[0m             \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/sklearn/base.py\u001b[0m in \u001b[0;36m_validate_data\u001b[0;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[1;32m    594\u001b[0m                 \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"y\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcheck_y_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    595\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 596\u001b[0;31m                 \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_X_y\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcheck_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    597\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    598\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_X_y\u001b[0;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[1;32m   1090\u001b[0m     \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_y\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmulti_output\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmulti_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_numeric\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my_numeric\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mestimator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1091\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1092\u001b[0;31m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1093\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1094\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_consistent_length\u001b[0;34m(*arrays)\u001b[0m\n\u001b[1;32m    385\u001b[0m     \u001b[0muniques\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlengths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    386\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 387\u001b[0;31m         raise ValueError(\n\u001b[0m\u001b[1;32m    388\u001b[0m             \u001b[0;34m\"Found input variables with inconsistent numbers of samples: %r\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    389\u001b[0m             \u001b[0;34m%\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlengths\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [5, 70]"
     ]
    }
   ],
   "source": [
    "models = get_models()\n",
    "\n",
    "\n",
    "meta_X, meta_y = get_out_of_fold_predictions(X_train, Y_train, models, muni_cv)\n",
    "print('Meta ', meta_X.shape, meta_y.shape)\n",
    "\n",
    "\n",
    "fit_base_models(X_train, Y_train, models)\n",
    "\n",
    "\n",
    "meta_model = fit_meta_model(meta_X, meta_y)\n",
    "\n",
    "\n",
    "evaluate_models(X_test, Y_test, models)\n",
    "\n",
    "\n",
    "yhat = super_learner_predictions(X_train, models, meta_model)\n",
    "print('Super Learner: %.3f' % (accuracy_score(Y_test, yhat) * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>rain1</th>\n",
       "      <th>elevation</th>\n",
       "      <th>slope</th>\n",
       "      <th>aspect</th>\n",
       "      <th>near_mines</th>\n",
       "      <th>near_roads</th>\n",
       "      <th>near_hidrovia</th>\n",
       "      <th>indigenous_homol</th>\n",
       "      <th>mun_election_year</th>\n",
       "      <th>...</th>\n",
       "      <th>urban</th>\n",
       "      <th>mining</th>\n",
       "      <th>water</th>\n",
       "      <th>soybean</th>\n",
       "      <th>rice</th>\n",
       "      <th>other_crop</th>\n",
       "      <th>coffee</th>\n",
       "      <th>citrus</th>\n",
       "      <th>other_perennial</th>\n",
       "      <th>forest_lag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2016</td>\n",
       "      <td>25.33551</td>\n",
       "      <td>331.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>63.0</td>\n",
       "      <td>142159.50</td>\n",
       "      <td>101514.0</td>\n",
       "      <td>200226.30</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.144766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2004</td>\n",
       "      <td>30.56614</td>\n",
       "      <td>342.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>114.0</td>\n",
       "      <td>68099.66</td>\n",
       "      <td>128724.6</td>\n",
       "      <td>92014.58</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.022848</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.460419</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows Ã— 204 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    year     rain1  elevation  slope  aspect  near_mines  near_roads  \\\n",
       "10  2016  25.33551      331.0    1.0    63.0   142159.50    101514.0   \n",
       "11  2004  30.56614      342.0    1.0   114.0    68099.66    128724.6   \n",
       "\n",
       "    near_hidrovia  indigenous_homol  mun_election_year  ...  urban  mining  \\\n",
       "10      200226.30               0.0                1.0  ...    0.0     0.0   \n",
       "11       92014.58               0.0                1.0  ...    0.0     0.0   \n",
       "\n",
       "       water  soybean  rice  other_crop  coffee  citrus  other_perennial  \\\n",
       "10  0.000000      0.0   0.0         0.0       0       0              0.0   \n",
       "11  0.022848      0.0   0.0         0.0       0       0              0.0   \n",
       "\n",
       "    forest_lag  \n",
       "10    3.144766  \n",
       "11    1.460419  \n",
       "\n",
       "[2 rows x 204 columns]"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.iloc[[10,11]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for train_ix, test_ix in muni_cv:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(array([ 0,  1,  2,  3,  5,  6,  7,  8,  9, 10, 11, 14, 15, 16, 17, 18, 19,\n",
       "         20, 21, 22, 23, 24, 25, 26, 28, 29, 31, 32, 34, 35, 37, 38, 40, 41,\n",
       "         42, 43, 44, 45, 47, 48, 50, 51, 53, 54, 55, 56, 57, 58, 59, 61, 64,\n",
       "         65, 66, 67, 68, 69]),\n",
       "  array([ 4, 12, 13, 27, 30, 33, 36, 39, 46, 49, 52, 60, 62, 63])),\n",
       " (array([ 1,  4,  5,  6,  8,  9, 10, 12, 13, 14, 15, 16, 17, 18, 20, 21, 22,\n",
       "         24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 36, 39, 40, 42, 43, 45,\n",
       "         46, 47, 48, 49, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64,\n",
       "         65, 66, 67, 68, 69]),\n",
       "  array([ 0,  2,  3,  7, 11, 19, 23, 35, 37, 38, 41, 44, 50, 51])),\n",
       " (array([ 0,  2,  3,  4,  6,  7,  8,  9, 10, 11, 12, 13, 15, 17, 18, 19, 20,\n",
       "         21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37,\n",
       "         38, 39, 41, 42, 43, 44, 46, 48, 49, 50, 51, 52, 56, 58, 59, 60, 62,\n",
       "         63, 64, 66, 67, 68]),\n",
       "  array([ 1,  5, 14, 16, 40, 45, 47, 53, 54, 55, 57, 61, 65, 69])),\n",
       " (array([ 0,  1,  2,  3,  4,  5,  7, 11, 12, 13, 14, 16, 17, 19, 20, 22, 23,\n",
       "         25, 27, 29, 30, 31, 32, 33, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44,\n",
       "         45, 46, 47, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62,\n",
       "         63, 64, 65, 67, 69]),\n",
       "  array([ 6,  8,  9, 10, 15, 18, 21, 24, 26, 28, 34, 48, 66, 68])),\n",
       " (array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
       "         18, 19, 21, 23, 24, 26, 27, 28, 30, 33, 34, 35, 36, 37, 38, 39, 40,\n",
       "         41, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 57, 60, 61, 62,\n",
       "         63, 65, 66, 68, 69]),\n",
       "  array([17, 20, 22, 25, 29, 31, 32, 42, 43, 56, 58, 59, 64, 67]))]"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "muni_cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for train_ix, test_ix in kfold.split(X):\n",
    "\t\tfold_yhats = list()\n",
    "\t\t# get data\n",
    "\t\ttrain_X, test_X = X[train_ix], X[test_ix]\n",
    "\t\ttrain_y, test_y = y[train_ix], y[test_ix]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_X, meta_y = list(), list()\n",
    "\t\n",
    "kfold = KFold(n_splits=10, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object _BaseKFold.split at 0x7fe65841c890>"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kfold.split(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fitting 2 layers\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "'folds' must be an integer. type(<class 'list'>) was passed.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-202-7f3014f7bf94>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# Fit ensemble to the data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mensemble\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/mlens/ensemble/base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, **kwargs)\u001b[0m\n\u001b[1;32m    512\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_id_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    513\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 514\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    515\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    516\u001b[0m             \u001b[0;31m# fit_transform\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/mlens/ensemble/base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, **kwargs)\u001b[0m\n\u001b[1;32m    156\u001b[0m         with ParallelProcessing(self.backend, self.n_jobs,\n\u001b[1;32m    157\u001b[0m                                 max(self.verbose - 4, 0)) as manager:\n\u001b[0;32m--> 158\u001b[0;31m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmanager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'fit'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    159\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/mlens/parallel/backend.py\u001b[0m in \u001b[0;36mstack\u001b[0;34m(self, caller, job, X, y, path, return_preds, warm_start, split, **kwargs)\u001b[0m\n\u001b[1;32m    671\u001b[0m             \u001b[0mjob\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwarm_start\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwarm_start\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    672\u001b[0m             return_preds=return_preds, split=split, stack=True)\n\u001b[0;32m--> 673\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcaller\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcaller\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    674\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    675\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaller\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/mlens/parallel/backend.py\u001b[0m in \u001b[0;36mprocess\u001b[0;34m(self, caller, out, **kwargs)\u001b[0m\n\u001b[1;32m    716\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    717\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 718\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_partial_process\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparallel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    719\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    720\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mtask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreturn_names\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/mlens/parallel/backend.py\u001b[0m in \u001b[0;36m_partial_process\u001b[0;34m(self, task, parallel, **kwargs)\u001b[0m\n\u001b[1;32m    732\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'random_state'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    733\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 734\u001b[0;31m         \u001b[0mtask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_in\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtargets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    735\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    736\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__no_output__\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/mlens/parallel/base.py\u001b[0m in \u001b[0;36msetup\u001b[0;34m(self, X, y, job, skip, **kwargs)\u001b[0m\n\u001b[1;32m    275\u001b[0m             \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__func__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__code__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mco_varnames\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m             \u001b[0mfargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 277\u001b[0;31m             \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    278\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    279\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/mlens/parallel/base.py\u001b[0m in \u001b[0;36m_setup_0_index\u001b[0;34m(self, X, y, job)\u001b[0m\n\u001b[1;32m    108\u001b[0m         \u001b[0mindexers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_indexers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mindexer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindexers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 110\u001b[0;31m             \u001b[0mindexer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    111\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/mlens/index/fold.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, job)\u001b[0m\n\u001b[1;32m    145\u001b[0m         \"\"\"\n\u001b[1;32m    146\u001b[0m         \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m         \u001b[0mcheck_full_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfolds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_on_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_test_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/mlens/index/_checks.py\u001b[0m in \u001b[0;36mcheck_full_index\u001b[0;34m(n_samples, folds, raise_on_exception)\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;34m\"\"\"Check that folds can be constructed from passed arguments.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfolds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mIntegral\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         raise ValueError(\"'folds' must be an integer. \"\n\u001b[0m\u001b[1;32m     19\u001b[0m                          \"type(%s) was passed.\" % type(folds))\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: 'folds' must be an integer. type(<class 'list'>) was passed."
     ]
    }
   ],
   "source": [
    "# Create Super Learner ensemble\n",
    "ensemble = SuperLearner(folds = muni_cv, scorer = 'accuracy', random_state = 42, verbose=2)\n",
    "ensemble.add(base_models)\n",
    "ensemble.add_meta(LogisticRegression())\n",
    "\n",
    "# Fit ensemble to the data\n",
    "ensemble.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Build ---\n",
    "# Passing a scoring function will create cv scores during fitting\n",
    "# the scorer should be a simple function accepting to vectors and returning a scalar\n",
    "ensemble = SuperLearner(random_state=42, verbose=2)\n",
    "\n",
    "# Build the first layer\n",
    "ensemble.add(base_models)\n",
    "\n",
    "# Attach the final meta estimator\n",
    "ensemble.add_meta(LogisticRegression())\n",
    "\n",
    "# --- Use ---\n",
    "\n",
    "# Fit ensemble\n",
    "ensemble.fit(X[:75], y[:75])\n",
    "\n",
    "# Predict\n",
    "preds = ensemble.predict(X[75:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 0,  1,  2,  3,  5,  6,  7,  8,  9, 10, 11, 14, 15, 16, 17, 18, 19,\n",
       "        20, 21, 22, 23, 24, 25, 26, 28, 29, 31, 32, 34, 35, 37, 38, 40, 41,\n",
       "        42, 43, 44, 45, 47, 48, 50, 51, 53, 54, 55, 56, 57, 58, 59, 61, 64,\n",
       "        65, 66, 67, 68, 69]),\n",
       " array([ 4, 12, 13, 27, 30, 33, 36, 39, 46, 49, 52, 60, 62, 63]))"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "muni_cv[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble = SuperLearner(folds=5, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SuperLearner(array_check=None, backend=None, folds=5,\n",
       "       layers=[Layer(backend='threading', dtype=<class 'numpy.float32'>, n_jobs=-1,\n",
       "   name='layer-1', propagate_features=None, raise_on_exception=True,\n",
       "   random_state=None, shuffle=False,\n",
       "   stack=[Group(backend='threading', dtype=<class 'numpy.float32'>,\n",
       "   indexer=FoldIndex(X=None, folds=5, raise_on_ex...rer=None)],\n",
       "   n_jobs=-1, name='group-0', raise_on_exception=True, transformers=[])],\n",
       "   verbose=1)],\n",
       "       model_selection=False, n_jobs=None, raise_on_exception=True,\n",
       "       random_state=None, sample_size=20, scorer=None, shuffle=False,\n",
       "       verbose=2)"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ensemble.add(base_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fitting 1 layers\n",
      "Processing layer-1             done | 00:00:14\n",
      "Fit complete                        | 00:00:16\n"
     ]
    }
   ],
   "source": [
    "search = ensemble.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SuperLearner(array_check=None, backend=None, folds=5,\n",
       "       layers=[Layer(backend='threading', dtype=<class 'numpy.float32'>, n_jobs=-1,\n",
       "   name='layer-1', propagate_features=None, raise_on_exception=True,\n",
       "   random_state=None, shuffle=False,\n",
       "   stack=[Group(backend='threading', dtype=<class 'numpy.float32'>,\n",
       "   indexer=FoldIndex(X=None, folds=5, raise_on_ex...rer=None)],\n",
       "   n_jobs=-1, name='group-0', raise_on_exception=True, transformers=[])],\n",
       "   verbose=1)],\n",
       "       model_selection=False, n_jobs=None, raise_on_exception=True,\n",
       "       random_state=None, sample_size=20, scorer=None, shuffle=False,\n",
       "       verbose=2)"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Viz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlens.visualization import corr_X_y, corrmat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An input array is constant; the correlation coefficent is not defined.\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1152x720 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "GridSpec(2, 2)"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corr_X_y(X_train, Y_train, figsize=(16, 10), label_rotation=80, hspace=1, fontsize=14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d49eb846d9e9e33562e6444db983d5d3192f0621d82336681e003790fb98d6fc"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
