1/23-1/29: 
- created repo, added csv data from Box, realized that the csvs don't seem to be very substantial so will next work on .rdata files
- worked on getting data into notebook, took a long time to figure out how to get .rdata file into python, ended up having to open with R studio and run code to output csv. created text file of each var name, might be good to have this dictionary esp w focus on interpretability, emailed Kathryn about where to start with data analysis since there are many .rdata files
- continued to work with data, looked through analysis data in r and outputted to csv to be compatible with python, still haven't figured out reading .rdata directly into python but was able to convert entire frame to csv 

1/30-2/5: 
- looking at variables and list sent by Kathryn of what to start with, spent time looking through some geospatial projects online to see how they work with this type of data and what types of things I should look for in my analysis 
- continued to look at geospatial data, a lot of the past projects of this sort use shape file data which is different than the format of the frame I'm working with so I spent time today thinking about how would be best to do my initial visualizations to get to know the data
- created some visualizations of variables that Kathryn mentioned as being important, looked at surface plots, histograms for each category

2/5-2/12: 
- learning Google Earth Engine, working on understanding data with Kathryn, seems like my initial data was truncating somewhere in the process of transferring from R to Python so looked into this as well

2/13-2/20:
- got entire data set to read in correctly, starting to explore working with decision tree to predict nn loss
- looking into machine learning that considers covariates

2/21-2/27:
- working on setting up a random forest for the data that I have that mimics the corruption paper
- considering analagous metrics for regression (corruption paper uses classification metrics) 
- looking at potential variable groupings for data I have so far

2/28-3/6:
- looking into variable importance and metrics to evaluate cross validation
- implementing gradient boosting in same framework as random forests

3/7-3/13:
- researching and implementing neural networks 

3/14-3/20:
- fixing gradient boosting and cross validation functions

3/21-3/27:
- looking for source of information leakage
- test train split by municipality

3/28-4/3:
- looking for source of information leakage
- new way of test train split by municipality that addresses the variance in mun population


4/4-4/10:
- merging new data
- researching normalization and importance
- check of test train split by municipality visually

4/11-4/17
- merging new data
- researching MLP
- researching covariate tests pre modeling

4/18-4/24
- merging data continued, addressing meaning of NAs in each variable 
- discussed time series issues with Kathryn and Luke 

4/24-4/29
- started to play with time series idea
- continued to pull in data, worked with Kathryn to correctly account for all files

4/30-5/6
-Pause for finals

5/7-5/13
- Pause for finals

5/14-5/20
- Pulling in data, translating variables to English
- dealing with duplicates in these new datasets that I'm pulling in, need help from Kathryn with this

5/21-5/27
- continued data merging, translating variables, need help with finbra dataset and need to meet with Kathryn to clarify a few points
- started with PCA, researched incremental PCA since full set has observations >> columns

5/28-6/3
-continued to research incremental PCA and worked on implementing PCA on small subset

6/4-6/10
- figuring out cross-validation parameters, understanding all options and how each may or may not be relevant to this data

6/11-6/17
- working on PIB data, worked with Kathryn to fix gaps in this data
- also missing emissions data from 2016, worked with Kathryn on this

6/18-6/24
- discovered municipalities that split as source of NAs in dataset with mun-level variables
- looking into these splits online to try to find some sort of list of all the changes, couldn't find this

6/25-7/1
- fixed emissions gaps, fixed audit NAs, calculations on emissions data
- discussed elections level data goals with Kathryn

7/2-7/8
- worked on starting elections calculations
- also discussed project with Dr. Michael Ross at UCLA which was illuminating

7/9-7/15
- more elections calculations and working on cross-validation pipelines for each method

7/16-7/22
- more elections data work, found that some candidates ran and held office in multiple municipalities which was source of error on some calculations  

7/23-7/29
- ran into difficulty after finding that some candidates ran again after term limit up, investigated this to see if it was actually what happened or an error in the way I calculated term limit and incumbent variables, turns out this actually happened
- discussed this with Kathryn too

7/30-8/5
- finished up elections variables after clarifying term limit and incumbent confusion with Kathryn 

8/6-8/12
- continued to look into interpretation of neural network results

8/13-8/19
- tried implementing several approaches to interpretable neural networks, after this will almost have all methods of variable importance complete
- the interpretability of neural networks seems to be non trivial and evolving. the corruption paper doesn't elucidate their method of interpretation in a way that is entirely clear to me at this point
- discuss with Luke and Kathryn

8/20-8/26
- Met with Kathryn and discussed runoff variable, realize change of approach is in order given presence of runoffs

8/26-9/2
- changed election data processing approach to account for runoffs (CandPrefeitoDataProcessingNew.ipynb)

9/3-9/9
- translating to English and sorting jobs, check with Kathryn about decisions at this stage
- found potential gap in data in a few years with runoff, ask Kathryn about this too

9/10-9/17
- continued reading up on spatiotemporal dependence options (reading R code from Oliviera paper)
- working on election data and sorting jobs in the meantime

9/18-9/24
- election data work, waiting to hear back from Kathryn to finalize some decisions

9/25-10/1
- working on turno issues with election data and figuring out where missing data is

10/2-10/8
- looking into different types of elections and figuring out which to use where
- reading through spatiotemporal cross validation R code

10/9-10/15
- reading up on spatiotemporal dependence to figure out R code
- election job cleaning and categorization

10/16-10/22
- election data supplementary elections analysis
- trying to merge in 2016 data, searching for name/codigo information (can't find!)

10/23-10/30
- cleaning bigger data frame, writing up summary of NAs that remain after my investigation for Kathryn
- more spatiotemporal cv R code into python work

10/31-11/5
- checking ML code with the current data to make sure it handles new data and get a sense of runtime for when data fully done (soon!)

11/6-11/12
- inventory of all nas that remain the large dataset since last I checked
- looking into municpality changes to see if these are the cause of the nulls

11/13-11/19
- investigating nulls in election data for 1996, 2016, 2020
- exploration of name-level merges from other data sources of winners for 1996 to get incumbants

11/20-11/26
- implementing changes to the election data for the 2016/20
- substituting use of deforest R file with the individual-level 
- investigating use of spatial merge to handle municipality structure changes


In progress/toDo: 
- finish election changes
- individual datasets replace the Rdata file with gaps
- another inventory of NAs after those changes!

- figure out neural network interpretability 
- add NMF after talking to Luke
- merge election data back into main data
- complete neural network interpretation






